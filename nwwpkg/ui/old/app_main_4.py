# app_main.py (ì‹¤ì œ í˜¸ì¶œ ë²„ì „: Ingest â†’ Normalize â†’ Analyze â†’ Gate â†’ Scoring â†’ Fusion â†’ Blocks â†’ Scenarios â†’ Alerts)
import os
import io
import re
import json
from datetime import datetime
from collections import Counter

import streamlit as st
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# --------------------------
# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
# --------------------------
from nwwpkg.ingest import news_collector
from nwwpkg.preprocess import cleaner, tokenizer, embedder
from nwwpkg.eds import frame_tagger
from nwwpkg.score import indicator_scorer, dbn_inference
from nwwpkg.fusion import fuse
from nwwpkg.scenario import scenario_matcher, scenario_predictor
from nwwpkg.analyze import impact_analyzer, frame_shift_detector, hidden_network_detector
from nwwpkg.judge import llm_judge
from nwwpkg.decider import alert_decider
from nwwpkg.ledger import recorder, viewer

# --- Korean font resolver (Windows/Mac/Linux ì§€ì›) ---
import platform, os
from matplotlib import font_manager, rcParams

def get_korean_font_path(custom_path: str | None = None) -> str | None:
    if custom_path and os.path.exists(custom_path):
        return custom_path

    candidates = []
    if platform.system() == "Windows":
        candidates += [
            r"C:\Windows\Fonts\malgun.ttf",            # ë§‘ì€ê³ ë”•
            r"C:\Windows\Fonts\malgunbd.ttf",
            r"C:\Windows\Fonts\NanumGothic.ttf",       # ë‚˜ëˆ”ê³ ë”•(ìˆëŠ” ê²½ìš°)
        ]
    elif platform.system() == "Darwin":  # macOS
        candidates += [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ]
    else:  # Linux
        candidates += [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        ]

    for p in candidates:
        if os.path.exists(p):
            return p
    return None

def ensure_matplotlib_korean(font_path: str | None):
    """matplotlibì— í°íŠ¸ë¥¼ ë“±ë¡ (íƒ€ì´í‹€/ë¼ë²¨ í•œê¸€ ê¹¨ì§ ë°©ì§€)"""
    if not font_path:
        return
    try:
        font_manager.fontManager.addfont(font_path)
        fp = font_manager.FontProperties(fname=font_path)
        rcParams["font.family"] = fp.get_name()
    except Exception:
        pass
# ===== Korean WordCloud Utilities =====

# --------------------------
# ê³µí†µ ìœ í‹¸
# --------------------------
DEFAULT_BUNDLE = "sample"

def load_jsonl(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        return pd.DataFrame()
    with open(path, "r", encoding="utf-8") as f:
        lines = [json.loads(line) for line in f]
    return pd.DataFrame(lines)

def save_jsonl(df: pd.DataFrame, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for _, row in df.iterrows():
            rec = row.to_dict()
            for k, v in rec.items():
                if isinstance(v, pd.Timestamp):
                    rec[k] = v.isoformat()
                elif isinstance(v, float) and pd.isna(v):
                    rec[k] = None
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

# --- Streamlit rerun í˜¸í™˜ ë˜í¼ ---
# =========================
# Helpers: rerun / fused backfill / quick pipeline
# =========================
import hashlib

def _safe_rerun():
    """Streamlit rerun: st.rerun() ìš°ì„ , ì—†ìœ¼ë©´ experimental_rerun()"""
    try:
        st.rerun()
    except Exception:
        try:
            st.experimental_rerun()  # ì¼ë¶€ ë²„ì „ë§Œ ì¡´ì¬
        except Exception:
            st.toast("ğŸ” í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨(F5) í•´ì£¼ì„¸ìš”.")

def _content_id(row):
    base = str(row.get("url") or row.get("normalized") or row.get("text") or "")
    return hashlib.sha1(base.encode("utf-8", "ignore")).hexdigest() if base else None

def _safe_two_cols(df: pd.DataFrame, cols: list[str]) -> list[str]:
    return [c for c in cols if c in df.columns]

def _ensure_fused(df: pd.DataFrame, bundle_id: str) -> pd.DataFrame:
    """dfì— fused_scoreê°€ ì—†ì„ ë•Œ ìƒì„± ê·œì¹™: indicator+dbn â†’ score â†’ 0.0"""
    if "fused_score" in df.columns:
        return df
    out = df.copy()
    if "indicator_score" in out.columns and "dbn_score" in out.columns:
        out["fused_score"] = out.apply(
            lambda r: fuse.combine([float(r.get("indicator_score", 0.0)),
                                    float(r.get("dbn_score", 0.0))]),
            axis=1
        )
        return out
    if "score" in out.columns:
        out["fused_score"] = out["score"].astype(float)
        return out
    out["fused_score"] = 0.0
    return out

def backfill_fused_score(df_blocks: pd.DataFrame, bundle_id: str) -> pd.DataFrame:
    """
    blocks.jsonl ë“±ì— fused_scoreê°€ ì—†ì„ ë•Œ ë³µêµ¬:
    1) fusion.jsonl â†’ url / content_id ë³‘í•©
    2) scoring.jsonl â†’ url / content_id ë³‘í•©
    3) ê·¸ë˜ë„ ì—†ìœ¼ë©´ indicator+dbn ì¬ê³„ì‚° or 0.0
    """
    if not isinstance(df_blocks, pd.DataFrame) or df_blocks.empty:
        return df_blocks
    if "fused_score" in df_blocks.columns:
        return df_blocks

    out = df_blocks.copy()
    if "content_id" not in out.columns:
        out["content_id"] = out.apply(_content_id, axis=1)

    for ref_name in ["fusion.jsonl", "scoring.jsonl"]:
        ref = load_jsonl(f"data/{bundle_id}/{ref_name}")
        if ref.empty:
            continue
        ref = _ensure_fused(ref, bundle_id)

        # url ë³‘í•©
        common = _safe_two_cols(ref, ["url", "fused_score"])
        if {"url", "fused_score"}.issubset(common) and "url" in out.columns:
            merged = out.merge(ref[common], on="url", how="left", suffixes=("", "_f"))
            if "fused_score_f" in merged.columns:
                merged["fused_score"] = merged["fused_score"].fillna(merged["fused_score_f"])
                merged = merged.drop(columns=[c for c in ["fused_score_f"] if c in merged.columns])
            if "fused_score" in merged.columns and merged["fused_score"].notna().any():
                out = merged

        # content_id ë³‘í•©
        if "content_id" not in ref.columns:
            ref = ref.copy(); ref["content_id"] = ref.apply(_content_id, axis=1)
        common = _safe_two_cols(ref, ["content_id", "fused_score"])
        if {"content_id", "fused_score"}.issubset(common):
            merged = out.merge(ref[common], on="content_id", how="left", suffixes=("", "_f"))
            if "fused_score_f" in merged.columns:
                merged["fused_score"] = merged["fused_score"].fillna(merged["fused_score_f"])
                merged = merged.drop(columns=[c for c in ["fused_score_f"] if c in merged.columns])
            if "fused_score" in merged.columns and merged["fused_score"].notna().any():
                out = merged

    out = _ensure_fused(out, bundle_id)
    return out

def _quick_pipeline(bundle_id: str) -> bool:
    """
    ingest â†’ normalize â†’ analyze â†’ gate â†’ scoring â†’ blocks â†’ scenarios â†’ alerts
    ë¹ ë¥¸ ì¼ê´„ ì²˜ë¦¬(MVP). íŒŒì¼ì„ ë‹¨ê³„ë³„ë¡œ ë®ì–´ì”€.
    """
    # 1) ingest â†’ normalize
    df_ing = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    if df_ing.empty:
        return False

    df_norm = df_ing.copy()
    df_norm["normalized"] = df_norm["text"].astype(str).apply(cleaner.normalize)
    df_norm["sentences"] = df_norm["normalized"].apply(tokenizer.split_sentences)
    save_jsonl(df_norm, f"data/{bundle_id}/normalize.jsonl")

    # 2) analyze
    df_ana = df_norm.copy()
    df_ana["frames"] = df_ana["normalized"].apply(frame_tagger.tag)
    save_jsonl(df_ana, f"data/{bundle_id}/analyze.jsonl")

    # 3) gate
    df_gate = df_ana.copy()
    df_gate["indicator_score"] = df_gate["frames"].apply(indicator_scorer.run)
    save_jsonl(df_gate, f"data/{bundle_id}/gate.jsonl")

    # 4) scoring
    ind_scores, dbn_scores, fused_scores = [], [], []
    prev_frames = None
    for _, r in df_gate.iterrows():
        fr = r.get("frames", [])
        ind = float(r.get("indicator_score", 0.0))
        dbn = dbn_inference.run(fr, prev_frames=prev_frames)
        fused = fuse.combine([ind, dbn])
        ind_scores.append(ind); dbn_scores.append(dbn); fused_scores.append(fused)
        prev_frames = fr

    df_score = df_gate.copy()
    df_score["dbn_score"]   = dbn_scores
    df_score["fused_score"] = fused_scores
    save_jsonl(df_score, f"data/{bundle_id}/scoring.jsonl")

    # 5) blocks
    def _primary(fs):
        if not fs: return "General"
        best = sorted(fs, key=lambda f: f.get("score", 0), reverse=True)[0]
        return best.get("frame", "General")
    df_blk = df_score.copy()
    df_blk["block"] = df_blk["frames"].apply(_primary)
    keep = [c for c in ["url","date","normalized","frames","indicator_score","dbn_score","fused_score"] if c in df_blk.columns]
    out_blk = df_blk[keep + ["block"]].copy()
    save_jsonl(out_blk, f"data/{bundle_id}/blocks.jsonl")

    # 6) scenarios
    matched, predicted = [], []
    for _, r in out_blk.iterrows():
        sents = r.get("sentences", [])
        if not sents and isinstance(r.get("normalized",""), str):
            sents = tokenizer.split_sentences(r["normalized"])
        vecs = embedder.embed(sents)
        matched.append(scenario_matcher.match(vecs, top_k=3))
        predicted.append(scenario_predictor.generate(r.get("normalized","")))
    df_scen = out_blk.copy()
    df_scen["scenario_matched"]   = matched
    df_scen["scenario_predicted"] = predicted
    save_jsonl(df_scen, f"data/{bundle_id}/scenarios.jsonl")

    # 7) alerts
    df_alert = df_scen.copy()
    df_alert["decision"] = df_alert["fused_score"].apply(alert_decider.decide)
    def _to_level(dec: str) -> str:
        if "High" in dec: return "High"
        if "Medium" in dec: return "Medium"
        return "Low"
    df_alert["alert_level"] = df_alert["decision"].apply(_to_level)
    save_jsonl(df_alert, f"data/{bundle_id}/alerts.jsonl")
    return True

# ===== Plotly íŒ”ë ˆíŠ¸ ìœ í‹¸ =====
import itertools
import plotly.express as px

# ëª…ëª©í˜•(discrete) íŒ”ë ˆíŠ¸ë“¤ ìˆœí™˜
_PALLETS = [
    px.colors.qualitative.Set1,
    px.colors.qualitative.Set2,
    px.colors.qualitative.Set3,
    px.colors.qualitative.Bold,
    px.colors.qualitative.Dark24,
    px.colors.qualitative.Pastel,
    px.colors.qualitative.D3,
    px.colors.qualitative.T10,
    px.colors.qualitative.G10,
]
_pal_iter = itertools.cycle(_PALLETS)

def next_palette():
    """ì°¨íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ìƒ‰ íŒ”ë ˆíŠ¸(qualitative)"""
    return next(_pal_iter)

# ì—°ì†(continuous) íŒ”ë ˆíŠ¸ë“¤ ìˆœí™˜ (ì§€ë„/heatmap ë“±)
_CSEQS = [
    px.colors.sequential.Viridis,
    px.colors.sequential.Plasma,
    px.colors.sequential.Magma,
    px.colors.sequential.Cividis,
    px.colors.sequential.Blues,
    px.colors.sequential.Greens,
    px.colors.sequential.Reds,
    px.colors.sequential.Oranges,
    px.colors.sequential.Turbo,
]
_cseq_iter = itertools.cycle(_CSEQS)

def next_cscale():
    """ì°¨íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì—°ì† íŒ”ë ˆíŠ¸"""
    return next(_cseq_iter)

# ===== Alerts Time-series Helper =====
def _alert_timeseries(df: pd.DataFrame, freq: str = "D") -> pd.DataFrame:
    """
    df_alerts -> (date, alert_level) ì‹œê³„ì—´ ì§‘ê³„
    freq: "D"(ì¼), "W"(ì£¼), "M"(ì›”)
    return: long-form [date, alert_level, count]
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["date", "alert_level", "count"])
    need = {"date", "alert_level"}
    if not need.issubset(df.columns):
        return pd.DataFrame(columns=["date", "alert_level", "count"])

    out = df.copy()
    try:
        out["date"] = pd.to_datetime(out["date"])
    except Exception:
        # dateê°€ ë¬¸ìì—´ì´ ì•„ë‹ˆê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ â†’ ë¹ˆ í”„ë ˆì„ ë°˜í™˜
        return pd.DataFrame(columns=["date", "alert_level", "count"])

    # íƒ€ì„ì¡´ ì œê±° ë° ì£¼ê¸°ë³„ ë²„í‚·íŒ…
    out["date"] = out["date"].dt.tz_localize(None, nonexistent="shift_forward", ambiguous="NaT", errors="ignore")
    out["date"] = out["date"].dt.to_period(freq).dt.to_timestamp()

    # ê²½ë³´ ë ˆë²¨ ì •ê·œí™”
    lvl_map = {"HIGH": "High", "high": "High", "H": "High",
               "MEDIUM": "Medium", "medium": "Medium", "M": "Medium",
               "LOW": "Low", "low": "Low", "L": "Low"}
    out["alert_level"] = out["alert_level"].astype(str).map(lambda x: lvl_map.get(x, x))
    order = ["High", "Medium", "Low"]
    out["alert_level"] = pd.Categorical(out["alert_level"], categories=order, ordered=True)

    grp = out.groupby(["date", "alert_level"]).size().reset_index(name="count")
    grp = grp.sort_values(["date", "alert_level"])
    return grp

def _to_wide(ts_long: pd.DataFrame) -> pd.DataFrame:
    """long-form(timeseries) â†’ wide index=date, columns(level)"""
    if ts_long.empty:
        return pd.DataFrame(columns=["date","High","Medium","Low"])
    wide = ts_long.pivot(index="date", columns="alert_level", values="count").fillna(0)
    # ë ˆë²¨ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì¥
    for c in ["High","Medium","Low"]:
        if c not in wide.columns:
            wide[c] = 0
    wide = wide[["High","Medium","Low"]]
    wide = wide.sort_index()
    return wide

def reset_index_as_date(df: pd.DataFrame) -> pd.DataFrame:
    out = df.reset_index()
    # ì´ë¯¸ 'date' ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ê³  'index'ë§Œ ìˆìœ¼ë©´ dateë¡œ
    if "date" not in out.columns and "index" in out.columns:
        out = out.rename(columns={"index":"date"})
    # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ë°©ì§€
    if "date" in out.columns and getattr(out.columns, "duplicated", lambda: False)().any():
        # ì²« ë²ˆì§¸ dateë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ date* ì»¬ëŸ¼ìœ¼ë¡œ ë³€ê²½
        seen = False
        new_cols = []
        for c in out.columns:
            if c == "date":
                if not seen:
                    new_cols.append(c); seen = True
                else:
                    new_cols.append("date_idx")
            else:
                new_cols.append(c)
        out.columns = new_cols
    return out

# --- hidden_network_detector í˜¸ì¶œ í˜¸í™˜ ë ˆì´ì–´ ---
def _postfilter_graph(graph: dict, min_count: int, stopwords: set[str]) -> dict:
    """êµ¬ë²„ì „ detect() ë°˜í™˜ ê·¸ë˜í”„ì— ëŒ€í•´ stopwords ì œê±° + min_count ì„ê³„ ì ìš©"""
    if not isinstance(graph, dict):
        return {"nodes": {}, "edges": {}}

    nodes = graph.get("nodes", {}) or {}
    edges = graph.get("edges", {}) or {}

    # ë…¸ë“œ: ë¶ˆìš©ì–´ ì œê±° + ì„ê³„ ì ìš©
    nodes2 = {k: int(v) for k, v in nodes.items()
              if isinstance(v, (int, float)) and v >= min_count and k not in stopwords}

    # ì—£ì§€: ë…¸ë“œ ìƒì¡´ ì—¬ë¶€ + ì„ê³„ ì ìš©
    edges2 = {}
    for key, c in (edges.items() if isinstance(edges, dict) else []):
        if not isinstance(c, (int, float)) or c < min_count:
            continue
        if isinstance(key, tuple):
            a, b = key
            edge_key = f"{a}â€”{b}"
        else:
            parts = str(key).split("â€”")
            if len(parts) != 2:
                continue
            a, b = parts
            edge_key = key
        if a in nodes2 and b in nodes2:
            edges2[edge_key] = int(c)

    return {"nodes": nodes2, "edges": edges2}

def _detect_network_compat(sentences, min_count: int, stopwords: set[str]):
    """
    ì‹ ë²„ì „: detect(sentences, min_count=..., stopwords=...)
    êµ¬ë²„ì „: detect(sentences)ë§Œ ì§€ì› â†’ ë°˜í™˜ ê·¸ë˜í”„ë¥¼ í›„ì²˜ë¦¬ë¡œ í•„í„°ë§
    """
    try:
        return hidden_network_detector.detect(sentences, min_count=min_count, stopwords=stopwords)
    except TypeError:
        g = hidden_network_detector.detect(sentences)
        return _postfilter_graph(g, min_count, stopwords)


# ===== Plotly íŒ”ë ˆíŠ¸ ìœ í‹¸ =====
import itertools
import plotly.express as px

_PALLETS = [
    px.colors.qualitative.Set1, px.colors.qualitative.Set2, px.colors.qualitative.Set3,
    px.colors.qualitative.Bold, px.colors.qualitative.Dark24, px.colors.qualitative.Pastel,
    px.colors.qualitative.D3, px.colors.qualitative.T10, px.colors.qualitative.G10,
]
_pal_iter = itertools.cycle(_PALLETS)
def next_palette(): return next(_pal_iter)

_CSEQS = [
    px.colors.sequential.Viridis, px.colors.sequential.Plasma, px.colors.sequential.Magma,
    px.colors.sequential.Cividis, px.colors.sequential.Blues, px.colors.sequential.Greens,
    px.colors.sequential.Reds, px.colors.sequential.Oranges, px.colors.sequential.Turbo,
]
_cseq_iter = itertools.cycle(_CSEQS)
def next_cscale(): return next(_cseq_iter)

# ===== GeoJSON & í–‰ì •ì½”ë“œ ìœ í‹¸ =====
import json, os

def load_geojson(path: str):
    if not path or not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def _detect_sig_props(geo: dict):
    """GeoJSONì˜ propertiesì—ì„œ ì½”ë“œ/ì´ë¦„ í‚¤ ìë™ íƒì§€"""
    if not geo or "features" not in geo or not geo["features"]:
        return None, None
    props = geo["features"][0].get("properties", {})
    # í”í•œ í‚¤ íŒ¨í„´
    candidates_code = ["SIG_CD", "adm_cd", "ADM_CD"]
    candidates_name = ["SIG_KOR_NM", "sig_kor_nm", "ADM_NM", "adm_nm", "EMD_KOR_NM"]
    code_key = next((k for k in candidates_code if k in props), None)
    name_key = next((k for k in candidates_name if k in props), None)
    return code_key, name_key

def attach_sig_cd_from_name(df: pd.DataFrame, geo: dict, name_col: str) -> pd.DataFrame:
    """df[name_col] (ì˜ˆ: 'ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬')ë¥¼ GeoJSONì˜ ì‹œêµ°êµ¬ ì½”ë“œë¡œ ë§¤í•‘í•´ df['sig_cd'] ìƒì„±"""
    code_key, name_key = _detect_sig_props(geo)
    if not (code_key and name_key) or name_col not in df.columns:
        return df
    # GeoJSON name->code ì‚¬ì „
    pairs = [(f["properties"][name_key], f["properties"][code_key]) for f in geo["features"]]
    name2code = {str(n): str(c) for n, c in pairs if n and c}
    out = df.copy()
    out["sig_cd"] = out[name_col].astype(str).map(name2code)
    return out

def ensure_sig_cd(df: pd.DataFrame, geo: dict) -> pd.DataFrame:
    """dfì— sig_cdê°€ ì—†ìœ¼ë©´ region/sigunguì—ì„œ ìœ ì¶”"""
    if "sig_cd" in df.columns:
        return df
    # ìš°ì„ ìˆœìœ„: sigungu(ì‹œêµ°êµ¬ ì „ì²´ëª…) â†’ region(ê¶Œì—­ëª…ì€ ë§¤ì¹­ ë‚®ìŒ)
    if "sigungu" in df.columns:
        return attach_sig_cd_from_name(df, geo, "sigungu")
    if "region" in df.columns:
        # regionì— ì‹œêµ°êµ¬ê°€ ì§ì ‘ ë“¤ì–´ì˜¬ ê°€ëŠ¥ì„±ì´ ë‚®ìŒ. ê·¸ë˜ë„ ì‹œë„.
        return attach_sig_cd_from_name(df, geo, "region")
    return df



# ==========================
# 1) Ingest
# ==========================
# ---------------- Ingest íƒ­ ----------------
def page_ingest(bundle_id="sample"):
    st.header("ğŸ“° Ingest â€“ ê¸°ì‚¬ ìˆ˜ì§‘")

    url = st.text_input("ê¸°ì‚¬ URL ì…ë ¥")
    text_input = st.text_area("ê¸°ì‚¬ ì›ë¬¸ ì…ë ¥(ì„ íƒ)", height=200)

    if st.button("ê¸°ì‚¬ ì €ì¥"):
        if not url and not text_input:
            st.warning("âš ï¸ URL ë˜ëŠ” ê¸°ì‚¬ ì›ë¬¸ ì…ë ¥ í•„ìš”")
            return

        # ì‹¤ì œ ê¸°ì‚¬ ìˆ˜ì§‘
        source = "Manual"; title = None; published = None
        text = text_input or ""
        if url:
            ing = news_collector.collect(url)
            text = text or ing.get("text", "")
            source = ing.get("source") or news_collector.get_domain(url)
            title = ing.get("title"); published = ing.get("published")

        df_new = pd.DataFrame([{
            "url": url or None,
            "text": text,
            "date": datetime.today().strftime("%Y-%m-%d"),
            "source": source,
            "title": title,
            "published": published
        }])
        recorder.save(bundle_id, "ingest.jsonl", df_new, dedup_on="url")
        st.success("âœ… ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")

    # ===========================
    # ğŸ“‘ ì €ì¥ëœ ê¸°ì‚¬ ëª©ë¡ (+ì‚­ì œ)
    # ===========================
    st.subheader("ğŸ“‘ ì €ì¥ëœ ê¸°ì‚¬ ëª©ë¡")
    ingest_path = f"data/{bundle_id}/ingest.jsonl"
    df = load_jsonl(ingest_path)

    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì•ˆì •ì  ì‚­ì œë¥¼ ìœ„í•œ ë‚´ë¶€ rowid ë³´ê°•
    df = df.copy()
    df["__rowid"] = df.index

    # í‘œì‹œìš© í…Œì´ë¸” êµ¬ì„±
    view_cols = [c for c in ["__rowid", "date", "source", "title", "url", "published", "text"] if c in df.columns]
    df_view = df[view_cols].copy()
    df_view["ì‚­ì œ"] = False  # ì²´í¬ë°•ìŠ¤ ì»¬ëŸ¼

    edited = st.data_editor(
        df_view,
        key=f"ingest_editor_{bundle_id}",
        hide_index=True,
        use_container_width=True,
        column_config={
            "__rowid": st.column_config.NumberColumn("rowid", help="ë‚´ë¶€ ì‹ë³„ì", disabled=True),
            "ì‚­ì œ": st.column_config.CheckboxColumn("ì‚­ì œ", help="ì‚­ì œí•  í–‰ ì„ íƒ"),
            "text": st.column_config.TextColumn("text", disabled=True)
        }
    )

    c1, c2, c3 = st.columns([1,1,1])

    # ğŸ—‘ ì„ íƒ í–‰ ì‚­ì œ
    if c1.button("ğŸ—‘ ì„ íƒ í–‰ ì‚­ì œ"):
        to_del = edited.loc[edited["ì‚­ì œ"] == True, "__rowid"].tolist()
        if not to_del:
            st.info("ì„ íƒëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            new_df = df[~df["__rowid"].isin(to_del)].drop(columns="__rowid")
            save_jsonl(new_df, ingest_path)
            st.success(f"âœ… {len(to_del)}ê±´ ì‚­ì œ ì™„ë£Œ")
            _safe_rerun()

    # ğŸ§¯ URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ìµœì‹ ë§Œ ìœ ì§€)
    if c2.button("ğŸ§¯ URL ì¤‘ë³µ ì œê±°"):
        if "url" in df.columns:
            before = len(df)
            # ìµœì‹  ë‚ ì§œë¥¼ ë‚¨ê¸°ë„ë¡ ì •ë ¬ í›„ drop_duplicates
            tmp = df.sort_values(by=["date"], ascending=True) if "date" in df.columns else df
            new_df = tmp.drop_duplicates(subset=["url"], keep="last").drop(columns="__rowid", errors="ignore")
            save_jsonl(new_df, ingest_path)
            st.success(f"âœ… ì¤‘ë³µ ì œê±°: {before - len(new_df)}ê±´ ì •ë¦¬")
            _safe_rerun()
        else:
            st.warning("URL ì»¬ëŸ¼ì´ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸ§¨ ì „ì²´ ì‚­ì œ(ì´ˆê¸°í™”)
    if c3.button("ğŸ§¨ ì „ì²´ ì‚­ì œ(ì´ˆê¸°í™”)"):
        empty = pd.DataFrame(columns=[c for c in df.columns if c != "__rowid"])
        save_jsonl(empty, ingest_path)
        st.success("âœ… ì „ì²´ ì‚­ì œ ì™„ë£Œ")
        _safe_rerun()

    # ê°„ë‹¨ ë¶„í¬ ì°¨íŠ¸
    if "source" in df.columns:
        counts = df["source"].fillna("Manual").value_counts().reset_index()
        counts.columns = ["source", "count"]
        fig = px.bar(counts, x="source", y="count", text="count",
                     labels={"source": "ì–¸ë¡ ì‚¬", "count": "ê¸°ì‚¬ ìˆ˜"},
                     title="ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ë¶„í¬")
        fig.update_traces(textposition="outside")
        st.plotly_chart(fig, use_container_width=True)


# ==========================
# 2) Normalize (ì „ì²˜ë¦¬)
# ==========================
def page_normalize(bundle_id):
    st.header("ğŸ”¤ Normalize â€“ í…ìŠ¤íŠ¸ ì •ê·œí™”")

    df = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    if df.empty:
        st.info("âš ï¸ ë¨¼ì € Ingest ë‹¨ê³„ì—ì„œ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return

    # ì‹¤ì œ ì „ì²˜ë¦¬ í˜¸ì¶œ
    df["normalized"] = df["text"].astype(str).apply(cleaner.normalize)
    df["sentences"] = df["normalized"].apply(tokenizer.split_sentences)

    # ë¹„êµ í…Œì´ë¸”
    st.subheader("ì›ë¬¸ vs ì •ê·œí™” í…ìŠ¤íŠ¸")
    st.dataframe(df[["text", "normalized"]].head(10), use_container_width=True)

    # ë¬¸ì¥ ìˆ˜ ë¶„í¬
    df["sent_count"] = df["sentences"].apply(len)
    fig = px.histogram(df, x="sent_count", nbins=20, title="ë¬¸ì¥ ìˆ˜ ë¶„í¬")
    st.plotly_chart(fig, use_container_width=True)

    # ë‹¤ìš´ë¡œë“œ
    st.subheader("ğŸ“¥ ì •ê·œí™” ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
    st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", data=csv_buffer.getvalue(),
                       file_name=f"{bundle_id}_normalized.csv", mime="text/csv")

    jsonl_buffer = io.StringIO()
    for _, row in df.iterrows():
        jsonl_buffer.write(json.dumps(row.to_dict(), ensure_ascii=False) + "\n")
    st.download_button("â¬‡ï¸ JSONL ë‹¤ìš´ë¡œë“œ", data=jsonl_buffer.getvalue(),
                       file_name=f"{bundle_id}_normalized.jsonl", mime="application/json")

    save_jsonl(df, f"data/{bundle_id}/normalize.jsonl")
    st.success("âœ… ì •ê·œí™” ë°ì´í„° ì €ì¥ ì™„ë£Œ")

# ==========================
# 3) Analyze (í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/ì›Œë“œí´ë¼ìš°ë“œ)
# ==========================
def extract_keywords(texts, topn=20):
    words = " ".join(texts).split()
    counter = Counter(words)
    return pd.DataFrame(counter.most_common(topn), columns=["word", "freq"])

def page_analyze(bundle_id: str):
    """
    ğŸ” Analyze â€“ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/í‚¤ì›Œë“œ/ì›Œë“œí´ë¼ìš°ë“œ(+ê°ì • ì˜ˆì‹œ, ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬)
    - frame_tagger.tag() : ê¸°ì‚¬ë³„ í”„ë ˆì„ íƒœê¹…
    - hidden_network_detector.detect(sentences) : ê¸°ì‚¬ë³„ & ì „ì²´ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬
    - í•œê¸€ ì›Œë“œí´ë¼ìš°ë“œ: í°íŠ¸ ìë™íƒìƒ‰+ê²€ì¦, í•œê¸€ í† í°ë§Œ ì¶”ì¶œ
    - ê²°ê³¼ ì €ì¥: data/{bundle_id}/analyze.jsonl
    """
    import os, platform, re
    from collections import Counter
    from PIL import ImageFont
    from matplotlib import font_manager, rcParams
    from wordcloud import WordCloud
    import plotly.express as px
    import matplotlib.pyplot as plt

    # ------------------------
    # ë‚´ë¶€ ìœ í‹¸ (ì´ í•¨ìˆ˜ ì•ˆì—ì„œë§Œ ì‚¬ìš©)
    # ------------------------
    KO_FONT_CANDIDATES = {
        "Windows": [
            r"C:\Windows\Fonts\malgun.ttf",
            r"C:\Windows\Fonts\malgunbd.ttf",
            r"C:\Windows\Fonts\NanumGothic.ttf",
        ],
        "Darwin": [  # macOS
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ],
        "Linux": [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        ],
    }

    def _font_supports_korean(font_path: str) -> bool:
        try:
            f = ImageFont.truetype(font_path, 24)
            return f.getlength("í•œê¸€í…ŒìŠ¤íŠ¸") > 0
        except Exception:
            return False

    def resolve_korean_font(custom_path: str | None = None) -> str | None:
        # 1) ì‚¬ìš©ìê°€ ì„¸ì…˜ì— ë„£ì–´ë‘” ê²½ë¡œ ìš°ì„ 
        if custom_path and os.path.exists(custom_path) and _font_supports_korean(custom_path):
            return custom_path
        # 2) OS í›„ë³´êµ° ìˆœíšŒ
        for p in KO_FONT_CANDIDATES.get(platform.system(), []):
            if os.path.exists(p) and _font_supports_korean(p):
                return p
        return None

    def set_matplotlib_korean(font_path: str | None):
        if not font_path:
            return
        try:
            font_manager.fontManager.addfont(font_path)
            fp = font_manager.FontProperties(fname=font_path)
            rcParams["font.family"] = fp.get_name()
        except Exception:
            pass

    KO_TOKEN = re.compile(r"[ê°€-í£]{2,}")  # 2ê¸€ì ì´ìƒ í•œê¸€ë§Œ

    def korean_tokens(texts: list[str], stopwords: set[str] | None = None) -> list[str]:
        stopwords = stopwords or set()
        toks = []
        for t in texts:
            if not isinstance(t, str):
                continue
            words = KO_TOKEN.findall(t)
            words = [w for w in words if len(w) >= 2 and w not in stopwords]
            toks.extend(words)
        return toks

    # ------------------------
    # ë°ì´í„° ë¡œë“œ
    # ------------------------
    st.header("ğŸ” Analyze â€“ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/í‚¤ì›Œë“œ")
    df = load_jsonl(f"data/{bundle_id}/normalize.jsonl")
    if df.empty:
        st.info("âš ï¸ ë¨¼ì € Normalize ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í™•ë³´
    if "normalized" not in df.columns:
        df["normalized"] = df.get("text", "").astype(str)

    # ë¬¸ì¥ ì»¬ëŸ¼ ë³´ê°•
    if "sentences" not in df.columns:
        df["sentences"] = df["normalized"].apply(tokenizer.split_sentences)

    # ------------------------
    # í”„ë ˆì„ íƒœê¹… (ê¸°ì‚¬ë³„)
    # ------------------------
    df["frames"] = df["normalized"].apply(frame_tagger.tag)

    # ------------------------
    # ë„¤íŠ¸ì›Œí¬ íƒì§€ (ê¸°ì‚¬ë³„ + ì „ì²´(Global))
    # ------------------------
    st.subheader("ğŸ•¸ï¸ ë„¤íŠ¸ì›Œí¬ íƒì§€ ì˜µì…˜")
    min_count = st.slider("ìµœì†Œ ë“±ì¥ ë¹ˆë„(min_count)", 1, 20, 5, 1)
    sw_text = st.text_area(
        "ë¶ˆìš©ì–´(ì‰¼í‘œë¡œ êµ¬ë¶„, ì¶”ê°€)",
        value="ê¸°ì‚¬ì›ë¬¸,ì…ë ¥,ì˜¤í›„,ì‚¬ì§„,ì—°í•©ë‰´ìŠ¤,YTN,newsis,kmn,ì„œë¹„ìŠ¤,ë³´ë‚´ê¸°,ë³€ê²½í•˜ê¸°,ì‚¬ìš©í•˜ê¸°,ê´€ë ¨,ëŒ€í•œ,ë³¸ë¬¸,ê¸€ì,ìˆ˜ì •,ë³€í™˜",
        height=80
    )
    custom_sw = {w.strip() for w in sw_text.split(",") if w.strip()}

    # ---- ê¸°ì‚¬ë³„ ë„¤íŠ¸ì›Œí¬ ê³„ì‚° (ì˜µì…˜ ì ìš©) ----
    df["network"] = df["sentences"].apply(lambda s: _detect_network_compat(s, min_count, custom_sw))
    df["net_nodes"] = df["network"].apply(lambda g: len((g or {}).get("nodes", {})))
    df["net_edges"] = df["network"].apply(lambda g: len((g or {}).get("edges", {})))

    # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬
    all_sents = [sent for sents in df["sentences"] for sent in (sents or [])]
    global_net = _detect_network_compat(all_sents, min_count, custom_sw)
    # st.subheader("ğŸ•¸ï¸ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í”„ë¦¬ë·°")
    # st.json(global_net)

    # # (í”„ë¦¬ë·° í‘œë„ ì›í•˜ë©´)
    # top_nodes = sorted(global_net["nodes"].items(), key=lambda x: x[1], reverse=True)[:20]
    # st.dataframe(pd.DataFrame(top_nodes, columns=["node","count"]))
    # ------------------------
    # í‚¤ì›Œë“œ Top-N (í•œê¸€ ì „ìš©)
    # ------------------------
    st.subheader("ğŸ“‘ í‚¤ì›Œë“œ Top-N (í•œê¸€ ì „ìš©)")
    stopwords = {"ê¸°ì", "ì—°í•©ë‰´ìŠ¤", "ë‰´ìŠ¤", "ë‹¨ë…", "ì†ë³´", "ì¢…í•©", "ì‚¬ì§„"}
    toks = korean_tokens(df["normalized"].fillna("").tolist(), stopwords=stopwords)
    kw_df = pd.DataFrame(Counter(toks).most_common(30), columns=["word", "freq"])
    st.dataframe(kw_df.head(20), use_container_width=True)

    # ------------------------
    # ê°ì • ë¶„í¬ (ì˜ˆì‹œ ë°ì´í„° ìœ ì§€)
    # ------------------------
    st.subheader("ğŸ“Š ê°ì • ë¶„í¬ (ì˜ˆì‹œ)")
    fig = px.pie(values=[40, 30, 20, 10], names=["ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½", "ê¸°íƒ€"], title="ê°ì • ë¹„ìœ¨ (ìƒ˜í”Œ)")
    st.plotly_chart(fig, use_container_width=True)

    # ------------------------
    # ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€ í°íŠ¸ ìë™íƒìƒ‰ + ê²€ì¦)
    # ------------------------
    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€)")
    # ì„¸ì…˜ì— í°íŠ¸ ê²½ë¡œ ì €ì¥í•´ë‘” ê²½ìš° ìš°ì„  ì‚¬ìš© (ì—†ìœ¼ë©´ ìë™íƒìƒ‰)
    custom_font = st.session_state.get("kofont") if "kofont" in st.session_state else ""
    font_path = resolve_korean_font(custom_font or None)

    if not font_path:
        st.error("í•œê¸€ ì§€ì› í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆ) Windows: C:\\Windows\\Fonts\\malgun.ttf")
    else:
        set_matplotlib_korean(font_path)
        corpus = " ".join(toks)
        if corpus.strip():
            wc = WordCloud(
                width=1000,
                height=500,
                background_color="white",
                font_path=font_path,
                prefer_horizontal=1.0,
                collocations=False
            ).generate(corpus)
            fig2, ax2 = plt.subplots(figsize=(10, 5))
            ax2.imshow(wc, interpolation="bilinear")
            ax2.axis("off")
            st.caption(f"font: {font_path}")
            st.pyplot(fig2, clear_figure=True)
        else:
            st.warning("ìœ íš¨í•œ í•œê¸€ í† í°ì´ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ------------------------
    # í”„ë¦¬ë·°(ê¸°ì‚¬ë³„/ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬) & ì €ì¥
    # ------------------------
    st.subheader("ğŸ•¸ï¸ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í”„ë¦¬ë·°")
    st.json(global_net)

    st.subheader("ğŸ§¾ ìƒ˜í”Œ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬")
    preview_cols = [c for c in ["url", "title", "frames", "net_nodes", "net_edges"] if c in df.columns]
    st.dataframe(df[preview_cols].head(5), use_container_width=True)

    save_jsonl(df, f"data/{bundle_id}/analyze.jsonl")
    st.success("âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")


# ==========================
# 4) Gate (í”„ë ˆì„â†’ì§€í‘œì ìˆ˜)
# ==========================
def page_gate(bundle_id: str):
    st.header("ğŸšª Gate â€“ í”„ë ˆì„ ê¸°ë°˜ ì²´í¬(Indicator)")

    df = load_jsonl(f"data/{bundle_id}/analyze.jsonl")
    if df.empty or "frames" not in df.columns:
        st.warning("ë¨¼ì € Analyze ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ì‹¤ì œ indicator_scorer í˜¸ì¶œ
    df["indicator_score"] = df["frames"].apply(indicator_scorer.run)
    st.subheader("í”„ë ˆì„ & ì§€í‘œ ì ìˆ˜")
    st.dataframe(df[["normalized", "frames", "indicator_score"]].head(), use_container_width=True)

    save_jsonl(df, f"data/{bundle_id}/gate.jsonl")
    st.success("âœ… Gate ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# ==========================
# 5) Scoring (DBN + Fused)
# ==========================
def page_scoring(bundle_id: str):
    st.header("ğŸ“Š Scoring â€“ DBN ì¶”ë¡  + ìœµí•© ì ìˆ˜ + ì˜í–¥/ì‹œí”„íŠ¸")

    df = load_jsonl(f"data/{bundle_id}/gate.jsonl")
    if df.empty or "frames" not in df.columns or "indicator_score" not in df.columns:
        st.warning("ë¨¼ì € Gate ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    ind_scores, dbn_scores, fused_scores = [], [], []
    impacts, shifts = [], []
    history = []  # ì§ì „ í”„ë ˆì„ ìŠ¤ëƒ…ìƒ· ëˆ„ì 

    for _, row in df.iterrows():
        frames = row.get("frames", [])
        ind = float(row.get("indicator_score", 0.0))

        # DBN(ê°„ì´): ì§ì „ ìŠ¤ëƒ…ìƒ·ê³¼ ë¹„êµ
        prev_frames = history[-1] if history else None
        dbn_score = dbn_inference.run(frames, prev_frames=prev_frames)

        # Fused
        fused = fuse.combine([ind, dbn_score])

        # âœ… ì˜í–¥ ë¶„ì„(impact_analyzer)
        imp = impact_analyzer.run(frames, fused)

        # âœ… í”„ë ˆì„ ì‹œí”„íŠ¸(frame_shift_detector)
        if history:
            sh = frame_shift_detector.detect(history + [frames])
        else:
            sh = {"shift_detected": False, "shift_score": 0.0, "changed": []}

        # ëˆ„ì 
        history.append(frames)
        ind_scores.append(ind); dbn_scores.append(dbn_score); fused_scores.append(fused)
        impacts.append(imp); shifts.append(sh)

    df["dbn_score"]   = dbn_scores
    df["fused_score"] = fused_scores
    df["impact"]      = impacts
    df["shift"]       = shifts

    st.subheader("ì ìˆ˜ & ë¶„ì„ ìš”ì•½")
    st.dataframe(df[["indicator_score","dbn_score","fused_score","impact","shift"]].head(), use_container_width=True)

    # ì‹œê³„ì—´
    df["date"] = pd.to_datetime(df.get("date", datetime.today().strftime("%Y-%m-%d")))
    fig = px.line(df, x="date", y=["indicator_score", "dbn_score", "fused_score"], title="ì ìˆ˜ ì‹œê³„ì—´")
    st.plotly_chart(fig, use_container_width=True)

    save_jsonl(df, f"data/{bundle_id}/scoring.jsonl")
    st.success("âœ… Scoring ê²°ê³¼ ì €ì¥ ì™„ë£Œ")


# ==========================
# 6) Fusion (ì‹œê° ë¹„êµ)
# ==========================
def page_fusion(bundle_id: str):
    st.header("âš¡ Fusion â€“ ì ìˆ˜ ë¹„êµ/ê²€ì¦")

    df = load_jsonl(f"data/{bundle_id}/scoring.jsonl")
    if df.empty or "fused_score" not in df.columns:
        st.warning("ë¨¼ì € Scoring ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    fig = px.scatter(df, x="indicator_score", y="fused_score", trendline="ols",
                     title="Indicator vs Fused")
    st.plotly_chart(fig, use_container_width=True)

    st.dataframe(df[["indicator_score", "dbn_score", "fused_score"]].head(), use_container_width=True)
    save_jsonl(df, f"data/{bundle_id}/fusion.jsonl")
    st.success("âœ… Fusion ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# ==========================
# 7) Blocks (ì£¼ í”„ë ˆì„ ê¸°ë°˜ ë¸”ë¡)
# ==========================
def _primary_frame(fs: list[dict]) -> str:
    if not fs:
        return "General"
    # score ê°€ì¥ í° í”„ë ˆì„
    best = sorted(fs, key=lambda f: f.get("score", 0), reverse=True)[0]
    return best.get("frame", "General")

def page_blocks(bundle_id: str):
    st.header("ğŸ§© Blocks (EDS) â€“ ê¸°ì‚¬ â†” ë¸”ë¡ ë§¤í•‘")

    df = load_jsonl(f"data/{bundle_id}/fusion.jsonl")
    if df.empty or "frames" not in df.columns:
        st.warning("ë¨¼ì € Fusion ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    df["block"] = df["frames"].apply(_primary_frame)

    # âœ… í•µì‹¬ ì»¬ëŸ¼ ë³´ì¡´ ë³´ê°•
    keep_cols = [c for c in ["url","date","normalized","frames","indicator_score","dbn_score","fused_score"] if c in df.columns]
    out = df[keep_cols + ["block"]].copy()

    st.subheader("ê¸°ì‚¬ â†” ë¸”ë¡ ë§¤í•‘")
    st.dataframe(out.head(), use_container_width=True)

    save_jsonl(out, f"data/{bundle_id}/blocks.jsonl")
    st.success("âœ… Blocks ì €ì¥ ì™„ë£Œ")

# ==========================
# 8) Scenarios (ë§¤ì¹­ + ì˜ˆì¸¡)
# ==========================
def page_scenarios(bundle_id: str):
    st.header("ğŸ“‘ Scenarios â€“ ë§¤ì¹­/ì˜ˆì¸¡")

    df = load_jsonl(f"data/{bundle_id}/blocks.jsonl")
    if df.empty or "normalized" not in df.columns:
        st.warning("ë¨¼ì € Blocks ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ì‹¤ì œ ì„ë² ë”© â†’ ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­ & ì˜ˆì¸¡
    matched_list, predicted_list = [], []
    for _, row in df.iterrows():
        sents = row.get("sentences", [])
        if not sents:
            sents = tokenizer.split_sentences(row.get("normalized", ""))
        vecs = embedder.embed(sents)
        matched = scenario_matcher.match(vecs, top_k=3)
        pred = scenario_predictor.generate(row.get("normalized", ""))
        matched_list.append(matched)
        predicted_list.append(pred)

    df["scenario_matched"] = matched_list
    df["scenario_predicted"] = predicted_list

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ìœ„í—˜ë„
    df["top_sim"] = df["scenario_matched"].apply(lambda xs: xs[0]["similarity"] if xs else 0.0)
    top = df[["block", "fused_score", "top_sim"]].groupby("block").mean().reset_index()
    st.subheader("ë¸”ë¡ë³„ í‰ê·  ì ìˆ˜/ìœ ì‚¬ë„")
    st.dataframe(top, use_container_width=True)

    save_jsonl(df, f"data/{bundle_id}/scenarios.jsonl")
    st.success("âœ… Scenarios ì €ì¥ ì™„ë£Œ")

# ==========================
# 9) Alerts (ìµœì¢… ê²½ë³´)
# ==========================
def page_alerts(bundle_id: str):
    st.header("ğŸš¨ Alerts â€“ ê²½ë³´ ë°œìƒ")

    df = load_jsonl(f"data/{bundle_id}/scenarios.jsonl")
    if df.empty or "fused_score" not in df.columns:
        st.warning("ë¨¼ì € Scenarios ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # âœ… LLM Judge ì„¤ëª… ìƒì„±(ì ìˆ˜+í”„ë ˆì„)
    def _scores_of(r):
        return {
            "indicator": float(r.get("indicator_score", 0.0)),
            "dbn":       float(r.get("dbn_score", 0.0)),
            "fused":     float(r.get("fused_score", 0.0)),
        }

    df["explanation"] = df.apply(lambda r: llm_judge.explain(_scores_of(r), r.get("frames", [])), axis=1)

    # âœ… ìµœì¢… ê²½ë³´ ê²°ì •(ê¸°ì¡´ ìœ ì§€)
    df["decision"] = df["fused_score"].apply(alert_decider.decide)

    def to_level(dec: str) -> str:
        if "High" in dec: return "High"
        if "Medium" in dec: return "Medium"
        return "Low"
    df["alert_level"] = df["decision"].apply(to_level)

    st.subheader("ê²½ë³´ ê²°ê³¼")
    st.dataframe(df[["normalized","fused_score","decision","alert_level","explanation"]].head(), use_container_width=True)

    fig = px.histogram(df, x="alert_level", title="ê²½ë³´ ë¶„í¬")
    st.plotly_chart(fig, use_container_width=True)

    save_jsonl(df, f"data/{bundle_id}/alerts.jsonl")
    st.success("âœ… Alerts ì €ì¥ ì™„ë£Œ")


# ==========================
# 10) Event Blocks (ê°„ì´ í´ëŸ¬ìŠ¤í„°ë§)
# ==========================
def page_eventblocks(bundle_id: str):
    st.header("ğŸ“¦ Event Blocks â€“ í´ëŸ¬ìŠ¤í„°ë§(ê°„ì´)")

    df = load_jsonl(f"data/{bundle_id}/alerts.jsonl")
    if df.empty:
        st.warning("ë¨¼ì € Alerts ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ê°„ì´ í´ëŸ¬ìŠ¤í„°
    df["cluster"] = df.index % 3
    pivot = df.groupby("cluster")["fused_score"].mean().reset_index()

    st.subheader("í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ìœ„í—˜ë„")
    st.dataframe(pivot, use_container_width=True)

    fig = px.imshow([pivot["fused_score"].tolist()],
                    labels=dict(x="Cluster", y="Risk", color="Score"),
                    title="Event Block ìœ„í—˜ë„ íˆíŠ¸ë§µ")
    st.plotly_chart(fig, use_container_width=True)

    save_jsonl(df, f"data/{bundle_id}/eventblocks.jsonl")
    st.success("âœ… Event Blocks ì €ì¥ ì™„ë£Œ")

# ==========================
# 11) Ledger (íŒŒì¼ ë¡œê·¸)
# ==========================
def page_ledger(bundle_id: str):
    st.header("ğŸ“œ Ledger â€“ ë‹¨ê³„ë³„ ë¡œê·¸")

    files = [
        "ingest.jsonl","normalize.jsonl","analyze.jsonl","gate.jsonl",
        "scoring.jsonl","fusion.jsonl","blocks.jsonl","scenarios.jsonl",
        "alerts.jsonl","eventblocks.jsonl"
    ]
    logs = []
    for f in files:
        path = f"data/{bundle_id}/{f}"
        if os.path.exists(path):
            logs.append({"file": f, "size": os.path.getsize(path)})
    st.dataframe(pd.DataFrame(logs), use_container_width=True)

    col1, col2 = st.columns(2)
    if col1.button("ğŸ“‚ alerts.jsonl ì—´ê¸°(viewer)"):
        try:
            st.dataframe(viewer.load(bundle_id, "alerts.jsonl"), use_container_width=True)
        except Exception as e:
            st.warning(f"viewer.load ì‹¤íŒ¨: {e}")

    if col2.button("ğŸ“¥ ledger.jsonl ì €ì¥"):
        ledger = {"bundle": bundle_id, "timestamp": datetime.now().isoformat(), "steps": files}
        save_jsonl(pd.DataFrame([ledger]), f"data/{bundle_id}/ledger.jsonl")
        st.success("ledger.jsonl ì €ì¥ ì™„ë£Œ")


# ==========================
# Overview (ëŒ€ì‹œë³´ë“œ)
# ==========================
def page_overview(bundle_id: str):
    st.title("ğŸŒ Crisis Overview")

    # --- ë°ì´í„° ë¡œë“œ ---
    df_ingest  = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    df_norm    = load_jsonl(f"data/{bundle_id}/normalize.jsonl")
    df_ana     = load_jsonl(f"data/{bundle_id}/analyze.jsonl")
    df_gate    = load_jsonl(f"data/{bundle_id}/gate.jsonl")
    df_score   = load_jsonl(f"data/{bundle_id}/scoring.jsonl")
    df_fuse    = load_jsonl(f"data/{bundle_id}/fusion.jsonl")
    df_blocks  = load_jsonl(f"data/{bundle_id}/blocks.jsonl")
    df_scen    = load_jsonl(f"data/{bundle_id}/scenarios.jsonl")  # âœ… ë³µêµ¬
    df_alerts  = load_jsonl(f"data/{bundle_id}/alerts.jsonl")

    # --- KPI: ìˆ˜ì§‘ vs ê²½ë³´ & ì²˜ë¦¬ìœ¨ ---
    n_ingest = 0 if df_ingest.empty else len(df_ingest)
    n_alerts = 0 if df_alerts.empty else len(df_alerts)
    ratio    = (n_alerts / n_ingest) if n_ingest else 0.0

    k1, k2, k3 = st.columns(3)
    k1.metric("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ (ingest)", n_ingest)
    k2.metric("ê²½ë³´ ì‚°ì¶œ ê¸°ì‚¬ ìˆ˜ (alerts)", n_alerts)
    k3.metric("ì²˜ë¦¬ìœ¨", f"{ratio*100:.0f}%")
    st.progress(min(1.0, ratio))

        # --- ğŸ“¡ ì‹œê³„ì—´ ê²½ë³´(Alerts) ---
    st.subheader("ğŸ“¡ ì‹œê³„ì—´ ê²½ë³´ (Alerts)")

    if not df_alerts.empty and {"date","alert_level"}.issubset(df_alerts.columns):
        freq_label = st.selectbox("ì§‘ê³„ ì£¼ê¸°", ["ì¼", "ì£¼", "ì›”"], index=0, key="alert_ts_freq")
        freq_map = {"ì¼":"D","ì£¼":"W","ì›”":"M"}
        freq = freq_map[freq_label]

        ts_long = _alert_timeseries(df_alerts, freq=freq)
        ts_wide = _to_wide(ts_long)
        ts_wide = ts_wide.copy()
        ts_wide["Total"] = ts_wide["High"] + ts_wide["Medium"] + ts_wide["Low"]

        # ìŠ¤ë¬´ë”© ì˜µì…˜(rolling)
        smooth = st.checkbox("7-ì°½ ìŠ¤ë¬´ë”©(rolling mean)", value=(freq=="D"), key="alert_ts_smooth")
        if smooth and not ts_wide.empty:
            ts_wide_sm = ts_wide.rolling(window=7, min_periods=1).mean()
        else:
            ts_wide_sm = ts_wide

        # íƒ­: ìŠ¤íƒë©´ì  / ë ˆë²¨ì„  / ëˆ„ì  / í‘œ
        tabs_ts = st.tabs(["ğŸŸ£ ìŠ¤íƒ ë©´ì ", "ğŸ“ˆ ë ˆë²¨ë³„ ì„ ", "â• ëˆ„ì  í•©", "ğŸ“„ í‘œ"])

        with tabs_ts[0]:
            # long-formì´ areaì— ì í•©
            fig_area = px.area(
                ts_long, x="date", y="count", color="alert_level",
                title=f"ê²½ë³´ ìˆ˜ ìŠ¤íƒ ë©´ì  ({freq_label} ë‹¨ìœ„)",
                color_discrete_sequence=next_palette()
            )
            st.plotly_chart(fig_area, use_container_width=True)

        with tabs_ts[1]:
            df_plot = reset_index_as_date(ts_wide_sm)
            fig_line = px.line(
                df_plot, x="date", y=["High","Medium","Low"],
                title=f"ê²½ë³´ ë ˆë²¨ë³„ ì¶”ì„¸ ({freq_label} ë‹¨ìœ„){' - 7ì°½ ìŠ¤ë¬´ë”©' if smooth else ''}",
                color_discrete_sequence=next_palette()
            )
            st.plotly_chart(fig_line, use_container_width=True)

        with tabs_ts[2]:
            df_cum  = reset_index_as_date(ts_wide.cumsum())
            fig_cum = px.line(
                df_cum, x="date", y=["High","Medium","Low","Total"],
                title=f"ê²½ë³´ ëˆ„ì  í•© ({freq_label} ë‹¨ìœ„)",
                color_discrete_sequence=next_palette()
            )
            st.plotly_chart(fig_cum, use_container_width=True)

        with tabs_ts[3]:
            # í‘œëŠ” ì›ì‹œ wide + Total ì œê³µ
            st.dataframe(ts_wide.reset_index(), use_container_width=True)
    else:
        st.info("ì‹œê³„ì—´ ê²½ë³´ë¥¼ ìœ„í•´ì„œëŠ” df_alertsì— 'date'ì™€ 'alert_level' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    
    # --- í‰ê·  ìœ„í—˜ë„ & ì¶”ì„¸ ---
    if not df_alerts.empty:
        df_alerts = _ensure_fused(df_alerts, bundle_id)
        avg_score = df_alerts["fused_score"].mean()
        st.caption(f"í‰ê·  ìœ„í—˜ë„(Alerts ê¸°ì¤€): {avg_score:.2f}")

        if "date" in df_alerts.columns:
            try:
                df_alerts["date"] = pd.to_datetime(df_alerts["date"]).dt.date
                trend = df_alerts.groupby("date").size().reset_index(name="count")
                tabs_trend = st.tabs(["ğŸ“ˆ ì•Œë¦¼ ì¶”ì„¸(ì¼ë³„ ê±´ìˆ˜)", "ğŸ“‰ ì¼í‰ê·  ìœ„í—˜ë„"])
                with tabs_trend[0]:
                    fig_trend = px.line(
                        trend, x="date", y="count", markers=True,
                        title="ğŸ“ˆ ìµœê·¼ ì•Œë¦¼ ì¶”ì„¸(ì¼ë³„ ê±´ìˆ˜)",
                        color_discrete_sequence=next_palette()
                    )
                    st.plotly_chart(fig_trend, use_container_width=True)
                with tabs_trend[1]:
                    daily = df_alerts.groupby("date")["fused_score"].mean().reset_index()
                    fig_avg = px.line(
                        daily, x="date", y="fused_score", markers=True,
                        title="ğŸ“‰ ì¼í‰ê·  ìœ„í—˜ë„(fused_score)",
                        color_discrete_sequence=next_palette()
                    )
                    st.plotly_chart(fig_avg, use_container_width=True)
            except Exception:
                pass

        # ê²½ë³´ ë“±ê¸‰ ë¶„í¬ (íƒ­)
        if "alert_level" in df_alerts.columns:
            lvl = df_alerts["alert_level"].value_counts().reset_index()
            lvl.columns = ["level", "count"]
            lvl_tabs = st.tabs(["ğŸ“Š Bar", "ğŸ¥§ Pie", "ğŸ“„ í‘œ"])
            with lvl_tabs[0]:
                fig_lvl_bar = px.bar(
                    lvl, x="level", y="count", text="count",
                    title="ğŸš¨ ê²½ë³´ ë“±ê¸‰ ë¶„í¬ (Bar)",
                    color_discrete_sequence=next_palette()
                )
                fig_lvl_bar.update_traces(textposition="outside")
                st.plotly_chart(fig_lvl_bar, use_container_width=True)
            with lvl_tabs[1]:
                fig_lvl_pie = px.pie(
                    lvl, names="level", values="count",
                    title="ğŸš¨ ê²½ë³´ ë“±ê¸‰ ë¶„í¬ (Pie)",
                    color_discrete_sequence=next_palette()
                )
                st.plotly_chart(fig_lvl_pie, use_container_width=True)
            with lvl_tabs[2]:
                st.dataframe(lvl, use_container_width=True)
    else:
        st.info("Alerts ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ë¡œ ìƒì„± ê°€ëŠ¥)")

    # --- Top ìœ„í—˜ ë¸”ë¡ (ë°±í•„ í¬í•¨, íƒ­) ---
    st.subheader("ğŸ”¥ Top ìœ„í—˜ ë¸”ë¡")
    if not df_blocks.empty:
        df_blocks = backfill_fused_score(df_blocks, bundle_id)
        if "fused_score" in df_blocks.columns and "block" in df_blocks.columns:
            top_blk = (df_blocks.groupby("block")["fused_score"]
                       .mean().reset_index()
                       .sort_values("fused_score", ascending=False)
                       .head(10))
            tabs_blk = st.tabs(["ğŸ“Š ì°¨íŠ¸", "ğŸ“„ í‘œ"])
            with tabs_blk[0]:
                fig_blk = px.bar(
                    top_blk, x="block", y="fused_score", text="fused_score",
                    title="ë¸”ë¡ë³„ í‰ê·  ìœ„í—˜ë„ (Top 10)",
                    labels={"block": "ë¸”ë¡", "fused_score": "í‰ê·  ìœ„í—˜ë„"},
                    color_discrete_sequence=next_palette()
                )
                fig_blk.update_traces(texttemplate="%{text:.2f}", textposition="outside")
                st.plotly_chart(fig_blk, use_container_width=True)
            with tabs_blk[1]:
                st.dataframe(top_blk, use_container_width=True)
        else:
            st.info("ë¸”ë¡ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    else:
        st.info("ë¸”ë¡ ë°ì´í„° ì—†ìŒ")

    # --- ğŸ§­ ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ (í‘œë§Œ) ---
    st.subheader("ğŸ§­ ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ (í‘œ)")
    if not df_scen.empty:
        df_scen = _ensure_fused(df_scen, bundle_id)

        # ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì¶”ì¶œ ìœ í‹¸
        def _scen_name(v):
            if isinstance(v, dict):
                return v.get("scenario") or v.get("title") or str(v)
            if isinstance(v, list) and v:
                first = v[0]
                if isinstance(first, dict):
                    return first.get("title") or first.get("scenario") or str(first)
                return str(first)
            if isinstance(v, str):
                return v
            return "Unknown"

        scen_col = "scenario_predicted" if "scenario_predicted" in df_scen.columns else (
            "scenario_matched" if "scenario_matched" in df_scen.columns else None
        )

        if scen_col:
            tmp = df_scen.copy()
            tmp["scenario_name"] = tmp[scen_col].apply(_scen_name)

            # í‰ê·  ìœ„í—˜ë„ ê¸°ì¤€ Top-N í‘œ
            topn = st.slider("í‘œì‹œ ê°œìˆ˜(Top-N)", 5, 50, 20, 5, key="topn_scen")
            top_scen = (tmp.groupby("scenario_name")["fused_score"]
                        .mean().reset_index()
                        .sort_values("fused_score", ascending=False)
                        .head(topn))
            st.dataframe(
                top_scen.rename(columns={"scenario_name":"ì‹œë‚˜ë¦¬ì˜¤", "fused_score":"í‰ê·  ìœ„í—˜ë„"}),
                use_container_width=True
            )

            # ìµœê·¼ Nê±´ ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°(ì˜µì…˜)
            with st.expander("ìµœê·¼ ì‹œë‚˜ë¦¬ì˜¤ ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                cols = [c for c in ["date","title","scenario_predicted","scenario_matched","fused_score"] if c in tmp.columns]
                st.dataframe(tmp.sort_values("date", ascending=False)[cols].head(topn), use_container_width=True)
        else:
            st.info("ì‹œë‚˜ë¦¬ì˜¤ ì»¬ëŸ¼(scenario_predicted / scenario_matched)ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("Scenarios ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ë¡œ ìƒì„± ê°€ëŠ¥)")


    # --- ğŸ—ºï¸ ëŒ€í•œë¯¼êµ­ ì‹œêµ°êµ¬ Choropleth (íƒ­) ---
    st.subheader("ğŸ—ºï¸ ëŒ€í•œë¯¼êµ­ ì‹œêµ°êµ¬ ë¶„í¬ (Choropleth)")
    geo_tabs = st.tabs(["ğŸ—ºï¸ ì§€ë„", "ğŸ“„ í‘œ"])
    with geo_tabs[0]:
        geo_path = st.session_state.get("siggeo", "")
        geo = load_geojson(geo_path) if geo_path else None
        if not df_alerts.empty and geo is not None:
            # df_alertsì— sig_cd í™•ë³´ (ì—†ìœ¼ë©´ region/sigunguì—ì„œ ìœ ì¶”)
            df_geo = ensure_sig_cd(df_alerts.copy(), geo)
            if "sig_cd" in df_geo.columns and df_geo["sig_cd"].notna().any():
                agg = (df_geo.dropna(subset=["sig_cd"])
                            .groupby("sig_cd")
                            .size().reset_index(name="count"))
                code_key, _name_key = _detect_sig_props(geo)
                featureidkey = f"properties.{code_key}" if code_key else None
                fig_ch = px.choropleth(
                    agg, geojson=geo, locations="sig_cd", featureidkey=featureidkey,
                    color="count", color_continuous_scale=next_cscale(),
                    title="ì‹œêµ°êµ¬ë³„ ê²½ë³´ ê¸°ì‚¬ ë¶„í¬"
                )
                fig_ch.update_geos(fitbounds="locations", visible=False)
                st.plotly_chart(fig_ch, use_container_width=True)
            else:
                st.info("df_alertsì— sig_cdë¥¼ ìœ ì¶”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'sigungu' ë˜ëŠ” 'region' ì»¬ëŸ¼ì„ ì±„ìš°ê±°ë‚˜, dfì— 'sig_cd'(5ìë¦¬ í–‰ì •ì½”ë“œ)ë¥¼ ì§ì ‘ ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            st.info("GeoJSON ê²½ë¡œê°€ ë¹„ì—ˆê±°ë‚˜ Alerts ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ GeoJSON ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
    with geo_tabs[1]:
        # í‘œëŠ” ì§‘ê³„ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
        geo_path = st.session_state.get("siggeo", "")
        geo = load_geojson(geo_path) if geo_path else None
        if not df_alerts.empty and geo is not None:
            df_geo = ensure_sig_cd(df_alerts.copy(), geo)
            if "sig_cd" in df_geo.columns and df_geo["sig_cd"].notna().any():
                st.dataframe(
                    df_geo.dropna(subset=["sig_cd"]).groupby("sig_cd").size().reset_index(name="count").sort_values("count", ascending=False),
                    use_container_width=True
                )
            else:
                st.info("ì§‘ê³„ í‘œë¥¼ ë§Œë“¤ sig_cdê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("GeoJSON/Alerts ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # --- ğŸ·ï¸ ì—”í‹°í‹° ìƒìœ„ Top-N (analyze.jsonlì˜ network ê¸°ë°˜) ---
    st.subheader("ğŸ·ï¸ ì—”í‹°í‹° ìƒìœ„ Top-N")
    ent_tabs = st.tabs(["ğŸ“Š ì°¨íŠ¸", "ğŸ“„ í‘œ"])
    if not df_ana.empty and "network" in df_ana.columns:
        from collections import Counter
        counter = Counter()
        for g in df_ana["network"].dropna():
            try:
                nodes = (g or {}).get("nodes", {})
                counter.update({k: int(v) for k, v in nodes.items()})
            except Exception:
                continue
        topn = st.slider("í‘œì‹œ ê°œìˆ˜", 5, 50, 20, 5)
        ent_df = pd.DataFrame(counter.most_common(topn), columns=["entity","count"])
        with ent_tabs[0]:
            fig_ent = px.bar(
                ent_df, x="entity", y="count", text="count",
                title=f"ì—”í‹°í‹° ìƒìœ„ Top-{topn}",
                color_discrete_sequence=next_palette()
            )
            fig_ent.update_traces(textposition="outside")
            st.plotly_chart(fig_ent, use_container_width=True)
        with ent_tabs[1]:
            st.dataframe(ent_df, use_container_width=True)
    else:
        with ent_tabs[0]:
            st.info("Analyze ë°ì´í„°ì— ë„¤íŠ¸ì›Œí¬ ì»¬ëŸ¼ì´ ì—†ì–´ ì—”í‹°í‹° ìƒìœ„ Top-Nì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        with ent_tabs[1]:
            st.empty()

    # --- ğŸ“° ìˆ˜ì§‘ ì†ŒìŠ¤ ë¶„í¬ (íƒ­) ---
    st.subheader("ğŸ“° ìˆ˜ì§‘ ì†ŒìŠ¤ ë¶„í¬")
    if not df_ingest.empty and "source" in df_ingest.columns:
        src = df_ingest["source"].fillna("Manual").value_counts().reset_index()
        src.columns = ["source", "count"]
        src_tabs = st.tabs(["ğŸ“Š ì°¨íŠ¸", "ğŸ“„ í‘œ"])
        with src_tabs[0]:
            fig_src = px.bar(
                src.head(30), x="source", y="count", text="count",
                title="ì–¸ë¡ ì‚¬/ë„ë©”ì¸ ë¶„í¬ (Top 30)",
                labels={"source": "ì†ŒìŠ¤", "count": "ê±´ìˆ˜"},
                color_discrete_sequence=next_palette()
            )
            fig_src.update_traces(textposition="outside")
            st.plotly_chart(fig_src, use_container_width=True)
        with src_tabs[1]:
            st.dataframe(src, use_container_width=True)
    else:
        st.info("Ingestì˜ source ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ ë²„íŠ¼ ---
    st.divider()
    colA, colB = st.columns([1, 3])
    with colA:
        if st.button("âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ (ingest â†’ alerts)"):
            ok = _quick_pipeline(bundle_id)
            if ok:
                st.success("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: ingest â†’ alerts ìƒì„±")
            else:
                st.warning("ingest ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            _safe_rerun()



# ==========================
# Main
# ==========================
def main():
    st.set_page_config(page_title="NWW Early Warning Dashboard", layout="wide")

    st.sidebar.header("âš™ï¸ ì„¤ì •")
    bundle_id = st.sidebar.text_input("Bundle ID", DEFAULT_BUNDLE)
    page = st.sidebar.radio("ğŸ“Œ ë‹¨ê³„ ì´ë™", [
        "Landing", "Ingest", "Normalize", "Analyze", "Gate", "Scoring",
        "Fusion", "Blocks", "Scenarios", "Alerts", "EventBlocks", "Ledger"
    ])
    
    # === [SIDEBAR] GeoJSON ì…ë ¥ & ì§„ë‹¨ ===
    import io, json, os

    st.sidebar.markdown("### ğŸ—ºï¸ ì‹œêµ°êµ¬ GeoJSON")
    geojson_file = st.sidebar.file_uploader("íŒŒì¼ ì—…ë¡œë“œ(.geojson/.json)", type=["geojson", "json"])
    geojson_path = st.sidebar.text_input("ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ì…ë ¥", value=st.session_state.get("siggeo_path", ""))

    # ìš°ì„ ìˆœìœ„: ì—…ë¡œë“œ > ê²½ë¡œ
    geo_obj = None
    diag = {"source": None, "exists": None, "size": None, "features": None, "code_key": None, "name_key": None, "error": None}

    try:
        if geojson_file is not None:
            raw = geojson_file.getvalue().decode("utf-8", "ignore")
            geo_obj = json.loads(raw)
            st.session_state["siggeo_obj"] = geo_obj
            st.session_state["siggeo_path"] = ""  # ê²½ë¡œ ë¹„í™œì„±
            diag["source"] = "uploaded"
        elif geojson_path.strip():
            path = os.path.abspath(os.path.expanduser(geojson_path.strip()))
            st.session_state["siggeo_path"] = path
            diag["source"] = f"path: {path}"
            diag["exists"] = os.path.exists(path)
            if diag["exists"]:
                diag["size"] = os.path.getsize(path)
                with open(path, "r", encoding="utf-8") as f:
                    geo_obj = json.load(f)
                st.session_state["siggeo_obj"] = geo_obj
            else:
                diag["error"] = "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        else:
            # ì´ì „ ì„¸ì…˜ì˜ ê°ì²´ ì¬ì‚¬ìš©
            geo_obj = st.session_state.get("siggeo_obj")
            if geo_obj is not None:
                diag["source"] = "session"
    except Exception as e:
        diag["error"] = f"{type(e).__name__}: {e}"

    # êµ¬ì¡° í‚¤ ìë™ íƒì§€
    def _detect_sig_props(geo: dict):
        if not geo or "features" not in geo or not geo["features"]:
            return None, None
        props = geo["features"][0].get("properties", {})
        candidates_code = ["SIG_CD", "adm_cd", "ADM_CD"]
        candidates_name = ["SIG_KOR_NM", "sig_kor_nm", "ADM_NM", "adm_nm", "EMD_KOR_NM"]
        code_key = next((k for k in candidates_code if k in props), None)
        name_key = next((k for k in candidates_name if k in props), None)
        return code_key, name_key

    if geo_obj:
        diag["features"] = len(geo_obj.get("features", []))
        diag["code_key"], diag["name_key"] = _detect_sig_props(geo_obj)

    with st.sidebar.expander("GeoJSON ì§„ë‹¨", expanded=True):
        st.write(diag)
        if geo_obj and (not diag["code_key"] or not diag["name_key"]):
            st.warning("âš ï¸ propertiesì— ì‹œêµ°êµ¬ ì½”ë“œ/ì´ë¦„ í‚¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆ: SIG_CD, SIG_KOR_NM")
        elif not geo_obj:
            st.info("GeoJSONì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")


    if page == "Landing":
        page_overview(bundle_id)
    elif page == "Ingest":
        page_ingest(bundle_id)
    elif page == "Normalize":
        page_normalize(bundle_id)
    elif page == "Analyze":
        page_analyze(bundle_id)
    elif page == "Gate":
        page_gate(bundle_id)
    elif page == "Scoring":
        page_scoring(bundle_id)
    elif page == "Fusion":
        page_fusion(bundle_id)
    elif page == "Blocks":
        page_blocks(bundle_id)
    elif page == "Scenarios":
        page_scenarios(bundle_id)
    elif page == "Alerts":
        page_alerts(bundle_id)
    elif page == "EventBlocks":
        page_eventblocks(bundle_id)
    elif page == "Ledger":
        page_ledger(bundle_id)

if __name__ == "__main__":
    main()
