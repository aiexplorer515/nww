# app_main.py (ì‹¤ì œ í˜¸ì¶œ ë²„ì „: Ingest â†’ Normalize â†’ Analyze â†’ Gate â†’ Scoring â†’ Fusion â†’ Blocks â†’ Scenarios â†’ Alerts)
from __future__ import annotations
import os
import io
import re
import json
from datetime import datetime
from collections import Counter

import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
from pathlib import Path
import plotly.express as px
import matplotlib.pyplot as plt
from importlib.util import spec_from_file_location, module_from_spec
import inspect
from pyvis.network import Network
import tempfile
from wordcloud import WordCloud
# íŒŒì¼ ìƒë‹¨ ì–´ë””ë“ (ê°€ëŠ¥í•˜ë©´ import ë°”ë¡œ ì•„ë˜)
try:
    from dotenv import load_dotenv, find_dotenv
    load_dotenv(find_dotenv(), override=False)   # .env ê°’ìœ¼ë¡œë§Œ ì±„ìš°ê³  ê¸°ì¡´ envëŠ” ë³´ì¡´
except Exception:
    pass



# --------------------------
# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
# --------------------------
from nwwpkg.ingest import news_collector
from nwwpkg.preprocess import cleaner, tokenizer, embedder
from nwwpkg.eds import frame_tagger
from nwwpkg.score import indicator_scorer, dbn_inference
from nwwpkg.fusion import fuse
from nwwpkg.scenario import scenario_matcher, scenario_predictor
from nwwpkg.analyze import impact_analyzer, frame_shift_detector, hidden_network_detector
from nwwpkg.judge import llm_judge
from nwwpkg.decider import alert_decider
from nwwpkg.ledger import recorder, viewer
from nwwpkg.utils.io import load_jsonl# from nwwpkg.ui.components.page_wrappers import (
from nwwpkg.eventblock import event_classifier, block_accumulator
from nwwpkg.score import crisis_detector
from nwwpkg.scenario import scenario_generator#     render_fusion, render_blocks, render_scenarios, render_ledger
from nwwpkg.network.network_analyze import draw_network
from nwwpkg.analyzer import block_analyzer
# )
from nwwpkg.ui.components.gate_indicator import render_gate_indicator
from nwwpkg.ui.components.scoring_page import render_scoring
from nwwpkg.ui.components.alerts_page import render_alerts as render_alerts_config
from nwwpkg.ui.components.eventblocks_page import render_eventblocks
from nwwpkg.ui.components.risk_snapshot import render_risk_snapshot
from nwwpkg.ui.components.risk_page import render_risk_page
from nwwpkg.ui.components.frames_eval import render_frames_eval
from nwwpkg.ui.components.analytics_basic import render_keywords_and_sentiment
from nwwpkg.ui.page_actor_ranking import page_analyze

# Dashboard ëª¨ë“ˆ import
from nwwpkg.ui.dashboard.pipeline_dag import render_pipeline_dag
from nwwpkg.ui.dashboard.stage_qa import render_stage_qa
from nwwpkg.ui.dashboard.alerts import render_alert as render_alert_dashboard, render_alerts_timeseries_inline
from nwwpkg.ui.dashboard.choropleth import render_choropleth
from nwwpkg.ui.dashboard.risk_blocks import render_risk_blocks
from nwwpkg.ui.dashboard.scenarios import render_scenarios
from nwwpkg.ui.dashboard.entities import render_entities
from nwwpkg.ui.dashboard.sources import render_sources


# --- Korean font resolver (Windows/Mac/Linux ì§€ì›) ---
import platform, os
from matplotlib import font_manager, rcParams

def get_korean_font_path(custom_path: str | None = None) -> str | None:
    if custom_path and os.path.exists(custom_path):
        return custom_path

    candidates = []
    if platform.system() == "Windows":
        candidates += [
            r"C:\Windows\Fonts\malgun.ttf",            # ë§‘ì€ê³ ë”•
            r"C:\Windows\Fonts\malgunbd.ttf",
            r"C:\Windows\Fonts\NanumGothic.ttf",       # ë‚˜ëˆ”ê³ ë”•(ìˆëŠ” ê²½ìš°)
        ]
    elif platform.system() == "Darwin":  # macOS
        candidates += [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ]
    else:  # Linux
        candidates += [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        ]

    for p in candidates:
        if os.path.exists(p):
            return p
    return None



PAGES_OFF_DIR = Path(__file__).resolve().parent / "_pages_off"

def _load_module_from_file(stem: str):
    """
    stem: 'fusion' -> _pages_off/fusion.py
    return: module or None
    """
    mod_path = PAGES_OFF_DIR / f"{stem}.py"
    if not mod_path.exists():
        return None
    spec = spec_from_file_location(f"_pages_off.{stem}", str(mod_path))
    if not spec or not spec.loader:
        return None
    mod = module_from_spec(spec)
    spec.loader.exec_module(mod)
    return mod

def _call_page(stem: str, func: str, root: Path):
    mod = _load_module_from_file(stem)
    if mod is None:
        st.info(f"`_pages_off/{stem}.py` ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    fn = getattr(mod, func, None)
    if not callable(fn):
        st.info(f"`_pages_off/{stem}.py`ì— `{func}` í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹œê·¸ë‹ˆì²˜ ê²€ì‚¬ (ì•ˆ ë˜ë©´ ê·¸ëƒ¥ ROOT ë„£ì–´ í˜¸ì¶œ)
    try:
        sig = inspect.signature(fn)
    except Exception:
        try:
            return fn(root)
        except TypeError:
            return fn(os.getenv("NWW_BUNDLE", root.name))

    params = list(sig.parameters.values())
    bundle_id = os.getenv("NWW_BUNDLE", root.name)

    if len(params) == 1:
        p = params[0]
        # ì£¼ì„(Annotation) ì•ˆì „ ì¶”ì¶œ
        try:
            ann = p.annotation
            # ann ì´ ë¬¸ìì—´ / typing ê°ì²´ / class ì¼ ìˆ˜ ìˆìŒ â†’ ë¬¸ìì—´ë¡œ í†µì¼
            ann_s = (getattr(ann, "__name__", None) or str(ann) or "").lower()
        except Exception:
            ann_s = ""

        name_s = (p.name or "").lower()

        # ROOT(Path)ê°€ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì¼€ì´ìŠ¤ ê°ì§€
        wants_path = (
            "root" in name_s or
            "path" in name_s or
            "dir"  in name_s or
            "path" in ann_s or         # '<class pathlib.Path>' / 'Path' / 'pathlib.Path'
            "pathlib.path" in ann_s
        )

        if wants_path:
            try:
                return fn(root)
            except TypeError:
                return fn(bundle_id)
        else:
            try:
                return fn(bundle_id)
            except TypeError:
                return fn(root)
    else:
        # ì¸ì 2ê°œ ì´ìƒì´ë©´ ìš°ì„  ROOT ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ bundle_id
        try:
            return fn(root)
        except TypeError:
            return fn(bundle_id)


def ensure_matplotlib_korean(font_path: str | None):
    """matplotlibì— í°íŠ¸ë¥¼ ë“±ë¡ (íƒ€ì´í‹€/ë¼ë²¨ í•œê¸€ ê¹¨ì§ ë°©ì§€)"""
    if not font_path:
        return
    try:
        font_manager.fontManager.addfont(font_path)
        fp = font_manager.FontProperties(fname=font_path)
        rcParams["font.family"] = fp.get_name()
    except Exception:
        pass
# ===== Korean WordCloud Utilities =====

# --------------------------
# ê³µí†µ ìœ í‹¸
# --------------------------
DEFAULT_BUNDLE = "b01"
ROOT = Path(os.getenv("NWW_DATA_HOME","data")) / os.getenv("NWW_BUNDLE","b01")

def bundle_id_from_root(root: Path) -> str:
    # data/<bundle> êµ¬ì¡°ì¼ ë•Œ ë§ˆì§€ë§‰ í´ë”ëª…ì´ bundle_id
    return root.name

def load_jsonl(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        return pd.DataFrame()
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            content = f.read().strip()
            if not content:
                return pd.DataFrame()
            lines = [json.loads(line) for line in content.split('\n') if line.strip()]
        return pd.DataFrame(lines)
    except Exception as e:
        print(f"Warning: Failed to load {path}: {e}")
        return pd.DataFrame()

def save_jsonl(df, path):
    import pandas as pd, numpy as np, json, math, datetime as dt

    # NaN/NaT â†’ None
    df = df.where(pd.notnull(df), None)

    def _default(o):
        # pandas.Timestamp / datetime/date â†’ ISO ë¬¸ìì—´
        if isinstance(o, (pd.Timestamp, dt.datetime, dt.date)):
            # NaT ë³´í˜¸
            try:
                return None if pd.isna(o) else o.isoformat()
            except Exception:
                return None
        # numpy ìŠ¤ì¹¼ë¼ â†’ íŒŒì´ì¬ ìŠ¤ì¹¼ë¼
        if isinstance(o, np.generic):
            return o.item()
        return str(o)  # ìµœí›„ ë³´ë£¨

    with open(path, "w", encoding="utf-8") as f:
        for rec in df.to_dict(orient="records"):
            f.write(json.dumps(rec, ensure_ascii=False, default=_default, allow_nan=False) + "\n")

def load_json_safe(p: Path, default=None):
    if not p.exists(): return default
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return default

def read_jsonl_safe(p: Path) -> pd.DataFrame:
    if not p.exists(): return pd.DataFrame()
    rows=[]
    with p.open(encoding="utf-8-sig") as f:
        for line in f:
            s=line.strip()
            if not s: continue
            try: rows.append(json.loads(s))
            except: pass
    return pd.DataFrame(rows)

def _frames_pretty(x):
    # ë‹¤ì–‘í•œ í¬ë§· ë°©ì–´ì  ì²˜ë¦¬
    def pick_score(d):
        for k in ("score","conf","p","prob","confidence"):
            if k in d: 
                try: return float(d[k])
                except: pass
        return 0.0
    if isinstance(x, list):
        x_sorted = sorted(x, key=lambda d: pick_score(d), reverse=True)
        return "; ".join(f"{d.get('label') or d.get('frame')}({pick_score(d):.2f})" for d in x_sorted[:5])
    if isinstance(x, dict):
        return f"{x.get('label') or x.get('frame')}({pick_score(x):.2f})"
    return None

# --- add: text column fallback ---
TEXT_CANDIDATES = [
    "text", "clean_text", "body", "content", "article", "message",
    "raw_text", "doc", "desc", "summary"
]

# def ensure_text_column(df):
#     """ë°ì´í„°í”„ë ˆì„ì— df['text']ë¥¼ ë³´ì¥í•œë‹¤. ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ìœ¼ë©´ í›„ë³´ ì»¬ëŸ¼ì„ ë³µì‚¬/ìƒì„±."""
#     import pandas as pd
#     # 1) í›„ë³´ ì¤‘ ìˆëŠ” ê²ƒ ì„ íƒ
#     for c in TEXT_CANDIDATES:
#         if c in df.columns:
#             if c != "text":
#                 df["text"] = df[c].astype(str)
#             return "text"
#     # 2) title+body ê²°í•© í´ë°±
#     if "title" in df.columns and "body" in df.columns:
#         df["text"] = df["title"].fillna("").astype(str) + " " + df["body"].fillna("").astype(str)
#         return "text"
#     # 3) ë¬¸ìì—´(ê°ì²´) dtype ì²« ì»¬ëŸ¼ í´ë°±
#     obj_cols = [c for c in df.columns if df[c].dtype == "O"]
#     if obj_cols:
#         df["text"] = df[obj_cols[0]].astype(str)
#         return "text"
#     # 4) ì‹¤íŒ¨
#     return None
# # --- end add ---

# === [ADD] common helpers ===
from pathlib import Path
import json, pandas as pd

# app_main.py ìƒë‹¨ import ê·¼ì²˜
import os
from pathlib import Path

def _bundle_path(bundle: str) -> Path:
    base = Path(os.getenv("NWW_DATA_HOME", "data")) / bundle
    legacy = Path("data/bundles") / bundle
    return base if base.exists() else legacy


def _to_kst_naive(x):
    return (pd.to_datetime(x, errors="coerce", utc=True)
              .dt.tz_convert("Asia/Seoul")
              .dt.tz_localize(None))
# === [/ADD] ===

import os, pandas as pd

def _to_local_naive(s, output_tz: str = "Asia/Seoul"):
    """
    ì–´ë–¤ í˜•íƒœ(ë¬¸ìì—´/naive/aware)ë“  ë°›ì•„ì„œ KST naive(datetime64[ns])ë¡œ ë³€í™˜.
    - ëª¨ë“  ê°’ì„ ë¨¼ì € UTC ê¸°ì¤€ timezone-aware ë¡œ í†µì¼(pd.to_datetime(..., utc=True))
    - ê·¸ë‹¤ìŒ ì›í•˜ëŠ” ì‹œê°„ëŒ€ë¡œ tz-convert â†’ tz ì •ë³´ ì œê±°(naive)
    """
    import pandas as pd

    # 1) ë¬¸ìì—´/íŒŒì´ì¬ datetime/íƒ€ì… ì„ì—¬ë„ UTC aware ë¡œ í†µì¼
    dt = pd.to_datetime(s, errors="coerce", utc=True)

    # 2) KSTë¡œ ë³€í™˜ í›„ tz ì œê±°(naive)
    #   (ëª¨ë‘ NaTì´ì–´ë„ ì—ëŸ¬ ì—†ì´ NaT ë°˜í™˜)
    dt_local_naive = dt.dt.tz_convert(output_tz).dt.tz_localize(None)

    return dt_local_naive


# >>> START: frames pretty
def _frames_pretty(x):
    # x: [{"label":"ê¸´ì¥ìƒìŠ¹","conf":0.81}, ...] í˜¹ì€ dict/None
    try:
        if isinstance(x, list):
            return ", ".join(f"{d.get('label')}({float(d.get('conf',0)):.2f})" for d in x[:5])
        if isinstance(x, dict):
            return f"{x.get('label')}({float(x.get('conf',0)):.2f})"
    except Exception:
        pass
    return None
# df_sample["frames_pretty"] = df_sample["frames"].apply(_frames_pretty)
# st.dataframe(df_sample[["url","title","frames_pretty", ...]])
# <<< END: frames pretty


# --- Streamlit rerun í˜¸í™˜ ë˜í¼ ---
# =========================
# Helpers: rerun / fused backfill / quick pipeline
# =========================
import hashlib

def _safe_rerun():
    """Streamlit rerun: st.rerun() ìš°ì„ , ì—†ìœ¼ë©´ experimental_rerun()"""
    try:
        st.rerun()
    except Exception:
        try:
            st.experimental_rerun()  # ì¼ë¶€ ë²„ì „ë§Œ ì¡´ì¬
        except Exception:
            st.toast("ğŸ” í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨(F5) í•´ì£¼ì„¸ìš”.")

def _content_id(row):
    base = str(row.get("url") or row.get("normalized") or row.get("text") or "")
    return hashlib.sha1(base.encode("utf-8", "ignore")).hexdigest() if base else None

def _safe_two_cols(df: pd.DataFrame, cols: list[str]) -> list[str]:
    return [c for c in cols if c in df.columns]

def _ensure_fused(df: pd.DataFrame, bundle_id: str) -> pd.DataFrame:
    """dfì— fused_scoreê°€ ì—†ì„ ë•Œ ìƒì„± ê·œì¹™: indicator+dbn â†’ score â†’ 0.0"""
    if "fused_score" in df.columns:
        return df
    out = df.copy()
    if "indicator_score" in out.columns and "dbn_score" in out.columns:
        out["fused_score"] = out.apply(
            lambda r: fuse.combine([float(r.get("indicator_score", 0.0)),
                                    float(r.get("dbn_score", 0.0))]),
            axis=1
        )
        return out
    if "score" in out.columns:
        out["fused_score"] = out["score"].astype(float)
        return out
    out["fused_score"] = 0.0
    return out

def backfill_fused_score(df_blocks: pd.DataFrame, bundle_id: str) -> pd.DataFrame:
    """
    blocks.jsonl ë“±ì— fused_scoreê°€ ì—†ì„ ë•Œ ë³µêµ¬:
    1) fusion.jsonl â†’ url / content_id ë³‘í•©
    2) scoring.jsonl â†’ url / content_id ë³‘í•©
    3) ê·¸ë˜ë„ ì—†ìœ¼ë©´ indicator+dbn ì¬ê³„ì‚° or 0.0
    """
    if not isinstance(df_blocks, pd.DataFrame) or df_blocks.empty:
        return df_blocks
    if "fused_score" in df_blocks.columns:
        return df_blocks

    out = df_blocks.copy()
    if "content_id" not in out.columns:
        out["content_id"] = out.apply(_content_id, axis=1)

    for ref_name in ["fusion.jsonl", "scoring.jsonl"]:
        ref = load_jsonl(f"data/{bundle_id}/{ref_name}")
        if ref.empty:
            continue
        ref = _ensure_fused(ref, bundle_id)

        # url ë³‘í•©
        common = _safe_two_cols(ref, ["url", "fused_score"])
        if {"url", "fused_score"}.issubset(common) and "url" in out.columns:
            merged = out.merge(ref[common], on="url", how="left", suffixes=("", "_f"))
            if "fused_score_f" in merged.columns:
                merged["fused_score"] = merged["fused_score"].fillna(merged["fused_score_f"])
                merged = merged.drop(columns=[c for c in ["fused_score_f"] if c in merged.columns])
            if "fused_score" in merged.columns and merged["fused_score"].notna().any():
                out = merged

        # content_id ë³‘í•©
        if "content_id" not in ref.columns:
            ref = ref.copy(); ref["content_id"] = ref.apply(_content_id, axis=1)
        common = _safe_two_cols(ref, ["content_id", "fused_score"])
        if {"content_id", "fused_score"}.issubset(common):
            merged = out.merge(ref[common], on="content_id", how="left", suffixes=("", "_f"))
            if "fused_score_f" in merged.columns:
                merged["fused_score"] = merged["fused_score"].fillna(merged["fused_score_f"])
                merged = merged.drop(columns=[c for c in ["fused_score_f"] if c in merged.columns])
            if "fused_score" in merged.columns and merged["fused_score"].notna().any():
                out = merged

    out = _ensure_fused(out, bundle_id)
    return out

def _quick_pipeline(bundle_id: str) -> bool:
    """
    ingest â†’ normalize â†’ analyze â†’ gate â†’ scoring â†’ blocks â†’ scenarios â†’ alerts
    ë¹ ë¥¸ ì¼ê´„ ì²˜ë¦¬(MVP). íŒŒì¼ì„ ë‹¨ê³„ë³„ë¡œ ë®ì–´ì”€.
    """
    # 1) ingest â†’ normalize
    df_ing = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    if df_ing.empty:
        return False

    df_norm = df_ing.copy()
    df_norm["normalized"] = df_norm["text"].astype(str).apply(cleaner.normalize)
    df_norm["sentences"] = df_norm["normalized"].apply(tokenizer.split_sentences)
    save_jsonl(df_norm, f"data/{bundle_id}/normalize.jsonl")

    # 2) analyze
    df_ana = df_norm.copy()
    df_ana["frames"] = df_ana["normalized"].apply(frame_tagger.tag)
    save_jsonl(df_ana, f"data/{bundle_id}/analyze.jsonl")

    # 3) gate
    df_gate = df_ana.copy()
    df_gate["indicator_score"] = df_gate["frames"].apply(indicator_scorer.run)
    save_jsonl(df_gate, f"data/{bundle_id}/gate.jsonl")

    # 4) scoring
    ind_scores, dbn_scores, fused_scores = [], [], []
    prev_frames = None
    for _, r in df_gate.iterrows():
        fr = r.get("frames", [])
        ind = float(r.get("indicator_score", 0.0))
        dbn = dbn_inference.run(fr, prev_frames=prev_frames)
        fused = fuse.combine([ind, dbn])
        ind_scores.append(ind); dbn_scores.append(dbn); fused_scores.append(fused)
        prev_frames = fr

    df_score = df_gate.copy()
    df_score["dbn_score"]   = dbn_scores
    df_score["fused_score"] = fused_scores
    save_jsonl(df_score, f"data/{bundle_id}/scoring.jsonl")

    # 5) blocks
    def _primary(fs):
        if not fs: return "General"
        best = sorted(fs, key=lambda f: f.get("score", 0), reverse=True)[0]
        return best.get("frame", "General")
    df_blk = df_score.copy()
    df_blk["block"] = df_blk["frames"].apply(_primary)
    keep = [c for c in ["url","date","normalized","frames","indicator_score","dbn_score","fused_score"] if c in df_blk.columns]
    out_blk = df_blk[keep + ["block"]].copy()
    save_jsonl(out_blk, f"data/{bundle_id}/blocks.jsonl")

    # 6) scenarios
    matched, predicted = [], []
    for _, r in out_blk.iterrows():
        sents = r.get("sentences", [])
        if not sents and isinstance(r.get("normalized",""), str):
            sents = tokenizer.split_sentences(r["normalized"])
        vecs = embedder.embed(sents)
        matched.append(scenario_matcher.match(vecs, top_k=3))
        predicted.append(scenario_predictor.generate(r.get("normalized","")))
    df_scen = out_blk.copy()
    df_scen["scenario_matched"]   = matched
    df_scen["scenario_predicted"] = predicted
    save_jsonl(df_scen, f"data/{bundle_id}/scenarios.jsonl")

    # 7) alerts
    df_alert = df_scen.copy()
    df_alert["decision"] = df_alert["fused_score"].apply(alert_decider.decide)
    def _to_level(dec: str) -> str:
        if "High" in dec: return "High"
        if "Medium" in dec: return "Medium"
        return "Low"
    df_alert["alert_level"] = df_alert["decision"].apply(_to_level)
    save_jsonl(df_alert, f"data/{bundle_id}/alerts.jsonl")
    return True

# ===== Plotly íŒ”ë ˆíŠ¸ ìœ í‹¸ =====
import itertools
import plotly.express as px

# ëª…ëª©í˜•(discrete) íŒ”ë ˆíŠ¸ë“¤ ìˆœí™˜
_PALLETS = [
    px.colors.qualitative.Set1,
    px.colors.qualitative.Set2,
    px.colors.qualitative.Set3,
    px.colors.qualitative.Bold,
    px.colors.qualitative.Dark24,
    px.colors.qualitative.Pastel,
    px.colors.qualitative.D3,
    px.colors.qualitative.T10,
    px.colors.qualitative.G10,
]
_pal_iter = itertools.cycle(_PALLETS)

def next_palette():
    """ì°¨íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ìƒ‰ íŒ”ë ˆíŠ¸(qualitative)"""
    return next(_pal_iter)

# ì—°ì†(continuous) íŒ”ë ˆíŠ¸ë“¤ ìˆœí™˜ (ì§€ë„/heatmap ë“±)
_CSEQS = [
    px.colors.sequential.Viridis,
    px.colors.sequential.Plasma,
    px.colors.sequential.Magma,
    px.colors.sequential.Cividis,
    px.colors.sequential.Blues,
    px.colors.sequential.Greens,
    px.colors.sequential.Reds,
    px.colors.sequential.Oranges,
    px.colors.sequential.Turbo,
]
_cseq_iter = itertools.cycle(_CSEQS)

def next_cscale():
    """ì°¨íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì—°ì† íŒ”ë ˆíŠ¸"""
    return next(_cseq_iter)

# ===== Alerts Time-series Helper =====
def _alert_timeseries(df: pd.DataFrame, freq: str = "D") -> pd.DataFrame:
    """
    df_alerts -> (date, alert_level) ì‹œê³„ì—´ ì§‘ê³„
    freq: "D"(ì¼), "W"(ì£¼), "M"(ì›”)
    return: long-form [date, alert_level, count]
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["date", "alert_level", "count"])
    need = {"date", "alert_level"}
    if not need.issubset(df.columns):
        return pd.DataFrame(columns=["date", "alert_level", "count"])

    out = df.copy()
    try:
        out["date"] = pd.to_datetime(out["date"])
    except Exception:
        # dateê°€ ë¬¸ìì—´ì´ ì•„ë‹ˆê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ â†’ ë¹ˆ í”„ë ˆì„ ë°˜í™˜
        return pd.DataFrame(columns=["date", "alert_level", "count"])

    # íƒ€ì„ì¡´ ì œê±° ë° ì£¼ê¸°ë³„ ë²„í‚·íŒ…
    # out["date"] = out["date"].dt.tz_localize(None, nonexistent="shift_forward", ambiguous="NaT", errors="ignore")
    out["date"] = (
        pd.to_datetime(out["date"], errors="coerce", utc=True)   # tz-aware UTC
            .dt.tz_convert("Asia/Seoul")                           # KSTë¡œ ë³€í™˜ (DST ëª¨í˜¸ì„± ì—†ìŒ)
            .dt.tz_localize(None)                                  # tz ì œê±° â†’ naive KST
    )
    out["date"] = out["date"].dt.to_period(freq).dt.to_timestamp()

    # ê²½ë³´ ë ˆë²¨ ì •ê·œí™”
    lvl_map = {"HIGH": "High", "high": "High", "H": "High",
               "MEDIUM": "Medium", "medium": "Medium", "M": "Medium",
               "LOW": "Low", "low": "Low", "L": "Low"}
    out["alert_level"] = out["alert_level"].astype(str).map(lambda x: lvl_map.get(x, x))
    order = ["High", "Medium", "Low"]
    out["alert_level"] = pd.Categorical(out["alert_level"], categories=order, ordered=True)

    grp = out.groupby(["date", "alert_level"], observed=True).size().reset_index(name="count")
    grp = grp.sort_values(["date", "alert_level"])
    return grp

def _to_wide(ts_long: pd.DataFrame) -> pd.DataFrame:
    """long-form(timeseries) â†’ wide index=date, columns(level)"""
    if ts_long.empty:
        return pd.DataFrame(columns=["date","High","Medium","Low"])
    wide = ts_long.pivot(index="date", columns="alert_level", values="count").fillna(0)
    # ë ˆë²¨ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì¥
    for c in ["High","Medium","Low"]:
        if c not in wide.columns:
            wide[c] = 0
    wide = wide[["High","Medium","Low"]]
    wide = wide.sort_index()
    return wide

def reset_index_as_date(df: pd.DataFrame) -> pd.DataFrame:
    out = df.reset_index()
    # ì´ë¯¸ 'date' ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ê³  'index'ë§Œ ìˆìœ¼ë©´ dateë¡œ
    if "date" not in out.columns and "index" in out.columns:
        out = out.rename(columns={"index":"date"})
    # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ë°©ì§€
    if "date" in out.columns and getattr(out.columns, "duplicated", lambda: False)().any():
        # ì²« ë²ˆì§¸ dateë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ date* ì»¬ëŸ¼ìœ¼ë¡œ ë³€ê²½
        seen = False
        new_cols = []
        for c in out.columns:
            if c == "date":
                if not seen:
                    new_cols.append(c); seen = True
                else:
                    new_cols.append("date_idx")
            else:
                new_cols.append(c)
        out.columns = new_cols
    return out

# --- hidden_network_detector í˜¸ì¶œ í˜¸í™˜ ë ˆì´ì–´ ---
def _postfilter_graph(graph: dict, min_count: int, stopwords: set[str]) -> dict:
    """êµ¬ë²„ì „ detect() ë°˜í™˜ ê·¸ë˜í”„ì— ëŒ€í•´ stopwords ì œê±° + min_count ì„ê³„ ì ìš©"""
    if not isinstance(graph, dict):
        return {"nodes": {}, "edges": {}}

    nodes = graph.get("nodes", {}) or {}
    edges = graph.get("edges", {}) or {}

    # ë…¸ë“œ: ë¶ˆìš©ì–´ ì œê±° + ì„ê³„ ì ìš©
    nodes2 = {k: int(v) for k, v in nodes.items()
              if isinstance(v, (int, float)) and v >= min_count and k not in stopwords}

    # ì—£ì§€: ë…¸ë“œ ìƒì¡´ ì—¬ë¶€ + ì„ê³„ ì ìš©
    edges2 = {}
    for key, c in (edges.items() if isinstance(edges, dict) else []):
        if not isinstance(c, (int, float)) or c < min_count:
            continue
        if isinstance(key, tuple):
            a, b = key
            edge_key = f"{a}â€”{b}"
        else:
            parts = str(key).split("â€”")
            if len(parts) != 2:
                continue
            a, b = parts
            edge_key = key
        if a in nodes2 and b in nodes2:
            edges2[edge_key] = int(c)

    return {"nodes": nodes2, "edges": edges2}

def _detect_network_compat(sentences, min_count: int, stopwords: set[str]):
    """
    ì‹ ë²„ì „: detect(sentences, min_count=..., stopwords=...)
    êµ¬ë²„ì „: detect(sentences)ë§Œ ì§€ì› â†’ ë°˜í™˜ ê·¸ë˜í”„ë¥¼ í›„ì²˜ë¦¬ë¡œ í•„í„°ë§
    """
    try:
        return hidden_network_detector.detect(sentences, min_count=min_count, stopwords=stopwords)
    except TypeError:
        g = hidden_network_detector.detect(sentences)
        return _postfilter_graph(g, min_count, stopwords)


# ===== Plotly íŒ”ë ˆíŠ¸ ìœ í‹¸ =====
import itertools
import plotly.express as px

_PALLETS = [
    px.colors.qualitative.Set1, px.colors.qualitative.Set2, px.colors.qualitative.Set3,
    px.colors.qualitative.Bold, px.colors.qualitative.Dark24, px.colors.qualitative.Pastel,
    px.colors.qualitative.D3, px.colors.qualitative.T10, px.colors.qualitative.G10,
]
_pal_iter = itertools.cycle(_PALLETS)
def next_palette(): return next(_pal_iter)

_CSEQS = [
    px.colors.sequential.Viridis, px.colors.sequential.Plasma, px.colors.sequential.Magma,
    px.colors.sequential.Cividis, px.colors.sequential.Blues, px.colors.sequential.Greens,
    px.colors.sequential.Reds, px.colors.sequential.Oranges, px.colors.sequential.Turbo,
]
_cseq_iter = itertools.cycle(_CSEQS)
def next_cscale(): return next(_cseq_iter)

# ===== GeoJSON & í–‰ì •ì½”ë“œ ìœ í‹¸ =====
import json, os

def load_geojson(path: str):
    if not path or not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8-sig") as f:
        return json.load(f)

def _detect_sig_props(geo: dict):
    """GeoJSONì˜ propertiesì—ì„œ ì½”ë“œ/ì´ë¦„ í‚¤ ìë™ íƒì§€"""
    if not geo or "features" not in geo or not geo["features"]:
        return None, None
    props = geo["features"][0].get("properties", {})
    # í”í•œ í‚¤ íŒ¨í„´
    candidates_code = ["SIG_CD", "adm_cd", "ADM_CD"]
    candidates_name = ["SIG_KOR_NM", "sig_kor_nm", "ADM_NM", "adm_nm", "EMD_KOR_NM"]
    code_key = next((k for k in candidates_code if k in props), None)
    name_key = next((k for k in candidates_name if k in props), None)
    return code_key, name_key

def attach_sig_cd_from_name(df: pd.DataFrame, geo: dict, name_col: str) -> pd.DataFrame:
    """df[name_col] (ì˜ˆ: 'ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬')ë¥¼ GeoJSONì˜ ì‹œêµ°êµ¬ ì½”ë“œë¡œ ë§¤í•‘í•´ df['sig_cd'] ìƒì„±"""
    code_key, name_key = _detect_sig_props(geo)
    if not (code_key and name_key) or name_col not in df.columns:
        return df
    # GeoJSON name->code ì‚¬ì „
    pairs = [(f["properties"][name_key], f["properties"][code_key]) for f in geo["features"]]
    name2code = {str(n): str(c) for n, c in pairs if n and c}
    out = df.copy()
    out["sig_cd"] = out[name_col].astype(str).map(name2code)
    return out

def ensure_sig_cd(df: pd.DataFrame, geo: dict) -> pd.DataFrame:
    """dfì— sig_cdê°€ ì—†ìœ¼ë©´ region/sigunguì—ì„œ ìœ ì¶”"""
    if "sig_cd" in df.columns:
        return df
    # ìš°ì„ ìˆœìœ„: sigungu(ì‹œêµ°êµ¬ ì „ì²´ëª…) â†’ region(ê¶Œì—­ëª…ì€ ë§¤ì¹­ ë‚®ìŒ)
    if "sigungu" in df.columns:
        return attach_sig_cd_from_name(df, geo, "sigungu")
    if "region" in df.columns:
        # regionì— ì‹œêµ°êµ¬ê°€ ì§ì ‘ ë“¤ì–´ì˜¬ ê°€ëŠ¥ì„±ì´ ë‚®ìŒ. ê·¸ë˜ë„ ì‹œë„.
        return attach_sig_cd_from_name(df, geo, "region")
    return df



# ==========================
# 1) Ingest
# ==========================
# ---------------- Ingest íƒ­ ----------------
def page_ingest(bundle_id="sample"):
    st.header("ğŸ“° Ingest â€“ ê¸°ì‚¬ ìˆ˜ì§‘")

    url = st.text_input("ê¸°ì‚¬ URL ì…ë ¥")
    text_input = st.text_area("ê¸°ì‚¬ ì›ë¬¸ ì…ë ¥(ì„ íƒ)", height=200)

    if st.button("ê¸°ì‚¬ ì €ì¥"):
        if not url and not text_input:
            st.warning("âš ï¸ URL ë˜ëŠ” ê¸°ì‚¬ ì›ë¬¸ ì…ë ¥ í•„ìš”")
            return

        # ì‹¤ì œ ê¸°ì‚¬ ìˆ˜ì§‘
        source = "Manual"; title = None; published = None
        text = text_input or ""
        if url:
            ing = news_collector.collect(url)
            text = text or ing.get("text", "")
            source = ing.get("source") or news_collector.get_domain(url)
            title = ing.get("title"); published = ing.get("published")

        df_new = pd.DataFrame([{
            "url": url or None,
            "text": text,
            "date": datetime.today().strftime("%Y-%m-%d"),
            "source": source,
            "title": title,
            "published": published
        }])
        recorder.save(bundle_id, "ingest.jsonl", df_new, dedup_on="url")
        st.success("âœ… ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")

    # ===========================
    # ğŸ“‘ ì €ì¥ëœ ê¸°ì‚¬ ëª©ë¡ (+ì‚­ì œ)
    # ===========================
    st.subheader("ğŸ“‘ ì €ì¥ëœ ê¸°ì‚¬ ëª©ë¡")
    ingest_path = f"data/{bundle_id}/ingest.jsonl"
    df = load_jsonl(ingest_path)

    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì•ˆì •ì  ì‚­ì œë¥¼ ìœ„í•œ ë‚´ë¶€ rowid ë³´ê°•
    df = df.copy()
    df["__rowid"] = df.index

    # í‘œì‹œìš© í…Œì´ë¸” êµ¬ì„±
    view_cols = [c for c in ["__rowid", "date", "source", "title", "url", "published", "text"] if c in df.columns]
    df_view = df[view_cols].copy()
    df_view["ì‚­ì œ"] = False  # ì²´í¬ë°•ìŠ¤ ì»¬ëŸ¼

    edited = st.data_editor(
        df_view,
        key=f"ingest_editor_{bundle_id}",
        hide_index=True,
        width='stretch',
        column_config={
            "__rowid": st.column_config.NumberColumn("rowid", help="ë‚´ë¶€ ì‹ë³„ì", disabled=True),
            "ì‚­ì œ": st.column_config.CheckboxColumn("ì‚­ì œ", help="ì‚­ì œí•  í–‰ ì„ íƒ"),
            "text": st.column_config.TextColumn("text", disabled=True)
        }
    )

    c1, c2, c3 = st.columns([1,1,1])

    # ğŸ—‘ ì„ íƒ í–‰ ì‚­ì œ
    if c1.button("ğŸ—‘ ì„ íƒ í–‰ ì‚­ì œ"):
        to_del = edited.loc[edited["ì‚­ì œ"] == True, "__rowid"].tolist()
        if not to_del:
            st.info("ì„ íƒëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            new_df = df[~df["__rowid"].isin(to_del)].drop(columns="__rowid")
            save_jsonl(new_df, ingest_path)
            st.success(f"âœ… {len(to_del)}ê±´ ì‚­ì œ ì™„ë£Œ")
            _safe_rerun()

    # ğŸ§¯ URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ìµœì‹ ë§Œ ìœ ì§€)
    if c2.button("ğŸ§¯ URL ì¤‘ë³µ ì œê±°"):
        if "url" in df.columns:
            before = len(df)
            # ìµœì‹  ë‚ ì§œë¥¼ ë‚¨ê¸°ë„ë¡ ì •ë ¬ í›„ drop_duplicates
            tmp = df.sort_values(by=["date"], ascending=True) if "date" in df.columns else df
            new_df = tmp.drop_duplicates(subset=["url"], keep="last").drop(columns="__rowid", errors="ignore")
            save_jsonl(new_df, ingest_path)
            st.success(f"âœ… ì¤‘ë³µ ì œê±°: {before - len(new_df)}ê±´ ì •ë¦¬")
            _safe_rerun()
        else:
            st.warning("URL ì»¬ëŸ¼ì´ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸ§¨ ì „ì²´ ì‚­ì œ(ì´ˆê¸°í™”)
    if c3.button("ğŸ§¨ ì „ì²´ ì‚­ì œ(ì´ˆê¸°í™”)"):
        empty = pd.DataFrame(columns=[c for c in df.columns if c != "__rowid"])
        save_jsonl(empty, ingest_path)
        st.success("âœ… ì „ì²´ ì‚­ì œ ì™„ë£Œ")
        _safe_rerun()

    # ê°„ë‹¨ ë¶„í¬ ì°¨íŠ¸
    if "source" in df.columns:
        counts = df["source"].fillna("Manual").value_counts().reset_index()
        counts.columns = ["source", "count"]
        fig = px.bar(counts, x="source", y="count", text="count",
                     labels={"source": "ì–¸ë¡ ì‚¬", "count": "ê¸°ì‚¬ ìˆ˜"},
                     title="ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ë¶„í¬")
        fig.update_traces(textposition="outside")
        st.plotly_chart(fig, width='stretch')


# ==========================
# 2) Normalize (ì „ì²˜ë¦¬)
# ==========================
from nwwpkg.preprocess import cleaner, tokenizer, entities_extractor
from nwwpkg.utils.io import (
    load_jsonl,
    save_jsonl,
    count_lines,
    ensure_text_column,
)


def page_normalize(bundle_id: str):
    st.header("ğŸ”¤ Normalize â€“ í…ìŠ¤íŠ¸ ì •ê·œí™” & ì—”í‹°í‹° ì¶”ì¶œ")

    # ë°ì´í„° ë¡œë“œ
    ingest_path = Path(f"data/{bundle_id}/ingest.jsonl")
    norm_path = Path(f"data/{bundle_id}/normalize.jsonl")
    ent_path = Path(f"data/{bundle_id}/entities.jsonl")

    df = load_jsonl(ingest_path)
    if df.empty:
        st.info("âš ï¸ ë¨¼ì € Ingest ë‹¨ê³„ì—ì„œ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return

    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í™•ì¸
    col = ensure_text_column(df)
    if col is None:
        st.warning("ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼(text/clean_text/body)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹¤í–‰ ë²„íŠ¼
    if st.button("âš¡ ì •ê·œí™” + ì—”í‹°í‹° ì¶”ì¶œ ì‹¤í–‰"):
        # ì •ê·œí™”
        df["normalized"] = df[col].astype(str).apply(cleaner.normalize)
        df["sentences"] = df["normalized"].apply(tokenizer.split_sentences)

        # ì €ì¥
        save_jsonl(norm_path, df.to_dict("records"))

        # ì—”í‹°í‹° ì¶”ì¶œ
        ents = []
        for _, row in df.iterrows():
            text = row.get("normalized") or row.get(col, "")
            entities = entities_extractor.run(text)
            ents.append({
                "source_id": row.get("id", None),
                "text": text,
                "entities": entities
            })
        save_jsonl(ent_path, ents)

        st.success("âœ… ì •ê·œí™” ë° ì—”í‹°í‹° ì¶”ì¶œ ì™„ë£Œ")

    # ğŸ“‘ ì •ê·œí™” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    if norm_path.exists():
        norm_df = load_jsonl(norm_path)
        st.subheader("ğŸ“‘ ì›ë¬¸ vs ì •ê·œí™” ê²°ê³¼")
        if "normalized" in norm_df.columns and col in norm_df.columns:
            st.dataframe(norm_df[[col, "normalized"]], use_container_width=True)
        else:
            st.warning("âš ï¸ í‘œì‹œí•  ë³¸ë¬¸/ì •ê·œí™” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (text/clean_text/body/normalized í™•ì¸ í•„ìš”)")

        # ë¬¸ì¥ ìˆ˜ ë¶„í¬
        if "sentences" in norm_df.columns:
            norm_df["sent_count"] = norm_df["sentences"].apply(len)
            fig = px.histogram(norm_df, x="sent_count", nbins=20, title="ë¬¸ì¥ ìˆ˜ ë¶„í¬")
            st.plotly_chart(fig, use_container_width=True)

        # ë‹¤ìš´ë¡œë“œ
        st.subheader("ğŸ“¥ ì •ê·œí™” ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        csv_buffer = io.StringIO()
        norm_df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
        st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", data=csv_buffer.getvalue(),
                           file_name=f"{bundle_id}_normalized.csv", mime="text/csv")

        jsonl_buffer = io.StringIO()
        for _, row in norm_df.iterrows():
            jsonl_buffer.write(json.dumps(row.to_dict(), ensure_ascii=False) + "\n")
        st.download_button("â¬‡ï¸ JSONL ë‹¤ìš´ë¡œë“œ", data=jsonl_buffer.getvalue(),
                           file_name=f"{bundle_id}_normalized.jsonl", mime="application/json")

    # ğŸ§‘â€ğŸ¤â€ğŸ§‘ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼
    if ent_path.exists():
        ents_df = load_jsonl(ent_path)

        # dict/list â†’ ë¬¸ìì—´ ë³€í™˜ (í‘œì‹œìš©)
        if "entities" in ents_df.columns:
            ents_df["entities"] = ents_df["entities"].apply(
                lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, (dict, list)) else str(x)
            )

        st.subheader("ğŸ§‘â€ğŸ¤â€ğŸ§‘ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼")
        st.dataframe(ents_df, use_container_width=True)
        # st.json(ents_df.to_dict("records")[:3])  # ìƒìœ„ 3ê°œ ë¯¸ë¦¬ë³´ê¸°

        # ë‹¤ìš´ë¡œë“œ
        ent_buffer = io.StringIO()
        for _, row in ents_df.iterrows():
            ent_buffer.write(json.dumps(row.to_dict(), ensure_ascii=False) + "\n")
        st.download_button("â¬‡ï¸ ì—”í‹°í‹° JSONL ë‹¤ìš´ë¡œë“œ", data=ent_buffer.getvalue(),
                           file_name=f"{bundle_id}_entities.jsonl", mime="application/json")



# ==========================
# 3) Analyze (í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/ì›Œë“œí´ë¼ìš°ë“œ)
# ==========================
def extract_keywords(texts, topn=20):
    words = " ".join(texts).split()
    counter = Counter(words)
    return pd.DataFrame(counter.most_common(topn), columns=["word", "freq"])

def page_analyze(bundle_id: str):
    """
    ğŸ” Analyze â€“ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/í‚¤ì›Œë“œ/ì›Œë“œí´ë¼ìš°ë“œ(+ê°ì • ì˜ˆì‹œ, ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬)
    - frame_tagger.tag() : ê¸°ì‚¬ë³„ í”„ë ˆì„ íƒœê¹…
    - hidden_network_detector.detect(sentences) : ê¸°ì‚¬ë³„ & ì „ì²´ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬
    - í•œê¸€ ì›Œë“œí´ë¼ìš°ë“œ: í°íŠ¸ ìë™íƒìƒ‰+ê²€ì¦, í•œê¸€ í† í°ë§Œ ì¶”ì¶œ
    - ê²°ê³¼ ì €ì¥: data/{bundle_id}/analyze.jsonl
    """
    import os, platform, re
    from collections import Counter
    from PIL import ImageFont
    from matplotlib import font_manager, rcParams
    from wordcloud import WordCloud
    import plotly.express as px
    import matplotlib.pyplot as plt

    # ------------------------
    # ë‚´ë¶€ ìœ í‹¸ (ì´ í•¨ìˆ˜ ì•ˆì—ì„œë§Œ ì‚¬ìš©)
    # ------------------------
    KO_FONT_CANDIDATES = {
        "Windows": [
            r"C:\Windows\Fonts\malgun.ttf",
            r"C:\Windows\Fonts\malgunbd.ttf",
            r"C:\Windows\Fonts\NanumGothic.ttf",
        ],
        "Darwin": [  # macOS
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ],
        "Linux": [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        ],
    }

    def _font_supports_korean(font_path: str) -> bool:
        try:
            f = ImageFont.truetype(font_path, 24)
            return f.getlength("í•œê¸€í…ŒìŠ¤íŠ¸") > 0
        except Exception:
            return False

    def resolve_korean_font(custom_path: str | None = None) -> str | None:
        # 1) ì‚¬ìš©ìê°€ ì„¸ì…˜ì— ë„£ì–´ë‘” ê²½ë¡œ ìš°ì„ 
        if custom_path and os.path.exists(custom_path) and _font_supports_korean(custom_path):
            return custom_path
        # 2) OS í›„ë³´êµ° ìˆœíšŒ
        for p in KO_FONT_CANDIDATES.get(platform.system(), []):
            if os.path.exists(p) and _font_supports_korean(p):
                return p
        return None

    def set_matplotlib_korean(font_path: str | None):
        if not font_path:
            return
        try:
            font_manager.fontManager.addfont(font_path)
            fp = font_manager.FontProperties(fname=font_path)
            rcParams["font.family"] = fp.get_name()
        except Exception:
            pass

    KO_TOKEN = re.compile(r"[ê°€-í£]{2,}")  # 2ê¸€ì ì´ìƒ í•œê¸€ë§Œ

    def korean_tokens(texts: list[str], stopwords: set[str] | None = None) -> list[str]:
        stopwords = stopwords or set()
        toks = []
        for t in texts:
            if not isinstance(t, str):
                continue
            words = KO_TOKEN.findall(t)
            words = [w for w in words if len(w) >= 2 and w not in stopwords]
            toks.extend(words)
        return toks

    # ------------------------
    # ë°ì´í„° ë¡œë“œ
    # ------------------------
    st.header("ğŸ” Analyze â€“ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/í‚¤ì›Œë“œ")
    df = load_jsonl(f"data/{bundle_id}/normalize.jsonl")
    if df.empty:
        st.info("âš ï¸ ë¨¼ì € Normalize ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í™•ë³´
    if "normalized" not in df.columns:
        df["normalized"] = df.get("text", "").astype(str)

    # ë¬¸ì¥ ì»¬ëŸ¼ ë³´ê°•
    if "sentences" not in df.columns:
        df["sentences"] = df["normalized"].apply(tokenizer.split_sentences)

    # ------------------------
    # í”„ë ˆì„ íƒœê¹… (ê¸°ì‚¬ë³„)
    # ------------------------
    df["frames"] = df["normalized"].apply(frame_tagger.tag)

    # ------------------------
    # ë„¤íŠ¸ì›Œí¬ íƒì§€ (ê¸°ì‚¬ë³„ + ì „ì²´(Global))
    # ------------------------
    st.subheader("ğŸ•¸ï¸ ë„¤íŠ¸ì›Œí¬ íƒì§€ ì˜µì…˜")
    min_count = st.slider("ìµœì†Œ ë“±ì¥ ë¹ˆë„(min_count)", 1, 20, 5, 1)
    sw_text = st.text_area(
        "ë¶ˆìš©ì–´(ì‰¼í‘œë¡œ êµ¬ë¶„, ì¶”ê°€)",
        value="ê¸°ì‚¬ì›ë¬¸,ì…ë ¥,ì˜¤í›„,ì‚¬ì§„,ì—°í•©ë‰´ìŠ¤,YTN,newsis,kmn,ì„œë¹„ìŠ¤,ë³´ë‚´ê¸°,ë³€ê²½í•˜ê¸°,ì‚¬ìš©í•˜ê¸°,ê´€ë ¨,ëŒ€í•œ,ë³¸ë¬¸,ê¸€ì,ìˆ˜ì •,ë³€í™˜",
        height=80
    )
    custom_sw = {w.strip() for w in sw_text.split(",") if w.strip()}

    # ---- ê¸°ì‚¬ë³„ ë„¤íŠ¸ì›Œí¬ ê³„ì‚° (ì˜µì…˜ ì ìš©) ----
    df["network"] = df["sentences"].apply(lambda s: _detect_network_compat(s, min_count, custom_sw))
    df["net_nodes"] = df["network"].apply(lambda g: len((g or {}).get("nodes", {})))
    df["net_edges"] = df["network"].apply(lambda g: len((g or {}).get("edges", {})))

    # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬
    all_sents = [sent for sents in df["sentences"] for sent in (sents or [])]
    global_net = _detect_network_compat(all_sents, min_count, custom_sw)
    # st.subheader("ğŸ•¸ï¸ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í”„ë¦¬ë·°")
    # st.json(global_net)

    # # (í”„ë¦¬ë·° í‘œë„ ì›í•˜ë©´)
    # top_nodes = sorted(global_net["nodes"].items(), key=lambda x: x[1], reverse=True)[:20]
    # st.dataframe(pd.DataFrame(top_nodes, columns=["node","count"]))
    # ------------------------
    # í‚¤ì›Œë“œ Top-N (í•œê¸€ ì „ìš©)
    # ------------------------
    stopwords = {"ê¸°ì", "ì—°í•©ë‰´ìŠ¤", "ë‰´ìŠ¤", "ë‹¨ë…", "ì†ë³´", "ì¢…í•©", "ì‚¬ì§„"}
    toks = korean_tokens(df["normalized"].fillna("").tolist(), stopwords=stopwords)
    
    # ------------------------
    # ê°ì • ë¶„í¬ (ì˜ˆì‹œ ë°ì´í„° ìœ ì§€)
    # ------------------------
    

    # ------------------------
    # ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€ í°íŠ¸ ìë™íƒìƒ‰ + ê²€ì¦)
    # ------------------------
    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€)")
    # ì„¸ì…˜ì— í°íŠ¸ ê²½ë¡œ ì €ì¥í•´ë‘” ê²½ìš° ìš°ì„  ì‚¬ìš© (ì—†ìœ¼ë©´ ìë™íƒìƒ‰)
    custom_font = st.session_state.get("kofont") if "kofont" in st.session_state else ""
    font_path = resolve_korean_font(custom_font or None)

    if not font_path:
        st.error("í•œê¸€ ì§€ì› í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆ) Windows: C:\\Windows\\Fonts\\malgun.ttf")
    else:
        set_matplotlib_korean(font_path)
        corpus = " ".join(toks)
        if corpus.strip():
            wc = WordCloud(
                width=1000,
                height=500,
                background_color="white",
                font_path=font_path,
                prefer_horizontal=1.0,
                collocations=False
            ).generate(corpus)
            fig2, ax2 = plt.subplots(figsize=(10, 5))
            ax2.imshow(wc, interpolation="bilinear")
            ax2.axis("off")
            st.caption(f"font: {font_path}")
            st.pyplot(fig2, clear_figure=True)
        else:
            st.warning("ìœ íš¨í•œ í•œê¸€ í† í°ì´ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ------------------------
    # í”„ë¦¬ë·°(ê¸°ì‚¬ë³„/ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬) & ì €ì¥
    # ------------------------
    # >>> START: network real
    import pandas as pd
    import networkx as nx
    import matplotlib
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    # í•œê¸€ í°íŠ¸ (Windows ê¸°ì¤€: Malgun Gothic)
    matplotlib.rc("font", family="Malgun Gothic")
    matplotlib.rcParams["axes.unicode_minus"] = False

    netp = _bundle_path(bundle_id) / "network.json"
    net = load_json_safe(netp, default=None)
    st.subheader("ğŸŒ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í”„ë¦¬ë·°")

    if net:
        nodes = net.get("nodes", {})
        edges = net.get("edges", [])
        c_nodes, c_edges = len(nodes), len(edges)
        c_nonzero = sum(1 for e in edges if e.get("weight", 0) > 0)

        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        col1, col2, col3 = st.columns(3)
        col1.metric("ë…¸ë“œ ìˆ˜", c_nodes)
        col2.metric("ì—£ì§€ ìˆ˜", c_edges)
        col3.metric("ì–‘ì˜ ê°€ì¤‘ì¹˜ ì—£ì§€", c_nonzero)

        if len(edges) > 0:
            df_e = pd.DataFrame(edges)
            if "weight" in df_e.columns and "co" in df_e.columns:
                df_e = df_e.sort_values(["weight", "co"], ascending=False).head(50)
            else:
                df_e = df_e.head(50)
            st.dataframe(df_e, use_container_width=True, height=360)

            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            for e in df_e.to_dict("records"):
                if "source" in e and "target" in e:  # ë°©ì–´ì  ì²´í¬
                    G.add_edge(e["source"], e["target"], weight=e.get("weight", 1))

            # âš ï¸ ì—¬ê¸°ì„œ ë°˜ë“œì‹œ í™•ì¸
            if G.number_of_nodes() > 0:
                degrees = dict(G.degree())
                weights = [G[u][v].get("weight", 1) for u, v in G.edges()]

                plt.figure(figsize=(6, 4))
                pos = nx.kamada_kawai_layout(G)

                nx.draw_networkx_nodes(
                    G, pos,
                    node_color="skyblue",
                    node_size=[degrees.get(n, 1) * 120 for n in G.nodes()],
                    alpha=0.8
                )
                nx.draw_networkx_edges(
                    G, pos,
                    edge_color="gray",
                    width=[w * 0.15 for w in weights],
                    alpha=0.5
                )

                # ë¼ë²¨ í•œê¸€ ê¹¨ì§ ë°©ì§€
                import matplotlib.font_manager as fm
                font_path = "C:/Windows/Fonts/malgun.ttf"
                fontprop = fm.FontProperties(fname=font_path)

                labels = {n: n for n in G.nodes() if not str(n).isdigit()}
                nx.draw_networkx_labels(
                    G, pos, labels,
                    font_size=9,
                    font_family=fontprop.get_name()
                )

                plt.axis("off")
                st.pyplot(plt, clear_figure=True)
            else:
                st.info("ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì—£ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. min_count/window íŒŒë¼ë¯¸í„°ë¥¼ ë‚®ì¶° ì¬ìƒì„±í•˜ì„¸ìš”.")
    else:
        st.info("network.jsonì´ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")


    # ğŸ“‘ í”„ë ˆì„ í‘œ(Frames Table)

    st.subheader("ğŸ“‘ í”„ë ˆì„ í‘œ (ì‹¤ë°ì´í„°)")
    p_frames = _bundle_path(bundle_id) / "frames.jsonl"
    df_frames = read_jsonl_safe(p_frames)

    if not df_frames.empty:
        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì¶”ì¶œ
        for c in ("frames","frame_preds","frame_labels","labels"):
            if c in df_frames.columns:
                df_frames["frames_pretty"] = df_frames[c].apply(_frames_pretty)
                break
        # ëŒ€í‘œ í”„ë ˆì„/ì ìˆ˜
        if "frames_pretty" not in df_frames:
            df_frames["frames_pretty"] = None
        # ë³´ì—¬ì¤„ ì»¬ëŸ¼ êµ¬ì„±(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
        show = [c for c in ["url","title","frames_pretty","fused_score","date","id"] if c in df_frames.columns]
        st.dataframe(df_frames[show].head(200), use_container_width=True, height=420)
        st.caption(f"ì†ŒìŠ¤: {p_frames}")
    else:
        st.info("frames.jsonlì´ ì—†ìŠµë‹ˆë‹¤. (í‚¤ì›Œë“œâ†’ë£°ë¶€ìŠ¤íŠ¸â†’í”„ë ˆì„ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•„ìš”)")

# ==========================
# 4) Gate (í”„ë ˆì„â†’ì§€í‘œì ìˆ˜)
# ==========================
def page_gate(bundle_id: str):
    st.header("ğŸšª Gate â€“ í”„ë ˆì„ ê¸°ë°˜ ì²´í¬(Indicator)")

    df = load_jsonl(f"data/{bundle_id}/analyze.jsonl")
    if df.empty or "frames" not in df.columns:
        st.warning("ë¨¼ì € Analyze ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ì‹¤ì œ indicator_scorer í˜¸ì¶œ
    df["indicator_score"] = df["frames"].apply(indicator_scorer.run)
    st.subheader("í”„ë ˆì„ & ì§€í‘œ ì ìˆ˜")
    st.dataframe(df[["normalized", "frames", "indicator_score"]].head(), width='stretch')

    save_jsonl(df, f"data/{bundle_id}/gate.jsonl")
    st.success("âœ… Gate ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# ==========================
# 5) Scoring (DBN + Fused)
# ==========================
def page_scoring(bundle_id: str):
    st.header("ğŸ“Š Scoring â€“ DBN ì¶”ë¡  + ìœµí•© ì ìˆ˜ + ì˜í–¥/ì‹œí”„íŠ¸")

    df = load_jsonl(f"data/{bundle_id}/gate.jsonl")
    if df.empty or "frames" not in df.columns or "indicator_score" not in df.columns:
        st.warning("ë¨¼ì € Gate ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    ind_scores, dbn_scores, fused_scores = [], [], []
    impacts, shifts = [], []
    history = []  # ì§ì „ í”„ë ˆì„ ìŠ¤ëƒ…ìƒ· ëˆ„ì 

    for _, row in df.iterrows():
        frames = row.get("frames", [])
        ind = float(row.get("indicator_score", 0.0))

        # DBN(ê°„ì´): ì§ì „ ìŠ¤ëƒ…ìƒ·ê³¼ ë¹„êµ
        prev_frames = history[-1] if history else None
        dbn_score = dbn_inference.run(frames, prev_frames=prev_frames)

        # Fused
        fused = fuse.combine([ind, dbn_score])

        # âœ… ì˜í–¥ ë¶„ì„(impact_analyzer)
        imp = impact_analyzer.run(frames, fused)

        # âœ… í”„ë ˆì„ ì‹œí”„íŠ¸(frame_shift_detector)
        if history:
            sh = frame_shift_detector.detect(history + [frames])
        else:
            sh = {"shift_detected": False, "shift_score": 0.0, "changed": []}

        # ëˆ„ì 
        history.append(frames)
        ind_scores.append(ind); dbn_scores.append(dbn_score); fused_scores.append(fused)
        impacts.append(imp); shifts.append(sh)

    df["dbn_score"]   = dbn_scores
    df["fused_score"] = fused_scores
    df["impact"]      = impacts
    df["shift"]       = shifts

    st.subheader("ì ìˆ˜ & ë¶„ì„ ìš”ì•½")
    st.dataframe(df[["indicator_score","dbn_score","fused_score","impact","shift"]].head(), width='stretch')

    # ì‹œê³„ì—´
    df["date"] = pd.to_datetime(df.get("date", datetime.today().strftime("%Y-%m-%d")))
    fig = px.line(df, x="date", y=["indicator_score", "dbn_score", "fused_score"], title="ì ìˆ˜ ì‹œê³„ì—´")
    st.plotly_chart(fig, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/scoring.jsonl")
    st.success("âœ… Scoring ê²°ê³¼ ì €ì¥ ì™„ë£Œ")


# ==========================
# 6) Fusion (ì‹œê° ë¹„êµ)
# ==========================
def page_fusion(bundle_id: str):
    st.header("âš¡ Fusion â€“ ì ìˆ˜ ë¹„êµ/ê²€ì¦")

    df = load_jsonl(f"data/{bundle_id}/scoring.jsonl")
    if df.empty or "fused_score" not in df.columns:
        st.warning("ë¨¼ì € Scoring ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    fig = px.scatter(df, x="indicator_score", y="fused_score", trendline="ols",
                     title="Indicator vs Fused")
    st.plotly_chart(fig, width='stretch')

    st.dataframe(df[["indicator_score", "dbn_score", "fused_score"]].head(), width='stretch')
    save_jsonl(df, f"data/{bundle_id}/fusion.jsonl")
    st.success("âœ… Fusion ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# ==========================
# 7) Blocks (ì£¼ í”„ë ˆì„ ê¸°ë°˜ ë¸”ë¡)
# ==========================
from nwwpkg.ui.network_viz import draw_network_pyvis


def _primary_frame(fs: list[dict]) -> str:
    if not fs:
        return "General"
    # score ê°€ì¥ í° í”„ë ˆì„
    best = sorted(fs, key=lambda f: f.get("score", 0), reverse=True)[0]
    return best.get("frame", "General")

# def page_blocks(bundle_id: str):
#     st.header("ğŸ§© Blocks (EDS) â€“ ê¸°ì‚¬ â†” ë¸”ë¡ ë§¤í•‘")

#     df = load_jsonl(f"data/{bundle_id}/fusion.jsonl")
#     if df.empty or "frames" not in df.columns:
#         st.warning("ë¨¼ì € Fusion ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
#         return

#     df["block"] = df["frames"].apply(_primary_frame)

#     # âœ… í•µì‹¬ ì»¬ëŸ¼ ë³´ì¡´ ë³´ê°•
#     keep_cols = [c for c in ["url","date","normalized","frames","indicator_score","dbn_score","fused_score"] if c in df.columns]
#     out = df[keep_cols + ["block"]].copy()

#     st.subheader("ê¸°ì‚¬ â†” ë¸”ë¡ ë§¤í•‘")
#     st.dataframe(out.head(), width='stretch')

#     save_jsonl(out, f"data/{bundle_id}/blocks.jsonl")
#     st.success("âœ… Blocks ì €ì¥ ì™„ë£Œ")

# def page_block():
#     st.header("ğŸ§© Block í˜ì´ì§€ - ë¸”ë¡ ë¶„ì„ & ìœ„ê¸° íƒì§€")

#     blocks = st.session_state.get("blocks", [])
#     if not blocks:
#         st.warning("ë¨¼ì € EventBlock í˜ì´ì§€ì—ì„œ ë¸”ë¡ì„ ìƒì„±í•˜ì„¸ìš”.")
#         return

#     # ë¸”ë¡ ì„ íƒ
#     block_id = st.selectbox("ë¶„ì„í•  ë¸”ë¡ ì„ íƒ", [b["block_id"] for b in blocks])

#     # ë¸”ë¡ ë¶„ì„
#     if st.button("ë¸”ë¡ ë¶„ì„"):
#         block = next(b for b in blocks if b["block_id"] == block_id)
#         report = block_analyzer.run(block)
#         st.session_state["block_report"] = report
#         st.success("ë¸”ë¡ ë¶„ì„ ì™„ë£Œ")
#         st.json(report)

#         # # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì‹œê°í™”
#         # st.subheader("ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„")
#         # fig = draw_network(report["network"], block["events"])
#         # st.plotly_chart(fig, use_container_width=True)

#         # pyvis ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
#         st.subheader("ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ (ìœ„í—˜ë„ ìƒ‰ìƒ í‘œì‹œ)")
#         html = draw_network_pyvis(block, report)
#         components.html(html, height=500, scrolling=True)

#     # ìœ„ê¸° íƒì§€
#     if st.button("ìœ„ê¸° íƒì§€"):
#         crisis = crisis_detector.run(blocks)
#         st.metric("Crisis Score", crisis["score"])
#         st.write(f"ìƒíƒœ: {crisis['status']}")

#     # ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
#     if st.button("ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"):
#         scenario = scenario_generator.run(block_id)
#         st.success("ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ")
#         st.json(scenario)


# from nwwpkg.ui.network_viz import draw_network_pyvis

# def page_block():
#     st.header("ğŸ§© Block í˜ì´ì§€ - ë¸”ë¡ ë¶„ì„ & ìœ„ê¸° íƒì§€")

#     # ì„¸ì…˜ì—ì„œ ë¸”ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
#     blocks = st.session_state.get("blocks", [])
#     if not blocks:
#         st.warning("ë¨¼ì € EventBlock í˜ì´ì§€ì—ì„œ ë¸”ë¡ì„ ìƒì„±í•˜ì„¸ìš”.")
#         return

#     # ë¸”ë¡ ì„ íƒ
#     block_id = st.selectbox("ë¶„ì„í•  ë¸”ë¡ ì„ íƒ", [b["block_id"] for b in blocks])
#     block = next(b for b in blocks if b["block_id"] == block_id)

#     # ë¸”ë¡ ë¶„ì„
#     if st.button("ë¸”ë¡ ë¶„ì„ ì‹¤í–‰"):
#         report = block_analyzer.run(block)
#         st.session_state["block_report"] = report
#         st.success("âœ… ë¸”ë¡ ë¶„ì„ ì™„ë£Œ")

#         # ì¸ë¬¼ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
#         st.subheader("ğŸ‘¤ ì¸ë¬¼ ë¶„ì„")
#         st.dataframe(pd.DataFrame(report["actors"]))

#         # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ í‘œì‹œ (pyvis ì¸í„°ë™í‹°ë¸Œ)
#         st.subheader("ğŸŒ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„")
#         html = draw_network_pyvis(block, report)
#         components.html(html, height=500, scrolling=True)

#         # í”„ë ˆì„ ë³€í™” í‘œì‹œ
#         st.subheader("ğŸŒ€ í”„ë ˆì„ ë³€í™”")
#         frame_report = report["frame_report"]
#         st.json(frame_report)
#         if frame_report["frames_by_time"]:
#             df_frames = pd.DataFrame(frame_report["frames_by_time"])
#             st.line_chart(df_frames.set_index("time"))

#         # ìœ„í—˜ ì ìˆ˜ í‘œì‹œ
#         st.subheader("âš ï¸ ìœ„í—˜ ì ìˆ˜")
#         st.metric("Crisis Risk Score", report["risk_score"])

#         # ìš”ì•½ í‘œì‹œ
#         st.info(report["summary"])

#     # ìœ„ê¸° íƒì§€ ì‹¤í–‰
#     if st.button("ìœ„ê¸° íƒì§€ ì‹¤í–‰"):
#         crisis = crisis_detector.run(blocks)
#         st.success("ğŸš¨ ìœ„ê¸° íƒì§€ ì™„ë£Œ")
#         st.metric("Crisis Score", crisis["score"])
#         st.write(f"ìƒíƒœ: {crisis['status']}")

#     # ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì‹¤í–‰
#     if st.button("ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì‹¤í–‰"):
#         scenario = scenario_generator.run(block_id)
#         st.success("ğŸ“‘ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ")
#         st.json(scenario)

# import streamlit as st
# import streamlit.components.v1 as components
# import pandas as pd
# from pathlib import Path

import streamlit as st
import pandas as pd
import networkx as nx
import plotly.graph_objects as go

from nwwpkg.utils.io import load_jsonl
from nwwpkg.eventblock.block_accumulator import accumulate_blocks


def page_block(bundle_id: str):
    st.header("ğŸ§© EventBlock â€“ ì‚¬ê±´/ë¸”ë¡ ìƒì„± ë° í™•ì¸")

    events_file = f"data/bundles/{bundle_id}/events.jsonl"
    blocks_file = f"data/bundles/{bundle_id}/blocks.jsonl"

    # -----------------------
    # ë°ì´í„° ë¡œë“œ
    # -----------------------
    try:
        events_df = pd.DataFrame(load_jsonl(events_file))
    except Exception:
        st.warning("âš ï¸ ë¨¼ì € 'Normalize & Entity' ë‹¨ê³„ë¥¼ ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤. (events.jsonl í•„ìš”)")
        return

    try:
        events_df, blocks = accumulate_blocks(events_file, blocks_file, sim_thr=0.35)

        # blocksê°€ listë¼ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
        if isinstance(blocks, list):
            blocks_df = pd.DataFrame(blocks)
        else:
            blocks_df = blocks

    except Exception as e:
        st.error(f"âŒ ë¸”ë¡ ìƒì„± ì˜¤ë¥˜: {e}")
        return

    # -----------------------
    # ì‚¬ê±´ ë¶„ë¥˜ ê²°ê³¼
    # -----------------------
    st.subheader("ğŸ“‚ ì‚¬ê±´ ë¶„ë¥˜ ê²°ê³¼")
    if not events_df.empty:
        st.success(f"ì‚¬ê±´ {len(events_df)}ê±´ ë¶„ë¥˜ ì™„ë£Œ")

        base_cols = ["id", "time"]
        optional_cols = [c for c in ["normalized", "block"] if c in events_df.columns]
        show_cols = base_cols + optional_cols

        st.dataframe(events_df[show_cols].head(20), use_container_width=True)
    else:
        st.warning("ì‚¬ê±´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # -----------------------
    # ë¸”ë¡ ì ì¬ ê²°ê³¼
    # -----------------------
    st.subheader("ğŸ“¦ ë¸”ë¡ ì ì¬ ê²°ê³¼")
    if blocks_df.empty:
        st.warning("âš ï¸ blocks.jsonlì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    if "block_id" not in blocks_df.columns:
        st.warning("âš ï¸ blocks.jsonlì— block_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¸”ë¡ ì¬ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    st.success(f"ë¸”ë¡ {len(blocks_df)}ê±´ ì ì¬ ì™„ë£Œ")

    block_cols = [c for c in ["block_id", "block_label", "num_events", "frames"] if c in blocks_df.columns]
    st.dataframe(blocks_df[block_cols], use_container_width=True)

    # -----------------------
    # ë¸”ë¡ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„
    # -----------------------
    st.subheader("ğŸŒ ë¸”ë¡ ê°„ ê´€ê³„ (ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„)")

    try:
        G = nx.Graph()
        for _, row in blocks_df.iterrows():
            G.add_node(row["block_id"], label=row.get("block_label", "Unknown"))

        for i in range(len(blocks_df)):
            for j in range(i + 1, len(blocks_df)):
                frames_i = set(blocks_df.iloc[i].get("frames", []))
                frames_j = set(blocks_df.iloc[j].get("frames", []))
                if frames_i & frames_j:
                    G.add_edge(blocks_df.iloc[i]["block_id"], blocks_df.iloc[j]["block_id"])

        pos = nx.spring_layout(G, seed=42)
        edge_x, edge_y = [], []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x += [x0, x1, None]
            edge_y += [y0, y1, None]

        edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=1, color="#888"),
                                hoverinfo="none", mode="lines")

        node_x, node_y, node_text = [], [], []
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(f"{node}: {G.nodes[node]['label']}")

        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode="markers+text",
            hoverinfo="text",
            text=node_text,
            textposition="top center",
            marker=dict(size=15, color="skyblue", line_width=2)
        )

        fig = go.Figure(data=[edge_trace, node_trace],
                        layout=go.Layout(showlegend=False, hovermode="closest",
                                         margin=dict(b=0, l=0, r=0, t=0)))
        st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")

    # -----------------------
    # ì›ë³¸ events ë¯¸ë¦¬ë³´ê¸°
    # -----------------------
    with st.expander("ğŸ” ì›ë³¸ events.jsonl ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
        st.dataframe(events_df.head(10), use_container_width=True)




# ==========================
# 8) Scenarios (ë§¤ì¹­ + ì˜ˆì¸¡)
# ==========================
from nwwpkg.scenario.page_scenarios import page_scenarios
# from nwwpkg.scenario.llm_utils import generate_scenario_with_llm
# def page_scenarios(bundle_id: str):
#     st.header("ğŸ“‘ Scenarios â€“ ë§¤ì¹­/ì˜ˆì¸¡")

#     df = load_jsonl(f"data/{bundle_id}/blocks.jsonl")
#     if df.empty or "normalized" not in df.columns:
#         st.warning("ë¨¼ì € Blocks ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
#         return

#     # ì‹¤ì œ ì„ë² ë”© â†’ ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­ & ì˜ˆì¸¡
#     matched_list, predicted_list = [], []
#     for _, row in df.iterrows():
#         sents = row.get("sentences", [])
#         if not sents:
#             sents = tokenizer.split_sentences(row.get("normalized", ""))
#         vecs = embedder.embed(sents)
#         matched = scenario_matcher.match(vecs, top_k=3)
#         pred = scenario_predictor.generate(row.get("normalized", ""))
#         matched_list.append(matched)
#         predicted_list.append(pred)

#     df["scenario_matched"] = matched_list
#     df["scenario_predicted"] = predicted_list

#     # ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ìœ„í—˜ë„
#     df["top_sim"] = df["scenario_matched"].apply(lambda xs: xs[0]["similarity"] if xs else 0.0)
#     top = df[["block", "fused_score", "top_sim"]].groupby("block", observed=True).mean().reset_index()
#     st.subheader("ë¸”ë¡ë³„ í‰ê·  ì ìˆ˜/ìœ ì‚¬ë„")
#     st.dataframe(top, width='stretch')

#     save_jsonl(df, f"data/{bundle_id}/scenarios.jsonl")
#     st.success("âœ… Scenarios ì €ì¥ ì™„ë£Œ")

# ==========================
# 9) Alerts (ìµœì¢… ê²½ë³´)
# ==========================
def page_alerts(bundle_id: str):
    st.header("ğŸš¨ Alerts â€“ ê²½ë³´ ë°œìƒ")

    df = load_jsonl(f"data/{bundle_id}/scenarios.jsonl")
    if df.empty or "fused_score" not in df.columns:
        st.warning("ë¨¼ì € Scenarios ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # âœ… LLM Judge ì„¤ëª… ìƒì„±(ì ìˆ˜+í”„ë ˆì„)
    def _scores_of(r):
        return {
            "indicator": float(r.get("indicator_score", 0.0)),
            "dbn":       float(r.get("dbn_score", 0.0)),
            "fused":     float(r.get("fused_score", 0.0)),
        }

    df["explanation"] = df.apply(lambda r: llm_judge.explain(_scores_of(r), r.get("frames", [])), axis=1)

    # âœ… ìµœì¢… ê²½ë³´ ê²°ì •(ê¸°ì¡´ ìœ ì§€)
    df["decision"] = df["fused_score"].apply(alert_decider.decide)

    def to_level(dec: str) -> str:
        if "High" in dec: return "High"
        if "Medium" in dec: return "Medium"
        return "Low"
    df["alert_level"] = df["decision"].apply(to_level)

    st.subheader("ê²½ë³´ ê²°ê³¼")
    st.dataframe(df[["normalized","fused_score","decision","alert_level","explanation"]].head(), width='stretch')

    fig = px.histogram(df, x="alert_level", title="ê²½ë³´ ë¶„í¬")
    st.plotly_chart(fig, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/alerts.jsonl")
    st.success("âœ… Alerts ì €ì¥ ì™„ë£Œ")


# ==========================
# 10) Event Blocks (ê°„ì´ í´ëŸ¬ìŠ¤í„°ë§)
# ==========================
# ìƒ˜í”Œ ë°ì´í„° ê²½ë¡œ
DATA_PATH = Path("data/sample_articles.jsonl")


def page_eventblocks(bundle_id: str):
    st.header("ğŸ“¦ Event Blocks â€“ í´ëŸ¬ìŠ¤í„°ë§(ê°„ì´)")

    df = load_jsonl(f"data/{bundle_id}/alerts.jsonl")
    if df.empty:
        st.warning("ë¨¼ì € Alerts ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ê°„ì´ í´ëŸ¬ìŠ¤í„°
    df["cluster"] = df.index % 3
    pivot = df.groupby("cluster", observed=True)["fused_score"].mean().reset_index()

    st.subheader("í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ìœ„í—˜ë„")
    st.dataframe(pivot, width='stretch')

    fig = px.imshow([pivot["fused_score"].tolist()],
                    labels=dict(x="Cluster", y="Risk", color="Score"),
                    title="Event Block ìœ„í—˜ë„ íˆíŠ¸ë§µ")
    st.plotly_chart(fig, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/eventblocks.jsonl")
    st.success("âœ… Event Blocks ì €ì¥ ì™„ë£Œ")

from nwwpkg.eventblock.block_accumulator import accumulate_blocks


def page_eventblock(bundle_id: str):
    st.header("ğŸ§© EventBlock â€“ ì‚¬ê±´/ë¸”ë¡ ìƒì„± ë° í™•ì¸")

    root = Path(f"data/{bundle_id}")
    events_file = root / "events.jsonl"
    blocks_file = root / "blocks.jsonl"

    # âœ… ì‚¬ê±´ ë°ì´í„° í™•ì¸
    events_df = load_jsonl(events_file)
    if events_df.empty:
        st.warning("âš ï¸ ë¨¼ì € 'Normalize & Entity' ë‹¨ê³„ë¥¼ ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤. (events.jsonl í•„ìš”)")
        return

    # âœ… ë¸”ë¡ ìƒì„± ë²„íŠ¼
    if st.button("ğŸ”„ ë¸”ë¡ ì¬ìƒì„± (ìœ ì‚¬ë„ ê¸°ë°˜)"):
        clustered_df, blocks = accumulate_blocks(events_file, blocks_file, sim_thr=0.35)
        st.session_state["blocks_df"] = clustered_df
        st.session_state["blocks"] = blocks
        st.success(f"âœ… {len(blocks)}ê°œ ë¸”ë¡ ìƒì„± ì™„ë£Œ")

    # âœ… ì„¸ì…˜ ìƒíƒœì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    blocks = st.session_state.get("blocks", [])
    blocks_df = st.session_state.get("blocks_df", pd.DataFrame())

    if not blocks:
        st.info("ì•„ì§ ë¸”ë¡ì´ ì—†ìŠµë‹ˆë‹¤. 'ë¸”ë¡ ì¬ìƒì„±'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return

    # âœ… ë¸”ë¡ ê°œìš” í…Œì´ë¸”
    st.subheader("ğŸ“Š ë¸”ë¡ ê°œìš”")
    summary_rows = [
        {"block_id": b["block_id"], "num_events": b["num_events"]}
        for b in blocks
    ]
    st.dataframe(pd.DataFrame(summary_rows), use_container_width=True)

    # âœ… ë¸”ë¡ ì„ íƒ
    block_ids = [b["block_id"] for b in blocks]
    block_id = st.selectbox("ğŸ” ë¸”ë¡ ì„ íƒ", block_ids)

    block = next((b for b in blocks if b["block_id"] == block_id), None)
    if block:
        st.markdown(f"### ğŸ“¦ ë¸”ë¡ `{block_id}` ìƒì„¸")

        # ë¸”ë¡ ë‚´ ì‚¬ê±´ í‘œ
        block_events = pd.DataFrame(block["events"])
        st.dataframe(block_events, use_container_width=True)

        # JSON ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ğŸ“‘ JSON ë¯¸ë¦¬ë³´ê¸°")
        st.json(block, expanded=False)

    # âœ… ë‹¤ìš´ë¡œë“œ
    st.subheader("ğŸ“¥ ë¸”ë¡ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    json_buffer = "\n".join(json.dumps(b, ensure_ascii=False) for b in blocks)
    st.download_button(
        "â¬‡ï¸ blocks.jsonl ë‹¤ìš´ë¡œë“œ",
        data=json_buffer,
        file_name="blocks.jsonl",
        mime="application/json",
    )
# ==========================
# 11) Ledger (íŒŒì¼ ë¡œê·¸)
# ==========================
# def render_blocks(root: Path):
#     st.header("ğŸ§± Blocks")
#     fn = _load_callable("nwwpkg.ui.pages.blocks", "page_blocks")
#     if fn is None:
#         _fallback_page("Blocks (placeholder)", root,
#                        "nwwpkg/ui/pages/blocks.py ê°€ ì—†ê±°ë‚˜ import ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
#         return
#     fn(root)

# ==========================
# Overview (ëŒ€ì‹œë³´ë“œ)
# ==========================
def page_dashboard(bundle_id: str):
    """
    ğŸŒ NWW Dashboard
    - ëœë”©/ìš”ì•½ í˜ì´ì§€
    - KPI â†’ í™•ì¥ ë²„íŠ¼(expander) â†’ ì„¸ë¶€ ëª¨ë“ˆ í˜¸ì¶œ
    """

    st.title("ğŸŒ NWW Dashboard (Critical Overview)")

    bundle_path = _bundle_path(bundle_id)

    # --- Pipeline & QA ---
    with st.expander("ğŸ“¦ Pipeline DAG (ì²˜ë¦¬ ë‹¨ê³„ í˜„í™©)", expanded=True):
        render_pipeline_dag(bundle_path)

    with st.expander("ğŸ§ª Stage QA (í’ˆì§ˆ ì ê²€)", expanded=False):
        render_stage_qa(bundle_path)

    # --- ë°ì´í„° ë¡œë“œ ---
    df_ingest  = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    df_ana     = load_jsonl(f"data/{bundle_id}/analyze.jsonl")
    df_blocks  = load_jsonl(f"data/{bundle_id}/blocks.jsonl")
    df_scen    = load_jsonl(f"data/{bundle_id}/scenarios.jsonl")
    df_alerts  = load_jsonl(f"data/{bundle_id}/alerts.jsonl")

    # âœ… ëœë”©ì—ì„œ ë°”ë¡œ ë³´ì´ëŠ” ì¸ë¼ì¸ ì‹œê³„ì—´
    render_alerts_timeseries_inline(df_alerts, bundle_id)
    # --- Alerts ---
    with st.expander("ğŸ“¡ Alerts (ìœ„í—˜ ê²½ë³´) Â· ìƒì„¸", expanded=False):
        render_alert_dashboard(df_alerts, bundle_id, bundle_path)  # âœ… ìƒì„¸(ì»¨íŠ¸ë¡¤ ì—†ìŒ)

    # --- Choropleth (ìœ„ì¹˜ ë¶„ì„) ---
    with st.expander("ğŸ—ºï¸ Location (ê²½ë³´ ì§€ì—­ ë¶„ì„)", expanded=False):
        render_choropleth(df_alerts)

    # --- Risk Blocks ---
    with st.expander("ğŸ”¥ Risk Blocks (ìœ„í—˜ ë¸”ë¡)", expanded=False):
        render_risk_blocks(df_blocks, bundle_id)

    # --- Scenarios ---
    with st.expander("ğŸ§­ Scenarios (ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤)", expanded=False):
        render_scenarios(df_scen, bundle_id)

    # --- Entities ---
    with st.expander("ğŸ·ï¸ Entities (ì£¼ìš” ì¸ë¬¼/ì¡°ì§)", expanded=False):
        render_entities(df_ana)

    # --- Sources ---
    with st.expander("ğŸ“° Sources (ì–¸ë¡ ì‚¬/ì¶œì²˜ ë¶„í¬)", expanded=False):
        render_sources(df_ingest)

    # --- Backlog ì‹¤í–‰ ë²„íŠ¼ ---
    st.divider()
    if st.button("âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ (ingest â†’ alerts)"):
        from nwwpkg.utils.runner import _quick_pipeline, _safe_rerun
        ok = _quick_pipeline(bundle_id)
        if ok:
            st.success("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: ingest â†’ alerts ìƒì„±")
        else:
            st.warning("âš ï¸ ingest ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        _safe_rerun()


# ==========================
# Main
# ==========================
def main():
    st.set_page_config(page_title="NWW Early Warning Dashboard", layout="wide")

    st.sidebar.header("âš™ï¸ ì„¤ì •")
    bundle_id = st.sidebar.text_input("Bundle ID", DEFAULT_BUNDLE)
    page = st.sidebar.radio("ğŸ“Œ ë‹¨ê³„ ì´ë™", [
        "Landing", "Ingest", "Normalize", "Analyze", "Gate", "Scoring",
        "Risk", "Alerts","EventBlocks","Fusion", "Blocks", "Scenarios", "Ledger"
    ])
    
    # === [SIDEBAR] GeoJSON ì…ë ¥ & ì§„ë‹¨ ===
    import io, json, os

    st.sidebar.markdown("### ğŸ—ºï¸ ì‹œêµ°êµ¬ GeoJSON")
    geojson_file = st.sidebar.file_uploader("íŒŒì¼ ì—…ë¡œë“œ(.geojson/.json)", type=["geojson", "json"])
    geojson_path = st.sidebar.text_input("ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ì…ë ¥", value=st.session_state.get("siggeo_path", ""))

    # ìš°ì„ ìˆœìœ„: ì—…ë¡œë“œ > ê²½ë¡œ
    geo_obj = None
    diag = {"source": None, "exists": None, "size": None, "features": None, "code_key": None, "name_key": None, "error": None}

    try:
        if geojson_file is not None:
            raw = geojson_file.getvalue().decode("utf-8", "ignore")
            geo_obj = json.loads(raw)
            st.session_state["siggeo_obj"] = geo_obj
            st.session_state["siggeo_path"] = ""  # ê²½ë¡œ ë¹„í™œì„±
            diag["source"] = "uploaded"
        elif geojson_path.strip():
            path = os.path.abspath(os.path.expanduser(geojson_path.strip()))
            st.session_state["siggeo_path"] = path
            diag["source"] = f"path: {path}"
            diag["exists"] = os.path.exists(path)
            if diag["exists"]:
                diag["size"] = os.path.getsize(path)
                with open(path, "r", encoding="utf-8-sig") as f:
                    geo_obj = json.load(f)
                st.session_state["siggeo_obj"] = geo_obj
            else:
                diag["error"] = "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        else:
            # ì´ì „ ì„¸ì…˜ì˜ ê°ì²´ ì¬ì‚¬ìš©
            geo_obj = st.session_state.get("siggeo_obj")
            if geo_obj is not None:
                diag["source"] = "session"
    except Exception as e:
        diag["error"] = f"{type(e).__name__}: {e}"

    # êµ¬ì¡° í‚¤ ìë™ íƒì§€
    def _detect_sig_props(geo: dict):
        if not geo or "features" not in geo or not geo["features"]:
            return None, None
        props = geo["features"][0].get("properties", {})
        candidates_code = ["SIG_CD", "adm_cd", "ADM_CD"]
        candidates_name = ["SIG_KOR_NM", "sig_kor_nm", "ADM_NM", "adm_nm", "EMD_KOR_NM"]
        code_key = next((k for k in candidates_code if k in props), None)
        name_key = next((k for k in candidates_name if k in props), None)
        return code_key, name_key

    if geo_obj:
        diag["features"] = len(geo_obj.get("features", []))
        diag["code_key"], diag["name_key"] = _detect_sig_props(geo_obj)

    with st.sidebar.expander("GeoJSON ì§„ë‹¨", expanded=True):
        st.write(diag)
        if geo_obj and (not diag["code_key"] or not diag["name_key"]):
            st.warning("âš ï¸ propertiesì— ì‹œêµ°êµ¬ ì½”ë“œ/ì´ë¦„ í‚¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆ: SIG_CD, SIG_KOR_NM")
        elif not geo_obj:
            st.info("GeoJSONì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")


    if page == "Landing":
        page_dashboard(bundle_id)
        render_risk_snapshot(ROOT)
    elif page == "Ingest":
        page_ingest(bundle_id)
    elif page == "Normalize":
        page_normalize(bundle_id)
    elif page == "Analyze":
        page_analyze(bundle_id)
        render_keywords_and_sentiment(ROOT, topn=20)
        render_frames_eval(ROOT)
    elif page == "Gate":
        render_gate_indicator(ROOT) 
    elif page == "Scoring":
        render_scoring(ROOT)
    elif page == "Risk":
        render_risk_page(ROOT)
    elif page == "Alerts":
        render_alerts_config(ROOT)
    elif page == "EventBlocks":
        render_eventblocks(ROOT)
        page_eventblock(bundle_id)
    elif page == "Fusion":
        _call_page("fusion", "page_fusion", ROOT)
    elif page == "Blocks":
        _call_page("blocks", "page_blocks", ROOT)
        page_block(bundle_id)
    elif page == "Scenarios":
        # _call_page("scenarios", "page_scenarios", ROOT)
        page_scenarios(ROOT)  
    elif page == "Ledger":
        _call_page("ledger", "page_ledger", ROOT)

if __name__ == "__main__":
    main()
