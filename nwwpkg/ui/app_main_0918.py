# app_main.py (ì‹¤ì œ í˜¸ì¶œ ë²„ì „: Ingest â†’ Normalize â†’ Analyze â†’ Gate â†’ Scoring â†’ Fusion â†’ Blocks â†’ Scenarios â†’ Alerts)
from __future__ import annotations
import os
import io
import re
import json
from datetime import datetime
from collections import Counter

import streamlit as st
import pandas as pd
from pathlib import Path
import plotly.express as px
import matplotlib.pyplot as plt
from importlib.util import spec_from_file_location, module_from_spec
import inspect
from wordcloud import WordCloud
# íŒŒì¼ ìƒë‹¨ ì–´ë””ë“ (ê°€ëŠ¥í•˜ë©´ import ë°”ë¡œ ì•„ë˜)
try:
    from dotenv import load_dotenv, find_dotenv
    load_dotenv(find_dotenv(), override=False)   # .env ê°’ìœ¼ë¡œë§Œ ì±„ìš°ê³  ê¸°ì¡´ envëŠ” ë³´ì¡´
except Exception:
    pass

# --------------------------
# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
# --------------------------
from nwwpkg.ingest import news_collector
from nwwpkg.preprocess import cleaner, tokenizer, embedder
from nwwpkg.eds import frame_tagger
from nwwpkg.score import indicator_scorer, dbn_inference
from nwwpkg.fusion import fuse
from nwwpkg.scenario import scenario_matcher, scenario_predictor
from nwwpkg.analyze import impact_analyzer, frame_shift_detector, hidden_network_detector
from nwwpkg.judge import llm_judge
from nwwpkg.decider import alert_decider
from nwwpkg.ledger import recorder, viewer
# from nwwpkg.ui.components.page_wrappers import (
#     render_fusion, render_blocks, render_scenarios, render_ledger
# )
from nwwpkg.ui.components.gate_indicator import render_gate_indicator
from nwwpkg.ui.components.scoring_page import render_scoring
from nwwpkg.ui.components.alerts_page import render_alerts
from nwwpkg.ui.components.eventblocks_page import render_eventblocks
from nwwpkg.ui.components.risk_snapshot import render_risk_snapshot
from nwwpkg.ui.components.risk_page import render_risk_page
from nwwpkg.ui.components.frames_eval import render_frames_eval
from nwwpkg.ui.components.analytics_basic import render_keywords_and_sentiment
from nwwpkg.ui.page_actor_ranking import page_analyze
# from nwwpkg.ui.pages.fusion import page_fusion
# from nwwpkg.ui.pages.blocks import page_blocks
# from nwwpkg.ui.pages.scenarios import page_scenarios
# from nwwpkg.ui.pages.ledger import page_ledger
# from nwwpkg.ui.components.frames_eval import render_frames_eval
# from nwwpkg.ui.components.analytics_basic import render_keywords_and_sentiment
# from nwwpkg.ui.components.risk_snapshot import render_risk_snapshot
# from nwwpkg.ui.components.risk_page import render_risk_page
# from nwwpkg.ui.components.gate_indicator import render_gate_indicator
# from nwwpkg.ui.components.scoring_page import render_scoring
# from nwwpkg.ui.components.alerts_page import render_alerts
# from nwwpkg.ui.components.eventblocks_page import render_eventblocks


# --- Korean font resolver (Windows/Mac/Linux ì§€ì›) ---
import platform, os
from matplotlib import font_manager, rcParams

def get_korean_font_path(custom_path: str | None = None) -> str | None:
    if custom_path and os.path.exists(custom_path):
        return custom_path

    candidates = []
    if platform.system() == "Windows":
        candidates += [
            r"C:\Windows\Fonts\malgun.ttf",            # ë§‘ì€ê³ ë”•
            r"C:\Windows\Fonts\malgunbd.ttf",
            r"C:\Windows\Fonts\NanumGothic.ttf",       # ë‚˜ëˆ”ê³ ë”•(ìˆëŠ” ê²½ìš°)
        ]
    elif platform.system() == "Darwin":  # macOS
        candidates += [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ]
    else:  # Linux
        candidates += [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        ]

    for p in candidates:
        if os.path.exists(p):
            return p
    return None



PAGES_OFF_DIR = Path(__file__).resolve().parent / "_pages_off"

def _load_module_from_file(stem: str):
    """
    stem: 'fusion' -> _pages_off/fusion.py
    return: module or None
    """
    mod_path = PAGES_OFF_DIR / f"{stem}.py"
    if not mod_path.exists():
        return None
    spec = spec_from_file_location(f"_pages_off.{stem}", str(mod_path))
    if not spec or not spec.loader:
        return None
    mod = module_from_spec(spec)
    spec.loader.exec_module(mod)
    return mod

def _call_page(stem: str, func: str, root: Path):
    mod = _load_module_from_file(stem)
    if mod is None:
        st.info(f"`_pages_off/{stem}.py` ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    fn = getattr(mod, func, None)
    if not callable(fn):
        st.info(f"`_pages_off/{stem}.py`ì— `{func}` í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹œê·¸ë‹ˆì²˜ ê²€ì‚¬ (ì•ˆ ë˜ë©´ ê·¸ëƒ¥ ROOT ë„£ì–´ í˜¸ì¶œ)
    try:
        sig = inspect.signature(fn)
    except Exception:
        try:
            return fn(root)
        except TypeError:
            return fn(os.getenv("NWW_BUNDLE", root.name))

    params = list(sig.parameters.values())
    bundle_id = os.getenv("NWW_BUNDLE", root.name)

    if len(params) == 1:
        p = params[0]
        # ì£¼ì„(Annotation) ì•ˆì „ ì¶”ì¶œ
        try:
            ann = p.annotation
            # ann ì´ ë¬¸ìì—´ / typing ê°ì²´ / class ì¼ ìˆ˜ ìˆìŒ â†’ ë¬¸ìì—´ë¡œ í†µì¼
            ann_s = (getattr(ann, "__name__", None) or str(ann) or "").lower()
        except Exception:
            ann_s = ""

        name_s = (p.name or "").lower()

        # ROOT(Path)ê°€ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì¼€ì´ìŠ¤ ê°ì§€
        wants_path = (
            "root" in name_s or
            "path" in name_s or
            "dir"  in name_s or
            "path" in ann_s or         # '<class pathlib.Path>' / 'Path' / 'pathlib.Path'
            "pathlib.path" in ann_s
        )

        if wants_path:
            try:
                return fn(root)
            except TypeError:
                return fn(bundle_id)
        else:
            try:
                return fn(bundle_id)
            except TypeError:
                return fn(root)
    else:
        # ì¸ì 2ê°œ ì´ìƒì´ë©´ ìš°ì„  ROOT ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ bundle_id
        try:
            return fn(root)
        except TypeError:
            return fn(bundle_id)


def ensure_matplotlib_korean(font_path: str | None):
    """matplotlibì— í°íŠ¸ë¥¼ ë“±ë¡ (íƒ€ì´í‹€/ë¼ë²¨ í•œê¸€ ê¹¨ì§ ë°©ì§€)"""
    if not font_path:
        return
    try:
        font_manager.fontManager.addfont(font_path)
        fp = font_manager.FontProperties(fname=font_path)
        rcParams["font.family"] = fp.get_name()
    except Exception:
        pass
# ===== Korean WordCloud Utilities =====

# --------------------------
# ê³µí†µ ìœ í‹¸
# --------------------------
DEFAULT_BUNDLE = "b01"
ROOT = Path(os.getenv("NWW_DATA_HOME","data")) / os.getenv("NWW_BUNDLE","b01")

def bundle_id_from_root(root: Path) -> str:
    # data/<bundle> êµ¬ì¡°ì¼ ë•Œ ë§ˆì§€ë§‰ í´ë”ëª…ì´ bundle_id
    return root.name

def load_jsonl(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        return pd.DataFrame()
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            content = f.read().strip()
            if not content:
                return pd.DataFrame()
            lines = [json.loads(line) for line in content.split('\n') if line.strip()]
        return pd.DataFrame(lines)
    except Exception as e:
        print(f"Warning: Failed to load {path}: {e}")
        return pd.DataFrame()

def save_jsonl(df, path):
    import pandas as pd, numpy as np, json, math, datetime as dt

    # NaN/NaT â†’ None
    df = df.where(pd.notnull(df), None)

    def _default(o):
        # pandas.Timestamp / datetime/date â†’ ISO ë¬¸ìì—´
        if isinstance(o, (pd.Timestamp, dt.datetime, dt.date)):
            # NaT ë³´í˜¸
            try:
                return None if pd.isna(o) else o.isoformat()
            except Exception:
                return None
        # numpy ìŠ¤ì¹¼ë¼ â†’ íŒŒì´ì¬ ìŠ¤ì¹¼ë¼
        if isinstance(o, np.generic):
            return o.item()
        return str(o)  # ìµœí›„ ë³´ë£¨

    with open(path, "w", encoding="utf-8") as f:
        for rec in df.to_dict(orient="records"):
            f.write(json.dumps(rec, ensure_ascii=False, default=_default, allow_nan=False) + "\n")

def load_json_safe(p: Path, default=None):
    if not p.exists(): return default
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return default

def read_jsonl_safe(p: Path) -> pd.DataFrame:
    if not p.exists(): return pd.DataFrame()
    rows=[]
    with p.open(encoding="utf-8-sig") as f:
        for line in f:
            s=line.strip()
            if not s: continue
            try: rows.append(json.loads(s))
            except: pass
    return pd.DataFrame(rows)

def _frames_pretty(x):
    # ë‹¤ì–‘í•œ í¬ë§· ë°©ì–´ì  ì²˜ë¦¬
    def pick_score(d):
        for k in ("score","conf","p","prob","confidence"):
            if k in d: 
                try: return float(d[k])
                except: pass
        return 0.0
    if isinstance(x, list):
        x_sorted = sorted(x, key=lambda d: pick_score(d), reverse=True)
        return "; ".join(f"{d.get('label') or d.get('frame')}({pick_score(d):.2f})" for d in x_sorted[:5])
    if isinstance(x, dict):
        return f"{x.get('label') or x.get('frame')}({pick_score(x):.2f})"
    return None

# --- add: text column fallback ---
TEXT_CANDIDATES = [
    "text", "clean_text", "body", "content", "article", "message",
    "raw_text", "doc", "desc", "summary"
]

def ensure_text_column(df):
    """ë°ì´í„°í”„ë ˆì„ì— df['text']ë¥¼ ë³´ì¥í•œë‹¤. ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ìœ¼ë©´ í›„ë³´ ì»¬ëŸ¼ì„ ë³µì‚¬/ìƒì„±."""
    import pandas as pd
    # 1) í›„ë³´ ì¤‘ ìˆëŠ” ê²ƒ ì„ íƒ
    for c in TEXT_CANDIDATES:
        if c in df.columns:
            if c != "text":
                df["text"] = df[c].astype(str)
            return "text"
    # 2) title+body ê²°í•© í´ë°±
    if "title" in df.columns and "body" in df.columns:
        df["text"] = df["title"].fillna("").astype(str) + " " + df["body"].fillna("").astype(str)
        return "text"
    # 3) ë¬¸ìì—´(ê°ì²´) dtype ì²« ì»¬ëŸ¼ í´ë°±
    obj_cols = [c for c in df.columns if df[c].dtype == "O"]
    if obj_cols:
        df["text"] = df[obj_cols[0]].astype(str)
        return "text"
    # 4) ì‹¤íŒ¨
    return None
# --- end add ---

# === [ADD] common helpers ===
from pathlib import Path
import json, pandas as pd

# app_main.py ìƒë‹¨ import ê·¼ì²˜
import os
from pathlib import Path

def _bundle_path(bundle: str) -> Path:
    base = Path(os.getenv("NWW_DATA_HOME", "data")) / bundle
    legacy = Path("data/bundles") / bundle
    return base if base.exists() else legacy


def _count_lines(p: Path) -> int:
    try:
        with p.open(encoding="utf-8-sig") as f:
            for i, _ in enumerate(f, 1): pass
        return i
    except Exception:
        return 0

def _to_kst_naive(x):
    return (pd.to_datetime(x, errors="coerce", utc=True)
              .dt.tz_convert("Asia/Seoul")
              .dt.tz_localize(None))
# === [/ADD] ===

import os, pandas as pd

def _to_local_naive(s, output_tz: str = "Asia/Seoul"):
    """
    ì–´ë–¤ í˜•íƒœ(ë¬¸ìì—´/naive/aware)ë“  ë°›ì•„ì„œ KST naive(datetime64[ns])ë¡œ ë³€í™˜.
    - ëª¨ë“  ê°’ì„ ë¨¼ì € UTC ê¸°ì¤€ timezone-aware ë¡œ í†µì¼(pd.to_datetime(..., utc=True))
    - ê·¸ë‹¤ìŒ ì›í•˜ëŠ” ì‹œê°„ëŒ€ë¡œ tz-convert â†’ tz ì •ë³´ ì œê±°(naive)
    """
    import pandas as pd

    # 1) ë¬¸ìì—´/íŒŒì´ì¬ datetime/íƒ€ì… ì„ì—¬ë„ UTC aware ë¡œ í†µì¼
    dt = pd.to_datetime(s, errors="coerce", utc=True)

    # 2) KSTë¡œ ë³€í™˜ í›„ tz ì œê±°(naive)
    #   (ëª¨ë‘ NaTì´ì–´ë„ ì—ëŸ¬ ì—†ì´ NaT ë°˜í™˜)
    dt_local_naive = dt.dt.tz_convert(output_tz).dt.tz_localize(None)

    return dt_local_naive


# >>> START: frames pretty
def _frames_pretty(x):
    # x: [{"label":"ê¸´ì¥ìƒìŠ¹","conf":0.81}, ...] í˜¹ì€ dict/None
    try:
        if isinstance(x, list):
            return ", ".join(f"{d.get('label')}({float(d.get('conf',0)):.2f})" for d in x[:5])
        if isinstance(x, dict):
            return f"{x.get('label')}({float(x.get('conf',0)):.2f})"
    except Exception:
        pass
    return None
# df_sample["frames_pretty"] = df_sample["frames"].apply(_frames_pretty)
# st.dataframe(df_sample[["url","title","frames_pretty", ...]])
# <<< END: frames pretty


# --- Streamlit rerun í˜¸í™˜ ë˜í¼ ---
# =========================
# Helpers: rerun / fused backfill / quick pipeline
# =========================
import hashlib

def _safe_rerun():
    """Streamlit rerun: st.rerun() ìš°ì„ , ì—†ìœ¼ë©´ experimental_rerun()"""
    try:
        st.rerun()
    except Exception:
        try:
            st.experimental_rerun()  # ì¼ë¶€ ë²„ì „ë§Œ ì¡´ì¬
        except Exception:
            st.toast("ğŸ” í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨(F5) í•´ì£¼ì„¸ìš”.")

def _content_id(row):
    base = str(row.get("url") or row.get("normalized") or row.get("text") or "")
    return hashlib.sha1(base.encode("utf-8", "ignore")).hexdigest() if base else None

def _safe_two_cols(df: pd.DataFrame, cols: list[str]) -> list[str]:
    return [c for c in cols if c in df.columns]

def _ensure_fused(df: pd.DataFrame, bundle_id: str) -> pd.DataFrame:
    """dfì— fused_scoreê°€ ì—†ì„ ë•Œ ìƒì„± ê·œì¹™: indicator+dbn â†’ score â†’ 0.0"""
    if "fused_score" in df.columns:
        return df
    out = df.copy()
    if "indicator_score" in out.columns and "dbn_score" in out.columns:
        out["fused_score"] = out.apply(
            lambda r: fuse.combine([float(r.get("indicator_score", 0.0)),
                                    float(r.get("dbn_score", 0.0))]),
            axis=1
        )
        return out
    if "score" in out.columns:
        out["fused_score"] = out["score"].astype(float)
        return out
    out["fused_score"] = 0.0
    return out

def backfill_fused_score(df_blocks: pd.DataFrame, bundle_id: str) -> pd.DataFrame:
    """
    blocks.jsonl ë“±ì— fused_scoreê°€ ì—†ì„ ë•Œ ë³µêµ¬:
    1) fusion.jsonl â†’ url / content_id ë³‘í•©
    2) scoring.jsonl â†’ url / content_id ë³‘í•©
    3) ê·¸ë˜ë„ ì—†ìœ¼ë©´ indicator+dbn ì¬ê³„ì‚° or 0.0
    """
    if not isinstance(df_blocks, pd.DataFrame) or df_blocks.empty:
        return df_blocks
    if "fused_score" in df_blocks.columns:
        return df_blocks

    out = df_blocks.copy()
    if "content_id" not in out.columns:
        out["content_id"] = out.apply(_content_id, axis=1)

    for ref_name in ["fusion.jsonl", "scoring.jsonl"]:
        ref = load_jsonl(f"data/{bundle_id}/{ref_name}")
        if ref.empty:
            continue
        ref = _ensure_fused(ref, bundle_id)

        # url ë³‘í•©
        common = _safe_two_cols(ref, ["url", "fused_score"])
        if {"url", "fused_score"}.issubset(common) and "url" in out.columns:
            merged = out.merge(ref[common], on="url", how="left", suffixes=("", "_f"))
            if "fused_score_f" in merged.columns:
                merged["fused_score"] = merged["fused_score"].fillna(merged["fused_score_f"])
                merged = merged.drop(columns=[c for c in ["fused_score_f"] if c in merged.columns])
            if "fused_score" in merged.columns and merged["fused_score"].notna().any():
                out = merged

        # content_id ë³‘í•©
        if "content_id" not in ref.columns:
            ref = ref.copy(); ref["content_id"] = ref.apply(_content_id, axis=1)
        common = _safe_two_cols(ref, ["content_id", "fused_score"])
        if {"content_id", "fused_score"}.issubset(common):
            merged = out.merge(ref[common], on="content_id", how="left", suffixes=("", "_f"))
            if "fused_score_f" in merged.columns:
                merged["fused_score"] = merged["fused_score"].fillna(merged["fused_score_f"])
                merged = merged.drop(columns=[c for c in ["fused_score_f"] if c in merged.columns])
            if "fused_score" in merged.columns and merged["fused_score"].notna().any():
                out = merged

    out = _ensure_fused(out, bundle_id)
    return out

def _quick_pipeline(bundle_id: str) -> bool:
    """
    ingest â†’ normalize â†’ analyze â†’ gate â†’ scoring â†’ blocks â†’ scenarios â†’ alerts
    ë¹ ë¥¸ ì¼ê´„ ì²˜ë¦¬(MVP). íŒŒì¼ì„ ë‹¨ê³„ë³„ë¡œ ë®ì–´ì”€.
    """
    # 1) ingest â†’ normalize
    df_ing = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    if df_ing.empty:
        return False

    df_norm = df_ing.copy()
    df_norm["normalized"] = df_norm["text"].astype(str).apply(cleaner.normalize)
    df_norm["sentences"] = df_norm["normalized"].apply(tokenizer.split_sentences)
    save_jsonl(df_norm, f"data/{bundle_id}/normalize.jsonl")

    # 2) analyze
    df_ana = df_norm.copy()
    df_ana["frames"] = df_ana["normalized"].apply(frame_tagger.tag)
    save_jsonl(df_ana, f"data/{bundle_id}/analyze.jsonl")

    # 3) gate
    df_gate = df_ana.copy()
    df_gate["indicator_score"] = df_gate["frames"].apply(indicator_scorer.run)
    save_jsonl(df_gate, f"data/{bundle_id}/gate.jsonl")

    # 4) scoring
    ind_scores, dbn_scores, fused_scores = [], [], []
    prev_frames = None
    for _, r in df_gate.iterrows():
        fr = r.get("frames", [])
        ind = float(r.get("indicator_score", 0.0))
        dbn = dbn_inference.run(fr, prev_frames=prev_frames)
        fused = fuse.combine([ind, dbn])
        ind_scores.append(ind); dbn_scores.append(dbn); fused_scores.append(fused)
        prev_frames = fr

    df_score = df_gate.copy()
    df_score["dbn_score"]   = dbn_scores
    df_score["fused_score"] = fused_scores
    save_jsonl(df_score, f"data/{bundle_id}/scoring.jsonl")

    # 5) blocks
    def _primary(fs):
        if not fs: return "General"
        best = sorted(fs, key=lambda f: f.get("score", 0), reverse=True)[0]
        return best.get("frame", "General")
    df_blk = df_score.copy()
    df_blk["block"] = df_blk["frames"].apply(_primary)
    keep = [c for c in ["url","date","normalized","frames","indicator_score","dbn_score","fused_score"] if c in df_blk.columns]
    out_blk = df_blk[keep + ["block"]].copy()
    save_jsonl(out_blk, f"data/{bundle_id}/blocks.jsonl")

    # 6) scenarios
    matched, predicted = [], []
    for _, r in out_blk.iterrows():
        sents = r.get("sentences", [])
        if not sents and isinstance(r.get("normalized",""), str):
            sents = tokenizer.split_sentences(r["normalized"])
        vecs = embedder.embed(sents)
        matched.append(scenario_matcher.match(vecs, top_k=3))
        predicted.append(scenario_predictor.generate(r.get("normalized","")))
    df_scen = out_blk.copy()
    df_scen["scenario_matched"]   = matched
    df_scen["scenario_predicted"] = predicted
    save_jsonl(df_scen, f"data/{bundle_id}/scenarios.jsonl")

    # 7) alerts
    df_alert = df_scen.copy()
    df_alert["decision"] = df_alert["fused_score"].apply(alert_decider.decide)
    def _to_level(dec: str) -> str:
        if "High" in dec: return "High"
        if "Medium" in dec: return "Medium"
        return "Low"
    df_alert["alert_level"] = df_alert["decision"].apply(_to_level)
    save_jsonl(df_alert, f"data/{bundle_id}/alerts.jsonl")
    return True

# ===== Plotly íŒ”ë ˆíŠ¸ ìœ í‹¸ =====
import itertools
import plotly.express as px

# ëª…ëª©í˜•(discrete) íŒ”ë ˆíŠ¸ë“¤ ìˆœí™˜
_PALLETS = [
    px.colors.qualitative.Set1,
    px.colors.qualitative.Set2,
    px.colors.qualitative.Set3,
    px.colors.qualitative.Bold,
    px.colors.qualitative.Dark24,
    px.colors.qualitative.Pastel,
    px.colors.qualitative.D3,
    px.colors.qualitative.T10,
    px.colors.qualitative.G10,
]
_pal_iter = itertools.cycle(_PALLETS)

def next_palette():
    """ì°¨íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ìƒ‰ íŒ”ë ˆíŠ¸(qualitative)"""
    return next(_pal_iter)

# ì—°ì†(continuous) íŒ”ë ˆíŠ¸ë“¤ ìˆœí™˜ (ì§€ë„/heatmap ë“±)
_CSEQS = [
    px.colors.sequential.Viridis,
    px.colors.sequential.Plasma,
    px.colors.sequential.Magma,
    px.colors.sequential.Cividis,
    px.colors.sequential.Blues,
    px.colors.sequential.Greens,
    px.colors.sequential.Reds,
    px.colors.sequential.Oranges,
    px.colors.sequential.Turbo,
]
_cseq_iter = itertools.cycle(_CSEQS)

def next_cscale():
    """ì°¨íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì—°ì† íŒ”ë ˆíŠ¸"""
    return next(_cseq_iter)

# ===== Alerts Time-series Helper =====
def _alert_timeseries(df: pd.DataFrame, freq: str = "D") -> pd.DataFrame:
    """
    df_alerts -> (date, alert_level) ì‹œê³„ì—´ ì§‘ê³„
    freq: "D"(ì¼), "W"(ì£¼), "M"(ì›”)
    return: long-form [date, alert_level, count]
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["date", "alert_level", "count"])
    need = {"date", "alert_level"}
    if not need.issubset(df.columns):
        return pd.DataFrame(columns=["date", "alert_level", "count"])

    out = df.copy()
    try:
        out["date"] = pd.to_datetime(out["date"])
    except Exception:
        # dateê°€ ë¬¸ìì—´ì´ ì•„ë‹ˆê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ â†’ ë¹ˆ í”„ë ˆì„ ë°˜í™˜
        return pd.DataFrame(columns=["date", "alert_level", "count"])

    # íƒ€ì„ì¡´ ì œê±° ë° ì£¼ê¸°ë³„ ë²„í‚·íŒ…
    # out["date"] = out["date"].dt.tz_localize(None, nonexistent="shift_forward", ambiguous="NaT", errors="ignore")
    out["date"] = (
        pd.to_datetime(out["date"], errors="coerce", utc=True)   # tz-aware UTC
            .dt.tz_convert("Asia/Seoul")                           # KSTë¡œ ë³€í™˜ (DST ëª¨í˜¸ì„± ì—†ìŒ)
            .dt.tz_localize(None)                                  # tz ì œê±° â†’ naive KST
    )
    out["date"] = out["date"].dt.to_period(freq).dt.to_timestamp()

    # ê²½ë³´ ë ˆë²¨ ì •ê·œí™”
    lvl_map = {"HIGH": "High", "high": "High", "H": "High",
               "MEDIUM": "Medium", "medium": "Medium", "M": "Medium",
               "LOW": "Low", "low": "Low", "L": "Low"}
    out["alert_level"] = out["alert_level"].astype(str).map(lambda x: lvl_map.get(x, x))
    order = ["High", "Medium", "Low"]
    out["alert_level"] = pd.Categorical(out["alert_level"], categories=order, ordered=True)

    grp = out.groupby(["date", "alert_level"], observed=True).size().reset_index(name="count")
    grp = grp.sort_values(["date", "alert_level"])
    return grp

def _to_wide(ts_long: pd.DataFrame) -> pd.DataFrame:
    """long-form(timeseries) â†’ wide index=date, columns(level)"""
    if ts_long.empty:
        return pd.DataFrame(columns=["date","High","Medium","Low"])
    wide = ts_long.pivot(index="date", columns="alert_level", values="count").fillna(0)
    # ë ˆë²¨ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì¥
    for c in ["High","Medium","Low"]:
        if c not in wide.columns:
            wide[c] = 0
    wide = wide[["High","Medium","Low"]]
    wide = wide.sort_index()
    return wide

def reset_index_as_date(df: pd.DataFrame) -> pd.DataFrame:
    out = df.reset_index()
    # ì´ë¯¸ 'date' ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ê³  'index'ë§Œ ìˆìœ¼ë©´ dateë¡œ
    if "date" not in out.columns and "index" in out.columns:
        out = out.rename(columns={"index":"date"})
    # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ë°©ì§€
    if "date" in out.columns and getattr(out.columns, "duplicated", lambda: False)().any():
        # ì²« ë²ˆì§¸ dateë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ date* ì»¬ëŸ¼ìœ¼ë¡œ ë³€ê²½
        seen = False
        new_cols = []
        for c in out.columns:
            if c == "date":
                if not seen:
                    new_cols.append(c); seen = True
                else:
                    new_cols.append("date_idx")
            else:
                new_cols.append(c)
        out.columns = new_cols
    return out

# --- hidden_network_detector í˜¸ì¶œ í˜¸í™˜ ë ˆì´ì–´ ---
def _postfilter_graph(graph: dict, min_count: int, stopwords: set[str]) -> dict:
    """êµ¬ë²„ì „ detect() ë°˜í™˜ ê·¸ë˜í”„ì— ëŒ€í•´ stopwords ì œê±° + min_count ì„ê³„ ì ìš©"""
    if not isinstance(graph, dict):
        return {"nodes": {}, "edges": {}}

    nodes = graph.get("nodes", {}) or {}
    edges = graph.get("edges", {}) or {}

    # ë…¸ë“œ: ë¶ˆìš©ì–´ ì œê±° + ì„ê³„ ì ìš©
    nodes2 = {k: int(v) for k, v in nodes.items()
              if isinstance(v, (int, float)) and v >= min_count and k not in stopwords}

    # ì—£ì§€: ë…¸ë“œ ìƒì¡´ ì—¬ë¶€ + ì„ê³„ ì ìš©
    edges2 = {}
    for key, c in (edges.items() if isinstance(edges, dict) else []):
        if not isinstance(c, (int, float)) or c < min_count:
            continue
        if isinstance(key, tuple):
            a, b = key
            edge_key = f"{a}â€”{b}"
        else:
            parts = str(key).split("â€”")
            if len(parts) != 2:
                continue
            a, b = parts
            edge_key = key
        if a in nodes2 and b in nodes2:
            edges2[edge_key] = int(c)

    return {"nodes": nodes2, "edges": edges2}

def _detect_network_compat(sentences, min_count: int, stopwords: set[str]):
    """
    ì‹ ë²„ì „: detect(sentences, min_count=..., stopwords=...)
    êµ¬ë²„ì „: detect(sentences)ë§Œ ì§€ì› â†’ ë°˜í™˜ ê·¸ë˜í”„ë¥¼ í›„ì²˜ë¦¬ë¡œ í•„í„°ë§
    """
    try:
        return hidden_network_detector.detect(sentences, min_count=min_count, stopwords=stopwords)
    except TypeError:
        g = hidden_network_detector.detect(sentences)
        return _postfilter_graph(g, min_count, stopwords)


# ===== Plotly íŒ”ë ˆíŠ¸ ìœ í‹¸ =====
import itertools
import plotly.express as px

_PALLETS = [
    px.colors.qualitative.Set1, px.colors.qualitative.Set2, px.colors.qualitative.Set3,
    px.colors.qualitative.Bold, px.colors.qualitative.Dark24, px.colors.qualitative.Pastel,
    px.colors.qualitative.D3, px.colors.qualitative.T10, px.colors.qualitative.G10,
]
_pal_iter = itertools.cycle(_PALLETS)
def next_palette(): return next(_pal_iter)

_CSEQS = [
    px.colors.sequential.Viridis, px.colors.sequential.Plasma, px.colors.sequential.Magma,
    px.colors.sequential.Cividis, px.colors.sequential.Blues, px.colors.sequential.Greens,
    px.colors.sequential.Reds, px.colors.sequential.Oranges, px.colors.sequential.Turbo,
]
_cseq_iter = itertools.cycle(_CSEQS)
def next_cscale(): return next(_cseq_iter)

# ===== GeoJSON & í–‰ì •ì½”ë“œ ìœ í‹¸ =====
import json, os

def load_geojson(path: str):
    if not path or not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8-sig") as f:
        return json.load(f)

def _detect_sig_props(geo: dict):
    """GeoJSONì˜ propertiesì—ì„œ ì½”ë“œ/ì´ë¦„ í‚¤ ìë™ íƒì§€"""
    if not geo or "features" not in geo or not geo["features"]:
        return None, None
    props = geo["features"][0].get("properties", {})
    # í”í•œ í‚¤ íŒ¨í„´
    candidates_code = ["SIG_CD", "adm_cd", "ADM_CD"]
    candidates_name = ["SIG_KOR_NM", "sig_kor_nm", "ADM_NM", "adm_nm", "EMD_KOR_NM"]
    code_key = next((k for k in candidates_code if k in props), None)
    name_key = next((k for k in candidates_name if k in props), None)
    return code_key, name_key

def attach_sig_cd_from_name(df: pd.DataFrame, geo: dict, name_col: str) -> pd.DataFrame:
    """df[name_col] (ì˜ˆ: 'ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬')ë¥¼ GeoJSONì˜ ì‹œêµ°êµ¬ ì½”ë“œë¡œ ë§¤í•‘í•´ df['sig_cd'] ìƒì„±"""
    code_key, name_key = _detect_sig_props(geo)
    if not (code_key and name_key) or name_col not in df.columns:
        return df
    # GeoJSON name->code ì‚¬ì „
    pairs = [(f["properties"][name_key], f["properties"][code_key]) for f in geo["features"]]
    name2code = {str(n): str(c) for n, c in pairs if n and c}
    out = df.copy()
    out["sig_cd"] = out[name_col].astype(str).map(name2code)
    return out

def ensure_sig_cd(df: pd.DataFrame, geo: dict) -> pd.DataFrame:
    """dfì— sig_cdê°€ ì—†ìœ¼ë©´ region/sigunguì—ì„œ ìœ ì¶”"""
    if "sig_cd" in df.columns:
        return df
    # ìš°ì„ ìˆœìœ„: sigungu(ì‹œêµ°êµ¬ ì „ì²´ëª…) â†’ region(ê¶Œì—­ëª…ì€ ë§¤ì¹­ ë‚®ìŒ)
    if "sigungu" in df.columns:
        return attach_sig_cd_from_name(df, geo, "sigungu")
    if "region" in df.columns:
        # regionì— ì‹œêµ°êµ¬ê°€ ì§ì ‘ ë“¤ì–´ì˜¬ ê°€ëŠ¥ì„±ì´ ë‚®ìŒ. ê·¸ë˜ë„ ì‹œë„.
        return attach_sig_cd_from_name(df, geo, "region")
    return df



# ==========================
# 1) Ingest
# ==========================
# ---------------- Ingest íƒ­ ----------------
def page_ingest(bundle_id="sample"):
    st.header("ğŸ“° Ingest â€“ ê¸°ì‚¬ ìˆ˜ì§‘")

    url = st.text_input("ê¸°ì‚¬ URL ì…ë ¥")
    text_input = st.text_area("ê¸°ì‚¬ ì›ë¬¸ ì…ë ¥(ì„ íƒ)", height=200)

    if st.button("ê¸°ì‚¬ ì €ì¥"):
        if not url and not text_input:
            st.warning("âš ï¸ URL ë˜ëŠ” ê¸°ì‚¬ ì›ë¬¸ ì…ë ¥ í•„ìš”")
            return

        # ì‹¤ì œ ê¸°ì‚¬ ìˆ˜ì§‘
        source = "Manual"; title = None; published = None
        text = text_input or ""
        if url:
            ing = news_collector.collect(url)
            text = text or ing.get("text", "")
            source = ing.get("source") or news_collector.get_domain(url)
            title = ing.get("title"); published = ing.get("published")

        df_new = pd.DataFrame([{
            "url": url or None,
            "text": text,
            "date": datetime.today().strftime("%Y-%m-%d"),
            "source": source,
            "title": title,
            "published": published
        }])
        recorder.save(bundle_id, "ingest.jsonl", df_new, dedup_on="url")
        st.success("âœ… ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")

    # ===========================
    # ğŸ“‘ ì €ì¥ëœ ê¸°ì‚¬ ëª©ë¡ (+ì‚­ì œ)
    # ===========================
    st.subheader("ğŸ“‘ ì €ì¥ëœ ê¸°ì‚¬ ëª©ë¡")
    ingest_path = f"data/{bundle_id}/ingest.jsonl"
    df = load_jsonl(ingest_path)

    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì•ˆì •ì  ì‚­ì œë¥¼ ìœ„í•œ ë‚´ë¶€ rowid ë³´ê°•
    df = df.copy()
    df["__rowid"] = df.index

    # í‘œì‹œìš© í…Œì´ë¸” êµ¬ì„±
    view_cols = [c for c in ["__rowid", "date", "source", "title", "url", "published", "text"] if c in df.columns]
    df_view = df[view_cols].copy()
    df_view["ì‚­ì œ"] = False  # ì²´í¬ë°•ìŠ¤ ì»¬ëŸ¼

    edited = st.data_editor(
        df_view,
        key=f"ingest_editor_{bundle_id}",
        hide_index=True,
        width='stretch',
        column_config={
            "__rowid": st.column_config.NumberColumn("rowid", help="ë‚´ë¶€ ì‹ë³„ì", disabled=True),
            "ì‚­ì œ": st.column_config.CheckboxColumn("ì‚­ì œ", help="ì‚­ì œí•  í–‰ ì„ íƒ"),
            "text": st.column_config.TextColumn("text", disabled=True)
        }
    )

    c1, c2, c3 = st.columns([1,1,1])

    # ğŸ—‘ ì„ íƒ í–‰ ì‚­ì œ
    if c1.button("ğŸ—‘ ì„ íƒ í–‰ ì‚­ì œ"):
        to_del = edited.loc[edited["ì‚­ì œ"] == True, "__rowid"].tolist()
        if not to_del:
            st.info("ì„ íƒëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            new_df = df[~df["__rowid"].isin(to_del)].drop(columns="__rowid")
            save_jsonl(new_df, ingest_path)
            st.success(f"âœ… {len(to_del)}ê±´ ì‚­ì œ ì™„ë£Œ")
            _safe_rerun()

    # ğŸ§¯ URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ìµœì‹ ë§Œ ìœ ì§€)
    if c2.button("ğŸ§¯ URL ì¤‘ë³µ ì œê±°"):
        if "url" in df.columns:
            before = len(df)
            # ìµœì‹  ë‚ ì§œë¥¼ ë‚¨ê¸°ë„ë¡ ì •ë ¬ í›„ drop_duplicates
            tmp = df.sort_values(by=["date"], ascending=True) if "date" in df.columns else df
            new_df = tmp.drop_duplicates(subset=["url"], keep="last").drop(columns="__rowid", errors="ignore")
            save_jsonl(new_df, ingest_path)
            st.success(f"âœ… ì¤‘ë³µ ì œê±°: {before - len(new_df)}ê±´ ì •ë¦¬")
            _safe_rerun()
        else:
            st.warning("URL ì»¬ëŸ¼ì´ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸ§¨ ì „ì²´ ì‚­ì œ(ì´ˆê¸°í™”)
    if c3.button("ğŸ§¨ ì „ì²´ ì‚­ì œ(ì´ˆê¸°í™”)"):
        empty = pd.DataFrame(columns=[c for c in df.columns if c != "__rowid"])
        save_jsonl(empty, ingest_path)
        st.success("âœ… ì „ì²´ ì‚­ì œ ì™„ë£Œ")
        _safe_rerun()

    # ê°„ë‹¨ ë¶„í¬ ì°¨íŠ¸
    if "source" in df.columns:
        counts = df["source"].fillna("Manual").value_counts().reset_index()
        counts.columns = ["source", "count"]
        fig = px.bar(counts, x="source", y="count", text="count",
                     labels={"source": "ì–¸ë¡ ì‚¬", "count": "ê¸°ì‚¬ ìˆ˜"},
                     title="ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ë¶„í¬")
        fig.update_traces(textposition="outside")
        st.plotly_chart(fig, width='stretch')


# ==========================
# 2) Normalize (ì „ì²˜ë¦¬)
# ==========================
def page_normalize(bundle_id):
    st.header("ğŸ”¤ Normalize â€“ í…ìŠ¤íŠ¸ ì •ê·œí™”")

    df = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    if df.empty:
        st.info("âš ï¸ ë¨¼ì € Ingest ë‹¨ê³„ì—ì„œ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    # ensure df['text']
    col = ensure_text_column(df)
    if col is None:
        st.warning("ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆ: text/clean_text/body)")
        return


    # ì‹¤ì œ ì „ì²˜ë¦¬ í˜¸ì¶œ
    df["normalized"] = df["text"].astype(str).apply(cleaner.normalize)
    df["sentences"] = df["normalized"].apply(tokenizer.split_sentences)

    # ë¹„êµ í…Œì´ë¸”
    st.subheader("ì›ë¬¸ vs ì •ê·œí™” í…ìŠ¤íŠ¸")
    st.dataframe(df[["text", "normalized"]].head(10), width='stretch')

    # ë¬¸ì¥ ìˆ˜ ë¶„í¬
    df["sent_count"] = df["sentences"].apply(len)
    fig = px.histogram(df, x="sent_count", nbins=20, title="ë¬¸ì¥ ìˆ˜ ë¶„í¬")
    st.plotly_chart(fig, width='stretch')

    # ë‹¤ìš´ë¡œë“œ
    st.subheader("ğŸ“¥ ì •ê·œí™” ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
    st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", data=csv_buffer.getvalue(),
                       file_name=f"{bundle_id}_normalized.csv", mime="text/csv")

    jsonl_buffer = io.StringIO()
    for _, row in df.iterrows():
        jsonl_buffer.write(json.dumps(row.to_dict(), ensure_ascii=False) + "\n")
    st.download_button("â¬‡ï¸ JSONL ë‹¤ìš´ë¡œë“œ", data=jsonl_buffer.getvalue(),
                       file_name=f"{bundle_id}_normalized.jsonl", mime="application/json")

    save_jsonl(df, f"data/{bundle_id}/normalize.jsonl")
    st.success("âœ… ì •ê·œí™” ë°ì´í„° ì €ì¥ ì™„ë£Œ")

# ==========================
# 3) Analyze (í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/ì›Œë“œí´ë¼ìš°ë“œ)
# ==========================
def extract_keywords(texts, topn=20):
    words = " ".join(texts).split()
    counter = Counter(words)
    return pd.DataFrame(counter.most_common(topn), columns=["word", "freq"])

def page_analyze(bundle_id: str):
    """
    ğŸ” Analyze â€“ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/í‚¤ì›Œë“œ/ì›Œë“œí´ë¼ìš°ë“œ(+ê°ì • ì˜ˆì‹œ, ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬)
    - frame_tagger.tag() : ê¸°ì‚¬ë³„ í”„ë ˆì„ íƒœê¹…
    - hidden_network_detector.detect(sentences) : ê¸°ì‚¬ë³„ & ì „ì²´ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬
    - í•œê¸€ ì›Œë“œí´ë¼ìš°ë“œ: í°íŠ¸ ìë™íƒìƒ‰+ê²€ì¦, í•œê¸€ í† í°ë§Œ ì¶”ì¶œ
    - ê²°ê³¼ ì €ì¥: data/{bundle_id}/analyze.jsonl
    """
    import os, platform, re
    from collections import Counter
    from PIL import ImageFont
    from matplotlib import font_manager, rcParams
    from wordcloud import WordCloud
    import plotly.express as px
    import matplotlib.pyplot as plt

    # ------------------------
    # ë‚´ë¶€ ìœ í‹¸ (ì´ í•¨ìˆ˜ ì•ˆì—ì„œë§Œ ì‚¬ìš©)
    # ------------------------
    KO_FONT_CANDIDATES = {
        "Windows": [
            r"C:\Windows\Fonts\malgun.ttf",
            r"C:\Windows\Fonts\malgunbd.ttf",
            r"C:\Windows\Fonts\NanumGothic.ttf",
        ],
        "Darwin": [  # macOS
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ],
        "Linux": [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        ],
    }

    def _font_supports_korean(font_path: str) -> bool:
        try:
            f = ImageFont.truetype(font_path, 24)
            return f.getlength("í•œê¸€í…ŒìŠ¤íŠ¸") > 0
        except Exception:
            return False

    def resolve_korean_font(custom_path: str | None = None) -> str | None:
        # 1) ì‚¬ìš©ìê°€ ì„¸ì…˜ì— ë„£ì–´ë‘” ê²½ë¡œ ìš°ì„ 
        if custom_path and os.path.exists(custom_path) and _font_supports_korean(custom_path):
            return custom_path
        # 2) OS í›„ë³´êµ° ìˆœíšŒ
        for p in KO_FONT_CANDIDATES.get(platform.system(), []):
            if os.path.exists(p) and _font_supports_korean(p):
                return p
        return None

    def set_matplotlib_korean(font_path: str | None):
        if not font_path:
            return
        try:
            font_manager.fontManager.addfont(font_path)
            fp = font_manager.FontProperties(fname=font_path)
            rcParams["font.family"] = fp.get_name()
        except Exception:
            pass

    KO_TOKEN = re.compile(r"[ê°€-í£]{2,}")  # 2ê¸€ì ì´ìƒ í•œê¸€ë§Œ

    def korean_tokens(texts: list[str], stopwords: set[str] | None = None) -> list[str]:
        stopwords = stopwords or set()
        toks = []
        for t in texts:
            if not isinstance(t, str):
                continue
            words = KO_TOKEN.findall(t)
            words = [w for w in words if len(w) >= 2 and w not in stopwords]
            toks.extend(words)
        return toks

    # ------------------------
    # ë°ì´í„° ë¡œë“œ
    # ------------------------
    st.header("ğŸ” Analyze â€“ í”„ë ˆì„/ë„¤íŠ¸ì›Œí¬/í‚¤ì›Œë“œ")
    df = load_jsonl(f"data/{bundle_id}/normalize.jsonl")
    if df.empty:
        st.info("âš ï¸ ë¨¼ì € Normalize ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í™•ë³´
    if "normalized" not in df.columns:
        df["normalized"] = df.get("text", "").astype(str)

    # ë¬¸ì¥ ì»¬ëŸ¼ ë³´ê°•
    if "sentences" not in df.columns:
        df["sentences"] = df["normalized"].apply(tokenizer.split_sentences)

    # ------------------------
    # í”„ë ˆì„ íƒœê¹… (ê¸°ì‚¬ë³„)
    # ------------------------
    df["frames"] = df["normalized"].apply(frame_tagger.tag)

    # ------------------------
    # ë„¤íŠ¸ì›Œí¬ íƒì§€ (ê¸°ì‚¬ë³„ + ì „ì²´(Global))
    # ------------------------
    st.subheader("ğŸ•¸ï¸ ë„¤íŠ¸ì›Œí¬ íƒì§€ ì˜µì…˜")
    min_count = st.slider("ìµœì†Œ ë“±ì¥ ë¹ˆë„(min_count)", 1, 20, 5, 1)
    sw_text = st.text_area(
        "ë¶ˆìš©ì–´(ì‰¼í‘œë¡œ êµ¬ë¶„, ì¶”ê°€)",
        value="ê¸°ì‚¬ì›ë¬¸,ì…ë ¥,ì˜¤í›„,ì‚¬ì§„,ì—°í•©ë‰´ìŠ¤,YTN,newsis,kmn,ì„œë¹„ìŠ¤,ë³´ë‚´ê¸°,ë³€ê²½í•˜ê¸°,ì‚¬ìš©í•˜ê¸°,ê´€ë ¨,ëŒ€í•œ,ë³¸ë¬¸,ê¸€ì,ìˆ˜ì •,ë³€í™˜",
        height=80
    )
    custom_sw = {w.strip() for w in sw_text.split(",") if w.strip()}

    # ---- ê¸°ì‚¬ë³„ ë„¤íŠ¸ì›Œí¬ ê³„ì‚° (ì˜µì…˜ ì ìš©) ----
    df["network"] = df["sentences"].apply(lambda s: _detect_network_compat(s, min_count, custom_sw))
    df["net_nodes"] = df["network"].apply(lambda g: len((g or {}).get("nodes", {})))
    df["net_edges"] = df["network"].apply(lambda g: len((g or {}).get("edges", {})))

    # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬
    all_sents = [sent for sents in df["sentences"] for sent in (sents or [])]
    global_net = _detect_network_compat(all_sents, min_count, custom_sw)
    # st.subheader("ğŸ•¸ï¸ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í”„ë¦¬ë·°")
    # st.json(global_net)

    # # (í”„ë¦¬ë·° í‘œë„ ì›í•˜ë©´)
    # top_nodes = sorted(global_net["nodes"].items(), key=lambda x: x[1], reverse=True)[:20]
    # st.dataframe(pd.DataFrame(top_nodes, columns=["node","count"]))
    # ------------------------
    # í‚¤ì›Œë“œ Top-N (í•œê¸€ ì „ìš©)
    # ------------------------
    stopwords = {"ê¸°ì", "ì—°í•©ë‰´ìŠ¤", "ë‰´ìŠ¤", "ë‹¨ë…", "ì†ë³´", "ì¢…í•©", "ì‚¬ì§„"}
    toks = korean_tokens(df["normalized"].fillna("").tolist(), stopwords=stopwords)
    # kw_df = pd.DataFrame(Counter(toks).most_common(30), columns=["word", "freq"])
    # st.dataframe(kw_df.head(20), width='stretch')
    
    # í‚¤ì›Œë“œ íŒŒì¼ ì½ê¸° (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
    # >>> START: keywords real
    # kwp = _bundle_path(bundle_id) / "keywords.jsonl"
    # df_kw = read_jsonl_safe(kwp)
    # st.subheader("ğŸ—‚ï¸ í‚¤ì›Œë“œ Top-N (í•œê¸€)")
    # if not df_kw.empty:
    #     df_kw["freq"] = pd.to_numeric(df_kw.get("freq", 0), errors="coerce").fillna(0).astype(int)
    #     df_kw = df_kw.sort_values("freq", ascending=False).head(50)
    #     st.dataframe(df_kw, use_container_width=True, height=420)
    #     st.caption(f"ì†ŒìŠ¤: {kwp}")
    # else:
    #     st.info("í‚¤ì›Œë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    # <<< END: keywords real



    # ------------------------
    # ê°ì • ë¶„í¬ (ì˜ˆì‹œ ë°ì´í„° ìœ ì§€)
    # ------------------------
    # >>> START: sentiment real
    # sumf = _bundle_path(bundle_id) / "sentiment.summary.json"
    # data = load_json_safe(sumf, default=None)

    # st.subheader("ğŸ§ª ê°ì • ë¶„í¬ (ì‹¤ì œ)")
    # if data and "ratio" in data:
    #     import plotly.express as px
    #     ratio = data["ratio"]             # {'positive': 40.0, ...}
    #     labels = {"positive":"ê¸ì •","negative":"ë¶€ì •","mixed":"ì¤‘ë¦½","neutral":"ê¸°íƒ€"}
    #     names  = [labels.get(k,k) for k in ratio.keys()]
    #     vals   = list(ratio.values())
    #     fig = px.pie(values=vals, names=names)
    #     fig.update_traces(textinfo="percent+label")
    #     st.plotly_chart(fig, use_container_width=True)
    #     st.caption(f"ì†ŒìŠ¤: {sumf}")
    # else:
    #     st.info("sentiment.summary.jsonì´ ì—†ìŠµë‹ˆë‹¤. (ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•„ìš”)")
    # <<< END: sentiment real


    # ------------------------
    # ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€ í°íŠ¸ ìë™íƒìƒ‰ + ê²€ì¦)
    # ------------------------
    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ (í•œê¸€)")
    # ì„¸ì…˜ì— í°íŠ¸ ê²½ë¡œ ì €ì¥í•´ë‘” ê²½ìš° ìš°ì„  ì‚¬ìš© (ì—†ìœ¼ë©´ ìë™íƒìƒ‰)
    custom_font = st.session_state.get("kofont") if "kofont" in st.session_state else ""
    font_path = resolve_korean_font(custom_font or None)

    if not font_path:
        st.error("í•œê¸€ ì§€ì› í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆ) Windows: C:\\Windows\\Fonts\\malgun.ttf")
    else:
        set_matplotlib_korean(font_path)
        corpus = " ".join(toks)
        if corpus.strip():
            wc = WordCloud(
                width=1000,
                height=500,
                background_color="white",
                font_path=font_path,
                prefer_horizontal=1.0,
                collocations=False
            ).generate(corpus)
            fig2, ax2 = plt.subplots(figsize=(10, 5))
            ax2.imshow(wc, interpolation="bilinear")
            ax2.axis("off")
            st.caption(f"font: {font_path}")
            st.pyplot(fig2, clear_figure=True)
        else:
            st.warning("ìœ íš¨í•œ í•œê¸€ í† í°ì´ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ------------------------
    # í”„ë¦¬ë·°(ê¸°ì‚¬ë³„/ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬) & ì €ì¥
    # ------------------------
    # >>> START: network real
    import pandas as pd
    import networkx as nx
    import matplotlib
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    # í•œê¸€ í°íŠ¸ (Windows ê¸°ì¤€: Malgun Gothic)
    matplotlib.rc("font", family="Malgun Gothic")
    matplotlib.rcParams["axes.unicode_minus"] = False

    netp = _bundle_path(bundle_id) / "network.json"
    net = load_json_safe(netp, default=None)
    st.subheader("ğŸŒ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í”„ë¦¬ë·°")

    if net:
        nodes = net.get("nodes", {})
        edges = net.get("edges", [])
        c_nodes, c_edges = len(nodes), len(edges)
        c_nonzero = sum(1 for e in edges if e.get("weight", 0) > 0)

        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        col1, col2, col3 = st.columns(3)
        col1.metric("ë…¸ë“œ ìˆ˜", c_nodes)
        col2.metric("ì—£ì§€ ìˆ˜", c_edges)
        col3.metric("ì–‘ì˜ ê°€ì¤‘ì¹˜ ì—£ì§€", c_nonzero)

        if len(edges) > 0:
            df_e = pd.DataFrame(edges)
            if "weight" in df_e.columns and "co" in df_e.columns:
                df_e = df_e.sort_values(["weight", "co"], ascending=False).head(50)
            else:
                df_e = df_e.head(50)
            st.dataframe(df_e, use_container_width=True, height=360)

            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            for e in df_e.to_dict("records"):
                if "source" in e and "target" in e:  # ë°©ì–´ì  ì²´í¬
                    G.add_edge(e["source"], e["target"], weight=e.get("weight", 1))

            # âš ï¸ ì—¬ê¸°ì„œ ë°˜ë“œì‹œ í™•ì¸
            if G.number_of_nodes() > 0:
                degrees = dict(G.degree())
                weights = [G[u][v].get("weight", 1) for u, v in G.edges()]

                plt.figure(figsize=(6, 4))
                pos = nx.kamada_kawai_layout(G)

                nx.draw_networkx_nodes(
                    G, pos,
                    node_color="skyblue",
                    node_size=[degrees.get(n, 1) * 120 for n in G.nodes()],
                    alpha=0.8
                )
                nx.draw_networkx_edges(
                    G, pos,
                    edge_color="gray",
                    width=[w * 0.15 for w in weights],
                    alpha=0.5
                )

                # ë¼ë²¨ í•œê¸€ ê¹¨ì§ ë°©ì§€
                import matplotlib.font_manager as fm
                font_path = "C:/Windows/Fonts/malgun.ttf"
                fontprop = fm.FontProperties(fname=font_path)

                labels = {n: n for n in G.nodes() if not str(n).isdigit()}
                nx.draw_networkx_labels(
                    G, pos, labels,
                    font_size=9,
                    font_family=fontprop.get_name()
                )

                plt.axis("off")
                st.pyplot(plt, clear_figure=True)
            else:
                st.info("ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì—£ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. min_count/window íŒŒë¼ë¯¸í„°ë¥¼ ë‚®ì¶° ì¬ìƒì„±í•˜ì„¸ìš”.")
    else:
        st.info("network.jsonì´ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")


    # ğŸ“‘ í”„ë ˆì„ í‘œ(Frames Table)

    st.subheader("ğŸ“‘ í”„ë ˆì„ í‘œ (ì‹¤ë°ì´í„°)")
    p_frames = _bundle_path(bundle_id) / "frames.jsonl"
    df_frames = read_jsonl_safe(p_frames)

    if not df_frames.empty:
        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì¶”ì¶œ
        for c in ("frames","frame_preds","frame_labels","labels"):
            if c in df_frames.columns:
                df_frames["frames_pretty"] = df_frames[c].apply(_frames_pretty)
                break
        # ëŒ€í‘œ í”„ë ˆì„/ì ìˆ˜
        if "frames_pretty" not in df_frames:
            df_frames["frames_pretty"] = None
        # ë³´ì—¬ì¤„ ì»¬ëŸ¼ êµ¬ì„±(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
        show = [c for c in ["url","title","frames_pretty","fused_score","date","id"] if c in df_frames.columns]
        st.dataframe(df_frames[show].head(200), use_container_width=True, height=420)
        st.caption(f"ì†ŒìŠ¤: {p_frames}")
    else:
        st.info("frames.jsonlì´ ì—†ìŠµë‹ˆë‹¤. (í‚¤ì›Œë“œâ†’ë£°ë¶€ìŠ¤íŠ¸â†’í”„ë ˆì„ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•„ìš”)")

# ==========================
# 4) Gate (í”„ë ˆì„â†’ì§€í‘œì ìˆ˜)
# ==========================
def page_gate(bundle_id: str):
    st.header("ğŸšª Gate â€“ í”„ë ˆì„ ê¸°ë°˜ ì²´í¬(Indicator)")

    df = load_jsonl(f"data/{bundle_id}/analyze.jsonl")
    if df.empty or "frames" not in df.columns:
        st.warning("ë¨¼ì € Analyze ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ì‹¤ì œ indicator_scorer í˜¸ì¶œ
    df["indicator_score"] = df["frames"].apply(indicator_scorer.run)
    st.subheader("í”„ë ˆì„ & ì§€í‘œ ì ìˆ˜")
    st.dataframe(df[["normalized", "frames", "indicator_score"]].head(), width='stretch')

    save_jsonl(df, f"data/{bundle_id}/gate.jsonl")
    st.success("âœ… Gate ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# ==========================
# 5) Scoring (DBN + Fused)
# ==========================
def page_scoring(bundle_id: str):
    st.header("ğŸ“Š Scoring â€“ DBN ì¶”ë¡  + ìœµí•© ì ìˆ˜ + ì˜í–¥/ì‹œí”„íŠ¸")

    df = load_jsonl(f"data/{bundle_id}/gate.jsonl")
    if df.empty or "frames" not in df.columns or "indicator_score" not in df.columns:
        st.warning("ë¨¼ì € Gate ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    ind_scores, dbn_scores, fused_scores = [], [], []
    impacts, shifts = [], []
    history = []  # ì§ì „ í”„ë ˆì„ ìŠ¤ëƒ…ìƒ· ëˆ„ì 

    for _, row in df.iterrows():
        frames = row.get("frames", [])
        ind = float(row.get("indicator_score", 0.0))

        # DBN(ê°„ì´): ì§ì „ ìŠ¤ëƒ…ìƒ·ê³¼ ë¹„êµ
        prev_frames = history[-1] if history else None
        dbn_score = dbn_inference.run(frames, prev_frames=prev_frames)

        # Fused
        fused = fuse.combine([ind, dbn_score])

        # âœ… ì˜í–¥ ë¶„ì„(impact_analyzer)
        imp = impact_analyzer.run(frames, fused)

        # âœ… í”„ë ˆì„ ì‹œí”„íŠ¸(frame_shift_detector)
        if history:
            sh = frame_shift_detector.detect(history + [frames])
        else:
            sh = {"shift_detected": False, "shift_score": 0.0, "changed": []}

        # ëˆ„ì 
        history.append(frames)
        ind_scores.append(ind); dbn_scores.append(dbn_score); fused_scores.append(fused)
        impacts.append(imp); shifts.append(sh)

    df["dbn_score"]   = dbn_scores
    df["fused_score"] = fused_scores
    df["impact"]      = impacts
    df["shift"]       = shifts

    st.subheader("ì ìˆ˜ & ë¶„ì„ ìš”ì•½")
    st.dataframe(df[["indicator_score","dbn_score","fused_score","impact","shift"]].head(), width='stretch')

    # ì‹œê³„ì—´
    df["date"] = pd.to_datetime(df.get("date", datetime.today().strftime("%Y-%m-%d")))
    fig = px.line(df, x="date", y=["indicator_score", "dbn_score", "fused_score"], title="ì ìˆ˜ ì‹œê³„ì—´")
    st.plotly_chart(fig, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/scoring.jsonl")
    st.success("âœ… Scoring ê²°ê³¼ ì €ì¥ ì™„ë£Œ")


# ==========================
# 6) Fusion (ì‹œê° ë¹„êµ)
# ==========================
def page_fusion(bundle_id: str):
    st.header("âš¡ Fusion â€“ ì ìˆ˜ ë¹„êµ/ê²€ì¦")

    df = load_jsonl(f"data/{bundle_id}/scoring.jsonl")
    if df.empty or "fused_score" not in df.columns:
        st.warning("ë¨¼ì € Scoring ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    fig = px.scatter(df, x="indicator_score", y="fused_score", trendline="ols",
                     title="Indicator vs Fused")
    st.plotly_chart(fig, width='stretch')

    st.dataframe(df[["indicator_score", "dbn_score", "fused_score"]].head(), width='stretch')
    save_jsonl(df, f"data/{bundle_id}/fusion.jsonl")
    st.success("âœ… Fusion ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# ==========================
# 7) Blocks (ì£¼ í”„ë ˆì„ ê¸°ë°˜ ë¸”ë¡)
# ==========================
def _primary_frame(fs: list[dict]) -> str:
    if not fs:
        return "General"
    # score ê°€ì¥ í° í”„ë ˆì„
    best = sorted(fs, key=lambda f: f.get("score", 0), reverse=True)[0]
    return best.get("frame", "General")

def page_blocks(bundle_id: str):
    st.header("ğŸ§© Blocks (EDS) â€“ ê¸°ì‚¬ â†” ë¸”ë¡ ë§¤í•‘")

    df = load_jsonl(f"data/{bundle_id}/fusion.jsonl")
    if df.empty or "frames" not in df.columns:
        st.warning("ë¨¼ì € Fusion ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    df["block"] = df["frames"].apply(_primary_frame)

    # âœ… í•µì‹¬ ì»¬ëŸ¼ ë³´ì¡´ ë³´ê°•
    keep_cols = [c for c in ["url","date","normalized","frames","indicator_score","dbn_score","fused_score"] if c in df.columns]
    out = df[keep_cols + ["block"]].copy()

    st.subheader("ê¸°ì‚¬ â†” ë¸”ë¡ ë§¤í•‘")
    st.dataframe(out.head(), width='stretch')

    save_jsonl(out, f"data/{bundle_id}/blocks.jsonl")
    st.success("âœ… Blocks ì €ì¥ ì™„ë£Œ")

# ==========================
# 8) Scenarios (ë§¤ì¹­ + ì˜ˆì¸¡)
# ==========================
def page_scenarios(bundle_id: str):
    st.header("ğŸ“‘ Scenarios â€“ ë§¤ì¹­/ì˜ˆì¸¡")

    df = load_jsonl(f"data/{bundle_id}/blocks.jsonl")
    if df.empty or "normalized" not in df.columns:
        st.warning("ë¨¼ì € Blocks ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ì‹¤ì œ ì„ë² ë”© â†’ ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­ & ì˜ˆì¸¡
    matched_list, predicted_list = [], []
    for _, row in df.iterrows():
        sents = row.get("sentences", [])
        if not sents:
            sents = tokenizer.split_sentences(row.get("normalized", ""))
        vecs = embedder.embed(sents)
        matched = scenario_matcher.match(vecs, top_k=3)
        pred = scenario_predictor.generate(row.get("normalized", ""))
        matched_list.append(matched)
        predicted_list.append(pred)

    df["scenario_matched"] = matched_list
    df["scenario_predicted"] = predicted_list

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ìœ„í—˜ë„
    df["top_sim"] = df["scenario_matched"].apply(lambda xs: xs[0]["similarity"] if xs else 0.0)
    top = df[["block", "fused_score", "top_sim"]].groupby("block", observed=True).mean().reset_index()
    st.subheader("ë¸”ë¡ë³„ í‰ê·  ì ìˆ˜/ìœ ì‚¬ë„")
    st.dataframe(top, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/scenarios.jsonl")
    st.success("âœ… Scenarios ì €ì¥ ì™„ë£Œ")

# ==========================
# 9) Alerts (ìµœì¢… ê²½ë³´)
# ==========================
def page_alerts(bundle_id: str):
    st.header("ğŸš¨ Alerts â€“ ê²½ë³´ ë°œìƒ")

    df = load_jsonl(f"data/{bundle_id}/scenarios.jsonl")
    if df.empty or "fused_score" not in df.columns:
        st.warning("ë¨¼ì € Scenarios ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # âœ… LLM Judge ì„¤ëª… ìƒì„±(ì ìˆ˜+í”„ë ˆì„)
    def _scores_of(r):
        return {
            "indicator": float(r.get("indicator_score", 0.0)),
            "dbn":       float(r.get("dbn_score", 0.0)),
            "fused":     float(r.get("fused_score", 0.0)),
        }

    df["explanation"] = df.apply(lambda r: llm_judge.explain(_scores_of(r), r.get("frames", [])), axis=1)

    # âœ… ìµœì¢… ê²½ë³´ ê²°ì •(ê¸°ì¡´ ìœ ì§€)
    df["decision"] = df["fused_score"].apply(alert_decider.decide)

    def to_level(dec: str) -> str:
        if "High" in dec: return "High"
        if "Medium" in dec: return "Medium"
        return "Low"
    df["alert_level"] = df["decision"].apply(to_level)

    st.subheader("ê²½ë³´ ê²°ê³¼")
    st.dataframe(df[["normalized","fused_score","decision","alert_level","explanation"]].head(), width='stretch')

    fig = px.histogram(df, x="alert_level", title="ê²½ë³´ ë¶„í¬")
    st.plotly_chart(fig, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/alerts.jsonl")
    st.success("âœ… Alerts ì €ì¥ ì™„ë£Œ")


# ==========================
# 10) Event Blocks (ê°„ì´ í´ëŸ¬ìŠ¤í„°ë§)
# ==========================
def page_eventblocks(bundle_id: str):
    st.header("ğŸ“¦ Event Blocks â€“ í´ëŸ¬ìŠ¤í„°ë§(ê°„ì´)")

    df = load_jsonl(f"data/{bundle_id}/alerts.jsonl")
    if df.empty:
        st.warning("ë¨¼ì € Alerts ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ê°„ì´ í´ëŸ¬ìŠ¤í„°
    df["cluster"] = df.index % 3
    pivot = df.groupby("cluster", observed=True)["fused_score"].mean().reset_index()

    st.subheader("í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ìœ„í—˜ë„")
    st.dataframe(pivot, width='stretch')

    fig = px.imshow([pivot["fused_score"].tolist()],
                    labels=dict(x="Cluster", y="Risk", color="Score"),
                    title="Event Block ìœ„í—˜ë„ íˆíŠ¸ë§µ")
    st.plotly_chart(fig, width='stretch')

    save_jsonl(df, f"data/{bundle_id}/eventblocks.jsonl")
    st.success("âœ… Event Blocks ì €ì¥ ì™„ë£Œ")

# ==========================
# 11) Ledger (íŒŒì¼ ë¡œê·¸)
# ==========================
def page_ledger(bundle_id: str):
    st.header("ğŸ“œ Ledger â€“ ë‹¨ê³„ë³„ ë¡œê·¸")

    files = [
        "ingest.jsonl","normalize.jsonl","analyze.jsonl","gate.jsonl",
        "scoring.jsonl","fusion.jsonl","blocks.jsonl","scenarios.jsonl",
        "alerts.jsonl","eventblocks.jsonl"
    ]
    logs = []
    for f in files:
        path = f"data/{bundle_id}/{f}"
        if os.path.exists(path):
            logs.append({"file": f, "size": os.path.getsize(path)})
    st.dataframe(pd.DataFrame(logs), width='stretch')

    col1, col2 = st.columns(2)
    if col1.button("ğŸ“‚ alerts.jsonl ì—´ê¸°(viewer)"):
        try:
            st.dataframe(viewer.load(bundle_id, "alerts.jsonl"), width='stretch')
        except Exception as e:
            st.warning(f"viewer.load ì‹¤íŒ¨: {e}")

    if col2.button("ğŸ“¥ ledger.jsonl ì €ì¥"):
        ledger = {"bundle": bundle_id, "timestamp": datetime.now().isoformat(), "steps": files}
        save_jsonl(pd.DataFrame([ledger]), f"data/{bundle_id}/ledger.jsonl")
        st.success("ledger.jsonl ì €ì¥ ì™„ë£Œ")



# ==========================
# Overview (ëŒ€ì‹œë³´ë“œ)
# ==========================
def page_overview(bundle_id: str):
    st.title("ğŸŒ Crisis Overview")
    # === [ADD] DAG badges (counts) ===
    bundle_path = _bundle_path(bundle_id)

    STAGES = [
        ("ingest", "ingest.jsonl"),
        ("clean", "clean.jsonl"),
        ("dedup", ("clean.dedup.jsonl", "clean.cheap.jsonl")),
        ("keyword", "kyw.jsonl"),
        ("kboost", "kyw_boosted.jsonl"),
        ("frame", "frames.jsonl"),
        ("score", "scores.jsonl"),
        ("alert", "alerts.jsonl"),
    ]

    st.markdown("### Pipeline DAG")
    cols = st.columns(len(STAGES))
    for i, (name, fns) in enumerate(STAGES):
        if isinstance(fns, tuple):
            exist = [(bundle_path/x) for x in fns if (bundle_path/x).exists()]
            n = max((_count_lines(p) for p in exist), default=0)
        else:
            p = bundle_path / fns
            n = _count_lines(p)
        icon = "âœ…" if n > 0 else "âŒ"
        cols[i].metric(label=name, value=n, help=f"{icon} {name}")
    # === [/ADD] ===

    # === [ADD] Stage QA tiles ===
    st.markdown("### Stage QA")
    qc_path = bundle_path / "prep.qc.json"
    if qc_path.exists():
        qc = json.loads(qc_path.read_text(encoding="utf-8"))
    else:
        # Fallback: clean.jsonlì—ì„œ ê°„ì´ ê³„ì‚°
        import statistics as stats
        vals_c, vals_s = [], []
        p = bundle_path / "clean.jsonl"
        if p.exists():
            with p.open(encoding="utf-8-sig") as f:
                for i, line in enumerate(f):
                    if i > 2000: break
                    r = json.loads(line)
                    t = r.get("clean_text") or r.get("body") or r.get("text") or ""
                    vals_c.append(len(str(t)))
                    vals_s.append(int(r.get("num_sents") or 1))
        qc = {
            "chars_median": int(stats.median(vals_c)) if vals_c else 0,
            "sents_median": int(stats.median(vals_s)) if vals_s else 0,
            "short_docs_pct": round(100*sum(1 for x in vals_c if x < 200)/len(vals_c), 1) if vals_c else 100.0
        }

    c1, c2, c3 = st.columns(3)
    c1.metric("Chars(med)", qc["chars_median"])
    c2.metric("Sents(med)", qc["sents_median"])
    c3.metric("Short Docs%", f'{qc["short_docs_pct"]}%')

    g1 = "âœ…" if qc["chars_median"] >= 800 else ("âš ï¸" if qc["chars_median"] >= 600 else "âŒ")
    g2 = "âœ…" if qc["sents_median"] >= 8 else ("âš ï¸" if qc["sents_median"] >= 6 else "âŒ")
    g3 = "âœ…" if qc["short_docs_pct"] <= 20 else ("âš ï¸" if qc["short_docs_pct"] <= 30 else "âŒ")
    st.caption(f"Gate: ë³¸ë¬¸ {g1} / ë¬¸ì¥ìˆ˜ {g2} / ì§§ì€ë¬¸ì„œ {g3}")
    # === [/ADD] ===





    # --- ë°ì´í„° ë¡œë“œ ---
    df_ingest  = load_jsonl(f"data/{bundle_id}/ingest.jsonl")
    df_norm    = load_jsonl(f"data/{bundle_id}/normalize.jsonl")
    df_ana     = load_jsonl(f"data/{bundle_id}/analyze.jsonl")
    df_gate    = load_jsonl(f"data/{bundle_id}/gate.jsonl")
    df_score   = load_jsonl(f"data/{bundle_id}/scoring.jsonl")
    df_fuse    = load_jsonl(f"data/{bundle_id}/fusion.jsonl")
    df_blocks  = load_jsonl(f"data/{bundle_id}/blocks.jsonl")
    df_scen    = load_jsonl(f"data/{bundle_id}/scenarios.jsonl")  # âœ… ë³µêµ¬
    df_alerts  = load_jsonl(f"data/{bundle_id}/alerts.jsonl")

    # --- KPI: ìˆ˜ì§‘ vs ê²½ë³´ & ì²˜ë¦¬ìœ¨ ---
    n_ingest = 0 if df_ingest.empty else len(df_ingest)
    n_alerts = 0 if df_alerts.empty else len(df_alerts)
    ratio    = (n_alerts / n_ingest) if n_ingest else 0.0

    k1, k2, k3 = st.columns(3)
    k1.metric("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ (ingest)", n_ingest)
    k2.metric("ê²½ë³´ ì‚°ì¶œ ê¸°ì‚¬ ìˆ˜ (alerts)", n_alerts)
    k3.metric("ì²˜ë¦¬ìœ¨", f"{ratio*100:.0f}%")
    st.progress(min(1.0, ratio))

        # --- ğŸ“¡ ì‹œê³„ì—´ ê²½ë³´(Alerts) ---
    st.subheader("ğŸ“¡ ì‹œê³„ì—´ ê²½ë³´ (Alerts)")
    # Alerts ì„¹ì…˜ì—ì„œ df_alerts ë¡œë“œ ì§í›„
    basis = st.selectbox("ì‹œê°„ ê¸°ì¤€ (Time basis)", ["ê²½ë³´ ìƒì„±ì‹œê°", "ì›ë¬¸ ì‹œê°"], index=0)

        # === [REPLACE] Alerts rendering ===
    ap = bundle_path / "alerts.jsonl"
    if ap.exists() and _count_lines(ap) > 0:
        from io import StringIO
        with open(ap, 'r', encoding='utf-8-sig') as f:
            content = f.read()
        if content.strip():
            df_alerts = pd.read_json(StringIO(content), lines=True)
        else:
            df_alerts = pd.DataFrame()
        # ì»¬ëŸ¼ ë§¤í•‘(UI í˜¸í™˜)
        if "date" not in df_alerts.columns and "t_on" in df_alerts.columns:
            df_alerts["date"] = df_alerts["t_on"]
        if "alert_level" not in df_alerts.columns and "level" in df_alerts.columns:
            df_alerts["alert_level"] = df_alerts["level"]

        # ì‹œê°„ ì •ë¦¬(KST naive)
        # ì›ë¬¸ ì‹œê° í›„ë³´ ë§¤í•‘(ìˆì„ ë•Œë§Œ)
        if "src_time" not in df_alerts:
            for c in ["published_at", "doc_time", "event_time"]:
                if c in df_alerts.columns:
                    df_alerts["src_time"] = df_alerts[c]
                    break

        # ì„ íƒí•œ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„ ì»¬ëŸ¼ ê²°ì •
        time_col = "date" if basis == "ê²½ë³´ ìƒì„±ì‹œê°" else "src_time"

        if time_col in df_alerts.columns:
            df_alerts[time_col] = _to_local_naive(df_alerts[time_col], output_tz="Asia/Seoul")
            # ë‚´ë¶€ timeseries í•¨ìˆ˜ê°€ 'date'ë¥¼ ê¸°ëŒ€í•˜ë©´ ë¦¬ë„¤ì„í•´ì„œ ë„˜ê¸°ê¸°
            ts_long = _alert_timeseries(df_alerts.rename(columns={time_col: "date"}), freq="D")
        else:
            st.info("ì„ íƒí•œ ì‹œê°„ ê¸°ì¤€ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (date/t_on ë˜ëŠ” src_time/published_at)")

        # ì¹´ë“œ 5ê°œë§Œ
        for _, r in df_alerts.head(5).iterrows():
            lvl = str(r.get("alert_level", "watch")).upper()
            sc  = float(r.get("fused_score", 0.0))
            rsn = r.get("reason") or "(no reason)"
            st.markdown(f"**[{lvl}]** {rsn} Â· score={sc:.2f} Â· {r.get('date','')}")
    else:
        st.info("Alerts ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (âš¡ scoresâ†’alerts ìƒì„± í›„ Rerun)")
    # === [/REPLACE] ===


    if not df_alerts.empty and {"date","alert_level"}.issubset(df_alerts.columns):
        freq_label = st.selectbox("ì§‘ê³„ ì£¼ê¸°", ["ì¼", "ì£¼", "ì›”"], index=0, key="alert_ts_freq")
        freq_map = {"ì¼":"D","ì£¼":"W","ì›”":"M"}
        freq = freq_map[freq_label]

        ts_long = _alert_timeseries(df_alerts, freq=freq)
        ts_wide = _to_wide(ts_long)
        ts_wide = ts_wide.copy()
        ts_wide["Total"] = ts_wide["High"] + ts_wide["Medium"] + ts_wide["Low"]

        # ìŠ¤ë¬´ë”© ì˜µì…˜(rolling)
        smooth = st.checkbox("7-ì°½ ìŠ¤ë¬´ë”©(rolling mean)", value=(freq=="D"), key="alert_ts_smooth")
        if smooth and not ts_wide.empty:
            ts_wide_sm = ts_wide.rolling(window=7, min_periods=1).mean()
        else:
            ts_wide_sm = ts_wide

        # íƒ­: ìŠ¤íƒë©´ì  / ë ˆë²¨ì„  / ëˆ„ì  / í‘œ
        tabs_ts = st.tabs(["ğŸŸ£ ìŠ¤íƒ ë©´ì ", "ğŸ“ˆ ë ˆë²¨ë³„ ì„ ", "â• ëˆ„ì  í•©", "ğŸ“„ í‘œ"])

        with tabs_ts[0]:
            # long-formì´ areaì— ì í•©
            fig_area = px.area(
                ts_long, x="date", y="count", color="alert_level",
                title=f"ê²½ë³´ ìˆ˜ ìŠ¤íƒ ë©´ì  ({freq_label} ë‹¨ìœ„)",
                color_discrete_sequence=next_palette()
            )
            st.plotly_chart(fig_area, width='stretch')

        with tabs_ts[1]:
            df_plot = reset_index_as_date(ts_wide_sm)
            fig_line = px.line(
                df_plot, x="date", y=["High","Medium","Low"],
                title=f"ê²½ë³´ ë ˆë²¨ë³„ ì¶”ì„¸ ({freq_label} ë‹¨ìœ„){' - 7ì°½ ìŠ¤ë¬´ë”©' if smooth else ''}",
                color_discrete_sequence=next_palette()
            )
            st.plotly_chart(fig_line, width='stretch')

        with tabs_ts[2]:
            df_cum  = reset_index_as_date(ts_wide.cumsum())
            fig_cum = px.line(
                df_cum, x="date", y=["High","Medium","Low","Total"],
                title=f"ê²½ë³´ ëˆ„ì  í•© ({freq_label} ë‹¨ìœ„)",
                color_discrete_sequence=next_palette()
            )
            st.plotly_chart(fig_cum, width='stretch')

        with tabs_ts[3]:
            # í‘œëŠ” ì›ì‹œ wide + Total ì œê³µ
            st.dataframe(ts_wide.reset_index(), width='stretch')
    else:
        st.info("ì‹œê³„ì—´ ê²½ë³´ë¥¼ ìœ„í•´ì„œëŠ” df_alertsì— 'date'ì™€ 'alert_level' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    
    # --- í‰ê·  ìœ„í—˜ë„ & ì¶”ì„¸ ---
    if not df_alerts.empty:
        df_alerts = _ensure_fused(df_alerts, bundle_id)
        avg_score = df_alerts["fused_score"].mean()
        st.caption(f"í‰ê·  ìœ„í—˜ë„(Alerts ê¸°ì¤€): {avg_score:.2f}")

        if "date" in df_alerts.columns:
            try:
                df_alerts["date"] = pd.to_datetime(df_alerts["date"]).dt.date
                trend = df_alerts.groupby("date", observed=True).size().reset_index(name="count")
                tabs_trend = st.tabs(["ğŸ“ˆ ì•Œë¦¼ ì¶”ì„¸(ì¼ë³„ ê±´ìˆ˜)", "ğŸ“‰ ì¼í‰ê·  ìœ„í—˜ë„"])
                with tabs_trend[0]:
                    fig_trend = px.line(
                        trend, x="date", y="count", markers=True,
                        title="ğŸ“ˆ ìµœê·¼ ì•Œë¦¼ ì¶”ì„¸(ì¼ë³„ ê±´ìˆ˜)",
                        color_discrete_sequence=next_palette()
                    )
                    st.plotly_chart(fig_trend, width='stretch')
                with tabs_trend[1]:
                    daily = df_alerts.groupby("date", observed=True)["fused_score"].mean().reset_index()
                    fig_avg = px.line(
                        daily, x="date", y="fused_score", markers=True,
                        title="ğŸ“‰ ì¼í‰ê·  ìœ„í—˜ë„(fused_score)",
                        color_discrete_sequence=next_palette()
                    )
                    st.plotly_chart(fig_avg, width='stretch')
            except Exception:
                pass

        # ê²½ë³´ ë“±ê¸‰ ë¶„í¬ (íƒ­)
        if "alert_level" in df_alerts.columns:
            lvl = df_alerts["alert_level"].value_counts().reset_index()
            lvl.columns = ["level", "count"]
            lvl_tabs = st.tabs(["ğŸ“Š Bar", "ğŸ¥§ Pie", "ğŸ“„ í‘œ"])
            with lvl_tabs[0]:
                fig_lvl_bar = px.bar(
                    lvl, x="level", y="count", text="count",
                    title="ğŸš¨ ê²½ë³´ ë“±ê¸‰ ë¶„í¬ (Bar)",
                    color_discrete_sequence=next_palette()
                )
                fig_lvl_bar.update_traces(textposition="outside")
                st.plotly_chart(fig_lvl_bar, width='stretch')
            with lvl_tabs[1]:
                fig_lvl_pie = px.pie(
                    lvl, names="level", values="count",
                    title="ğŸš¨ ê²½ë³´ ë“±ê¸‰ ë¶„í¬ (Pie)",
                    color_discrete_sequence=next_palette()
                )
                st.plotly_chart(fig_lvl_pie, width='stretch')
            with lvl_tabs[2]:
                st.dataframe(lvl, width='stretch')
    else:
        st.info("Alerts ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ë¡œ ìƒì„± ê°€ëŠ¥)")

    # --- Top ìœ„í—˜ ë¸”ë¡ (ë°±í•„ í¬í•¨, íƒ­) ---
    st.subheader("ğŸ”¥ Top ìœ„í—˜ ë¸”ë¡")
    if not df_blocks.empty:
        df_blocks = backfill_fused_score(df_blocks, bundle_id)
        if "fused_score" in df_blocks.columns and "block" in df_blocks.columns:
            top_blk = (df_blocks.groupby("block", observed=True)["fused_score"]
                       .mean().reset_index()
                       .sort_values("fused_score", ascending=False)
                       .head(10))
            tabs_blk = st.tabs(["ğŸ“Š ì°¨íŠ¸", "ğŸ“„ í‘œ"])
            with tabs_blk[0]:
                fig_blk = px.bar(
                    top_blk, x="block", y="fused_score", text="fused_score",
                    title="ë¸”ë¡ë³„ í‰ê·  ìœ„í—˜ë„ (Top 10)",
                    labels={"block": "ë¸”ë¡", "fused_score": "í‰ê·  ìœ„í—˜ë„"},
                    color_discrete_sequence=next_palette()
                )
                fig_blk.update_traces(texttemplate="%{text:.2f}", textposition="outside")
                st.plotly_chart(fig_blk, width='stretch')
            with tabs_blk[1]:
                st.dataframe(top_blk, width='stretch')
        else:
            st.info("ë¸”ë¡ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    else:
        st.info("ë¸”ë¡ ë°ì´í„° ì—†ìŒ")

    # --- ğŸ§­ ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ (í‘œë§Œ) ---
    st.subheader("ğŸ§­ ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ (í‘œ)")
    if not df_scen.empty:
        df_scen = _ensure_fused(df_scen, bundle_id)

        # ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì¶”ì¶œ ìœ í‹¸
        def _scen_name(v):
            if isinstance(v, dict):
                return v.get("scenario") or v.get("title") or str(v)
            if isinstance(v, list) and v:
                first = v[0]
                if isinstance(first, dict):
                    return first.get("title") or first.get("scenario") or str(first)
                return str(first)
            if isinstance(v, str):
                return v
            return "Unknown"

        scen_col = "scenario_predicted" if "scenario_predicted" in df_scen.columns else (
            "scenario_matched" if "scenario_matched" in df_scen.columns else None
        )

        if scen_col:
            tmp = df_scen.copy()
            tmp["scenario_name"] = tmp[scen_col].apply(_scen_name)

            # í‰ê·  ìœ„í—˜ë„ ê¸°ì¤€ Top-N í‘œ
            topn = st.slider("í‘œì‹œ ê°œìˆ˜(Top-N)", 5, 50, 20, 5, key="topn_scen")
            top_scen = (tmp.groupby("scenario_name", observed=True)["fused_score"]
                        .mean().reset_index()
                        .sort_values("fused_score", ascending=False)
                        .head(topn))
            st.dataframe(
                top_scen.rename(columns={"scenario_name":"ì‹œë‚˜ë¦¬ì˜¤", "fused_score":"í‰ê·  ìœ„í—˜ë„"}),
                width='stretch'
            )

            # ìµœê·¼ Nê±´ ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°(ì˜µì…˜)
            with st.expander("ìµœê·¼ ì‹œë‚˜ë¦¬ì˜¤ ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                cols = [c for c in ["date","title","scenario_predicted","scenario_matched","fused_score"] if c in tmp.columns]
                st.dataframe(tmp.sort_values("date", ascending=False)[cols].head(topn), width='stretch')
        else:
            st.info("ì‹œë‚˜ë¦¬ì˜¤ ì»¬ëŸ¼(scenario_predicted / scenario_matched)ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("Scenarios ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ë¡œ ìƒì„± ê°€ëŠ¥)")


    # --- ğŸ—ºï¸ ëŒ€í•œë¯¼êµ­ ì‹œêµ°êµ¬ Choropleth (íƒ­) ---
    st.subheader("ğŸ—ºï¸ ëŒ€í•œë¯¼êµ­ ì‹œêµ°êµ¬ ë¶„í¬ (Choropleth)")
    geo_tabs = st.tabs(["ğŸ—ºï¸ ì§€ë„", "ğŸ“„ í‘œ"])
    with geo_tabs[0]:
        geo_path = st.session_state.get("siggeo", "")
        geo = load_geojson(geo_path) if geo_path else None
        if not df_alerts.empty and geo is not None:
            # df_alertsì— sig_cd í™•ë³´ (ì—†ìœ¼ë©´ region/sigunguì—ì„œ ìœ ì¶”)
            df_geo = ensure_sig_cd(df_alerts.copy(), geo)
            if "sig_cd" in df_geo.columns and df_geo["sig_cd"].notna().any():
                agg = (df_geo.dropna(subset=["sig_cd"])
                            .groupby("sig_cd", observed=True)
                            .size().reset_index(name="count"))
                code_key, _name_key = _detect_sig_props(geo)
                featureidkey = f"properties.{code_key}" if code_key else None
                fig_ch = px.choropleth(
                    agg, geojson=geo, locations="sig_cd", featureidkey=featureidkey,
                    color="count", color_continuous_scale=next_cscale(),
                    title="ì‹œêµ°êµ¬ë³„ ê²½ë³´ ê¸°ì‚¬ ë¶„í¬"
                )
                fig_ch.update_geos(fitbounds="locations", visible=False)
                st.plotly_chart(fig_ch, width='stretch')
            else:
                st.info("df_alertsì— sig_cdë¥¼ ìœ ì¶”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'sigungu' ë˜ëŠ” 'region' ì»¬ëŸ¼ì„ ì±„ìš°ê±°ë‚˜, dfì— 'sig_cd'(5ìë¦¬ í–‰ì •ì½”ë“œ)ë¥¼ ì§ì ‘ ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            st.info("GeoJSON ê²½ë¡œê°€ ë¹„ì—ˆê±°ë‚˜ Alerts ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ GeoJSON ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
    with geo_tabs[1]:
        # í‘œëŠ” ì§‘ê³„ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
        geo_path = st.session_state.get("siggeo", "")
        geo = load_geojson(geo_path) if geo_path else None
        if not df_alerts.empty and geo is not None:
            df_geo = ensure_sig_cd(df_alerts.copy(), geo)
            if "sig_cd" in df_geo.columns and df_geo["sig_cd"].notna().any():
                st.dataframe(
                    df_geo.dropna(subset=["sig_cd"]).groupby("sig_cd", observed=True).size().reset_index(name="count").sort_values("count", ascending=False),
                    width='stretch'
                )
            else:
                st.info("ì§‘ê³„ í‘œë¥¼ ë§Œë“¤ sig_cdê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("GeoJSON/Alerts ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # --- ğŸ·ï¸ ì—”í‹°í‹° ìƒìœ„ Top-N (analyze.jsonlì˜ network ê¸°ë°˜) ---
    st.subheader("ğŸ·ï¸ ì—”í‹°í‹° ìƒìœ„ Top-N")
    ent_tabs = st.tabs(["ğŸ“Š ì°¨íŠ¸", "ğŸ“„ í‘œ"])
    if not df_ana.empty and "network" in df_ana.columns:
        from collections import Counter
        counter = Counter()
        for g in df_ana["network"].dropna():
            try:
                nodes = (g or {}).get("nodes", {})
                counter.update({k: int(v) for k, v in nodes.items()})
            except Exception:
                continue
        topn = st.slider("í‘œì‹œ ê°œìˆ˜", 5, 50, 20, 5)
        ent_df = pd.DataFrame(counter.most_common(topn), columns=["entity","count"])
        with ent_tabs[0]:
            fig_ent = px.bar(
                ent_df, x="entity", y="count", text="count",
                title=f"ì—”í‹°í‹° ìƒìœ„ Top-{topn}",
                color_discrete_sequence=next_palette()
            )
            fig_ent.update_traces(textposition="outside")
            st.plotly_chart(fig_ent, width='stretch')
        with ent_tabs[1]:
            st.dataframe(ent_df, width='stretch')
    else:
        with ent_tabs[0]:
            st.info("Analyze ë°ì´í„°ì— ë„¤íŠ¸ì›Œí¬ ì»¬ëŸ¼ì´ ì—†ì–´ ì—”í‹°í‹° ìƒìœ„ Top-Nì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        with ent_tabs[1]:
            st.empty()

    # --- ğŸ“° ìˆ˜ì§‘ ì†ŒìŠ¤ ë¶„í¬ (íƒ­) ---
    st.subheader("ğŸ“° ìˆ˜ì§‘ ì†ŒìŠ¤ ë¶„í¬")
    if not df_ingest.empty and "source" in df_ingest.columns:
        src = df_ingest["source"].fillna("Manual").value_counts().reset_index()
        src.columns = ["source", "count"]
        src_tabs = st.tabs(["ğŸ“Š ì°¨íŠ¸", "ğŸ“„ í‘œ"])
        with src_tabs[0]:
            fig_src = px.bar(
                src.head(30), x="source", y="count", text="count",
                title="ì–¸ë¡ ì‚¬/ë„ë©”ì¸ ë¶„í¬ (Top 30)",
                labels={"source": "ì†ŒìŠ¤", "count": "ê±´ìˆ˜"},
                color_discrete_sequence=next_palette()
            )
            fig_src.update_traces(textposition="outside")
            st.plotly_chart(fig_src, width='stretch')
        with src_tabs[1]:
            st.dataframe(src, width='stretch')
    else:
        st.info("Ingestì˜ source ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ ë²„íŠ¼ ---
    st.divider()
    colA, colB = st.columns([1, 3])
    with colA:
        if st.button("âš¡ ë°±ë¡œê·¸ ì¼ê´„ ì²˜ë¦¬ (ingest â†’ alerts)"):
            ok = _quick_pipeline(bundle_id)
            if ok:
                st.success("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: ingest â†’ alerts ìƒì„±")
            else:
                st.warning("ingest ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            _safe_rerun()



# ==========================
# Main
# ==========================
def main():
    st.set_page_config(page_title="NWW Early Warning Dashboard", layout="wide")

    st.sidebar.header("âš™ï¸ ì„¤ì •")
    bundle_id = st.sidebar.text_input("Bundle ID", DEFAULT_BUNDLE)
    page = st.sidebar.radio("ğŸ“Œ ë‹¨ê³„ ì´ë™", [
        "Landing", "Ingest", "Normalize", "Analyze", "Gate", "Scoring",
        "Risk", "Alerts","EventBlocks","Fusion", "Blocks", "Scenarios", "Ledger"
    ])
    
    # === [SIDEBAR] GeoJSON ì…ë ¥ & ì§„ë‹¨ ===
    import io, json, os

    st.sidebar.markdown("### ğŸ—ºï¸ ì‹œêµ°êµ¬ GeoJSON")
    geojson_file = st.sidebar.file_uploader("íŒŒì¼ ì—…ë¡œë“œ(.geojson/.json)", type=["geojson", "json"])
    geojson_path = st.sidebar.text_input("ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ì…ë ¥", value=st.session_state.get("siggeo_path", ""))

    # ìš°ì„ ìˆœìœ„: ì—…ë¡œë“œ > ê²½ë¡œ
    geo_obj = None
    diag = {"source": None, "exists": None, "size": None, "features": None, "code_key": None, "name_key": None, "error": None}

    try:
        if geojson_file is not None:
            raw = geojson_file.getvalue().decode("utf-8", "ignore")
            geo_obj = json.loads(raw)
            st.session_state["siggeo_obj"] = geo_obj
            st.session_state["siggeo_path"] = ""  # ê²½ë¡œ ë¹„í™œì„±
            diag["source"] = "uploaded"
        elif geojson_path.strip():
            path = os.path.abspath(os.path.expanduser(geojson_path.strip()))
            st.session_state["siggeo_path"] = path
            diag["source"] = f"path: {path}"
            diag["exists"] = os.path.exists(path)
            if diag["exists"]:
                diag["size"] = os.path.getsize(path)
                with open(path, "r", encoding="utf-8-sig") as f:
                    geo_obj = json.load(f)
                st.session_state["siggeo_obj"] = geo_obj
            else:
                diag["error"] = "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        else:
            # ì´ì „ ì„¸ì…˜ì˜ ê°ì²´ ì¬ì‚¬ìš©
            geo_obj = st.session_state.get("siggeo_obj")
            if geo_obj is not None:
                diag["source"] = "session"
    except Exception as e:
        diag["error"] = f"{type(e).__name__}: {e}"

    # êµ¬ì¡° í‚¤ ìë™ íƒì§€
    def _detect_sig_props(geo: dict):
        if not geo or "features" not in geo or not geo["features"]:
            return None, None
        props = geo["features"][0].get("properties", {})
        candidates_code = ["SIG_CD", "adm_cd", "ADM_CD"]
        candidates_name = ["SIG_KOR_NM", "sig_kor_nm", "ADM_NM", "adm_nm", "EMD_KOR_NM"]
        code_key = next((k for k in candidates_code if k in props), None)
        name_key = next((k for k in candidates_name if k in props), None)
        return code_key, name_key

    if geo_obj:
        diag["features"] = len(geo_obj.get("features", []))
        diag["code_key"], diag["name_key"] = _detect_sig_props(geo_obj)

    with st.sidebar.expander("GeoJSON ì§„ë‹¨", expanded=True):
        st.write(diag)
        if geo_obj and (not diag["code_key"] or not diag["name_key"]):
            st.warning("âš ï¸ propertiesì— ì‹œêµ°êµ¬ ì½”ë“œ/ì´ë¦„ í‚¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆ: SIG_CD, SIG_KOR_NM")
        elif not geo_obj:
            st.info("GeoJSONì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")


    if page == "Landing":
        page_overview(bundle_id)
        render_risk_snapshot(ROOT)
    elif page == "Ingest":
        page_ingest(bundle_id)
    elif page == "Normalize":
        page_normalize(bundle_id)
    elif page == "Analyze":
        page_analyze(bundle_id)
        render_keywords_and_sentiment(ROOT, topn=20)
        render_frames_eval(ROOT)
    elif page == "Gate":
        render_gate_indicator(ROOT) 
    elif page == "Scoring":
        render_scoring(ROOT)
    elif page == "Risk":
        render_risk_page(ROOT)
    elif page == "Alerts":
        render_alerts(ROOT)
    elif page == "EventBlocks":
        render_eventblocks(ROOT)
    elif page == "Fusion":
        _call_page("fusion", "page_fusion", ROOT)
    elif page == "Blocks":
        _call_page("blocks", "page_blocks", ROOT)
    elif page == "Scenarios":
        _call_page("scenarios", "page_scenarios", ROOT)
    elif page == "Ledger":
        _call_page("ledger", "page_ledger", ROOT)

if __name__ == "__main__":
    main()
