# -*- coding: utf-8 -*-
from __future__ import annotations
import os, json, math, time, re
from pathlib import Path
from typing import List, Dict, Tuple
import pandas as pd
import streamlit as st

# ------------------- IO helpers -------------------
def _read_jsonl(p: Path) -> List[dict]:
    rows=[]
    if p.exists():
        with p.open(encoding="utf-8", errors="ignore") as f:
            for L in f:
                s=L.strip()
                if s: rows.append(json.loads(s))
    return rows

def _write_jsonl(p: Path, rows: List[dict]) -> None:
    # JSONL: ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ì €ì¥
    p.write_text("\n".join(json.dumps(r, ensure_ascii=False) for r in rows), encoding="utf-8")

# ------------------- text utils -------------------
_ws_re = re.compile(r"\s+")
_tok_re = re.compile(r"[A-Za-z0-9ê°€-í£]+")

def _norm_txt(x: str) -> str:
    if not x: return ""
    x = str(x)
    x = _ws_re.sub(" ", x).strip().lower()
    return x

def _tokens(x: str) -> List[str]:
    return _tok_re.findall(_norm_txt(x))

def _jaccard(a: List[str], b: List[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa or not sb: return 0.0
    return len(sa & sb) / max(1.0, len(sa | sb))

def _cosine_tfidf(titles: List[str]) -> Tuple[List[List[float]] | None, List[str]]:
    """
    ì„ íƒì  TF-IDF(ìˆìœ¼ë©´ ì‚¬ìš©). scikit-learn ë¯¸ì„¤ì¹˜ë©´ None ë°˜í™˜í•˜ì—¬ Jaccardë¡œ í´ë°±.
    """
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        vec = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1,2))
        m = vec.fit_transform([_norm_txt(t) for t in titles])
        sim = cosine_similarity(m)
        return sim.tolist(), vec.get_feature_names_out().tolist()
    except Exception:
        return None, []

def _parse_dt(s: str):
    dt = pd.to_datetime(s, errors="coerce", utc=True)
    if pd.isna(dt): return pd.NaT
    try: return dt.tz_convert(None)
    except Exception: return dt

# ------------------- core clustering -------------------
def _cluster_alerts(df: pd.DataFrame, sim_thr: float, win_days: int, use_tfidf: bool) -> List[List[int]]:
    """
    ê°„ë‹¨í•œ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§:
    - ë‘ ì•Œë¦¼ì˜ ìœ ì‚¬ë„ >= sim_thr
    - ê·¸ë¦¬ê³  ì‹œê°„ ì°¨ì´ê°€ win_days ì¼ ì´í•˜ì´ë©´ ì—£ì§€ ìƒì„±
    - ì—°ê²°ìš”ì†Œ(connected components)ë¥¼ ë¸”ë¡ìœ¼ë¡œ ê°„ì£¼
    """
    n = len(df)
    if n == 0: return []

    # ì „ì²˜ë¦¬
    toks = [ _tokens(t) for t in df["title"].astype(str).tolist() ]
    dts  = df["dt"].tolist()

    sim_matrix = None
    if use_tfidf:
        sim_matrix, _ = _cosine_tfidf(df["title"].astype(str).tolist())

    # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸
    adj = [[] for _ in range(n)]
    win = pd.Timedelta(days=win_days)

    for i in range(n):
        for j in range(i+1, n):
            # ì‹œê°„ì°½ ì²´í¬
            ti, tj = dts[i], dts[j]
            if pd.isna(ti) or pd.isna(tj):
                time_ok = True  # ë‚ ì§œ ì—†ìœ¼ë©´ ì‹œê°„ ì œì•½ íŒ¨ìŠ¤
            else:
                time_ok = abs(ti - tj) <= win
            if not time_ok:
                continue

            # ìœ ì‚¬ë„
            if sim_matrix is not None:
                sim = float(sim_matrix[i][j])
            else:
                sim = _jaccard(toks[i], toks[j])

            if sim >= sim_thr:
                adj[i].append(j); adj[j].append(i)

    # ì—°ê²°ìš”ì†Œ íƒìƒ‰
    vis = [False]*n
    blocks=[]
    for i in range(n):
        if vis[i]: continue
        stack=[i]; comp=[]
        vis[i]=True
        while stack:
            u=stack.pop()
            comp.append(u)
            for v in adj[u]:
                if not vis[v]:
                    vis[v]=True; stack.append(v)
        blocks.append(sorted(comp))
    return [b for b in blocks if len(b)>=1]

def _block_profile(df: pd.DataFrame, idxs: List[int]) -> dict:
    sub = df.iloc[idxs]
    # ì‹œê°„
    start = sub["dt"].min()
    end   = sub["dt"].max()
    # ë ˆë²¨(ì—†ìœ¼ë©´ info)
    level_rank = {"info":0, "warning":1, "alert":2}
    if "alert_level" in sub.columns:
        lvl_series = sub["alert_level"].map(level_rank).fillna(0)
        top_level = int(lvl_series.max()) if not lvl_series.empty else 0
    else:
        top_level = 0
    inv = {v:k for k,v in level_rank.items()}
    top_level = inv.get(top_level, "info")
    # ëŒ€í‘œ íƒ€ì´í‹€/í‚¤ì›Œë“œ
    titles = sub["title"].astype(str).tolist() if "title" in sub.columns else []
    all_toks = []
    for t in titles:
        all_toks.extend(_tokens(t))
    freq = pd.Series(all_toks).value_counts() if all_toks else pd.Series(dtype=int)
    top_kw = freq.head(5).index.tolist()
    rep_title = max(titles, key=lambda x: len(x))[:140] if titles else ""
    return {
        "start": str(start) if pd.notna(start) else "",
        "end":   str(end)   if pd.notna(end)   else "",
        "count": int(len(sub)),
        "top_level": top_level,
        "top_keywords": top_kw,
        "rep_title": rep_title
    }

def compute_eventblocks(root: Path, sim_thr: float=0.35, win_days: int=5, use_tfidf: bool=False, min_block: int=2) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    alerts.jsonl -> eventblocks.jsonl, eventblocks.timeline.json
    """
    alerts = _read_jsonl(root/"alerts.jsonl")
    if not alerts:
        # ê²°ê³¼ íŒŒì¼ë„ ë¹ˆ ìƒíƒœë¡œ ìƒì„±
        _write_jsonl(root/"eventblocks.jsonl", [])
        (root/"eventblocks.timeline.json").write_text(json.dumps({"blocks":[],"members":[]}, ensure_ascii=False, indent=2), encoding="utf-8")
        return pd.DataFrame(columns=["block_id","start","end","count","top_level","top_keywords","rep_title"]), pd.DataFrame(columns=["block_id","id","title","dt","alert_level","score"])

    # ê¸°ë³¸ DF
    df = pd.DataFrame(alerts)
    if "dt" not in df.columns:
        df["dt"] = ""
    df["dt"] = df["dt"].apply(_parse_dt)
    # ì•ˆì •ìš©: dt ì—†ìœ¼ë©´ ì¼ë ¨ë²ˆí˜¸ ë‚ ì§œ ë¶€ì—¬
    if df["dt"].isna().all():
        df["dt"] = pd.to_datetime(range(len(df)), unit="D", origin="2024-01-01")
    df = df.sort_values("dt").reset_index(drop=True)

    # í´ëŸ¬ìŠ¤í„°
    blocks = _cluster_alerts(df, sim_thr=sim_thr, win_days=win_days, use_tfidf=use_tfidf)
    # ìµœì†Œ í¬ê¸° í•„í„°
    blocks = [b for b in blocks if len(b) >= int(min_block)]

    # block_id ë§¤í•‘
    blk_rows=[]
    mem_rows=[]
    for k, idxs in enumerate(blocks, start=1):
        prof = _block_profile(df, idxs)
        blk = {
            "block_id": f"blk_{k:04d}",
            **prof
        }
        blk_rows.append(blk)
        for i in idxs:
            r = df.iloc[i].to_dict()
            mem_rows.append({
                "block_id": blk["block_id"],
                "id": r.get("id"),
                "title": r.get("title",""),
                "dt": str(df.iloc[i]["dt"]),
                "alert_level": r.get("alert_level",""),
                "score": r.get("score", 0.0)
            })

    # --- ìŠ¤í‚¤ë§ˆ ê³ ì • + ì•ˆì „ ì •ë ¬ ---
    blk_cols = ["block_id","start","end","count","top_level","top_keywords","rep_title"]
    df_blk = pd.DataFrame(blk_rows, columns=blk_cols)
    if not df_blk.empty:
        # ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ start/endë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•´ ì •ë ¬ ì•ˆì •í™”
        df_blk["_start_dt"] = pd.to_datetime(df_blk["start"], errors="coerce")
        df_blk["_end_dt"]   = pd.to_datetime(df_blk["end"], errors="coerce")
        df_blk = df_blk.sort_values(["_start_dt","_end_dt","count"], ascending=[True, True, False])
        df_blk = df_blk.drop(columns=["_start_dt","_end_dt"])
    df_mem = pd.DataFrame(mem_rows, columns=["block_id","id","title","dt","alert_level","score"])
    if not df_mem.empty:
        df_mem["dt"] = pd.to_datetime(df_mem["dt"], errors="coerce")
        df_mem = df_mem.sort_values(["block_id","dt"])

    # ì €ì¥
    _write_jsonl(root/"eventblocks.jsonl", df_blk.to_dict(orient="records"))
    (root/"eventblocks.timeline.json").write_text(
        json.dumps({"blocks": df_blk.to_dict(orient="records"),
                    "members": df_mem.to_dict(orient="records")},
                   ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    return df_blk, df_mem

# ------------------- streamlit render -------------------
def render_eventblocks(root: Path):
    st.header("ğŸ§© EventBlocks â€” í´ëŸ¬ìŠ¤í„°ë§ & íƒ€ì„ë¼ì¸")

    # íŒŒë¼ë¯¸í„°
    colA, colB, colC, colD = st.columns(4)
    with colA:
        sim_thr = st.slider("ìœ ì‚¬ë„ ì„ê³„ì¹˜(sim_thr)", 0.05, 0.95, 0.35, 0.05)
    with colB:
        win_days = st.slider("ì‹œê°„ì°½(day)", 1, 30, 5, 1)
    with colC:
        min_block = st.number_input("ë¸”ë¡ ìµœì†Œí¬ê¸°", min_value=1, max_value=50, value=2, step=1)
    with colD:
        use_tfidf = st.checkbox("TF-IDF ì‚¬ìš©(ìˆìœ¼ë©´)", value=False)

    # ì¬ê³„ì‚° ë²„íŠ¼
    btn = st.button("Recompute EventBlocks")
    blk_p = root/"eventblocks.jsonl"
    tim_p = root/"eventblocks.timeline.json"

    def _mt(p: Path): return os.path.getmtime(p) if p.exists() else 0
    # ê²°ê³¼ê°€ ì—†ê±°ë‚˜ alertsê°€ ë” ìµœì‹ ì´ê±°ë‚˜ ë²„íŠ¼ í´ë¦­ ì‹œ ì¬ê³„ì‚°
    need = (btn
            or (not blk_p.exists())
            or (_mt(blk_p) < _mt(root/"alerts.jsonl"))
            or (not tim_p.exists()))

    if need:
        df_blk, df_mem = compute_eventblocks(root, sim_thr=sim_thr, win_days=win_days, use_tfidf=use_tfidf, min_block=int(min_block))
    else:
        df_blk = pd.DataFrame(_read_jsonl(blk_p))
        timeline = json.loads(tim_p.read_text(encoding="utf-8", errors="ignore")) if tim_p.exists() else {"members":[],"blocks":[]}
        df_mem = pd.DataFrame(timeline.get("members", []))

    if df_blk.empty:
        st.info("ì´ë²¤íŠ¸ ë¸”ë¡ì´ ì—†ìŠµë‹ˆë‹¤. Alertsë¥¼ ë¨¼ì € ìƒì„±í•˜ê³ (ë˜ëŠ” ì„ê³„ì¹˜/ì‹œê°„ì°½ì„ ì™„í™”) ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    # KPI
    n_blk = len(df_blk)
    n_alerts = len(df_mem)
    c1,c2 = st.columns(2)
    c1.metric("ì´ë²¤íŠ¸ ë¸”ë¡ ìˆ˜", n_blk)
    c2.metric("ë¸”ë¡ ë‚´ ì•Œë¦¼ ìˆ˜", n_alerts)

    # íƒ€ì„ë¼ì¸(ê°„ë‹¨ ë§‰ëŒ€)
    st.subheader("íƒ€ì„ë¼ì¸(ê°œìš”)")
    try:
        tshow = df_blk.copy()
        tshow["start"] = pd.to_datetime(tshow["start"], errors="coerce")
        tshow["end"]   = pd.to_datetime(tshow["end"], errors="coerce")
        tshow["span_days"] = (tshow["end"] - tshow["start"]).dt.days.fillna(0).astype(int)
        tshow = tshow.sort_values("start")
        st.bar_chart(tshow.set_index("block_id")["span_days"], height=200)
    except Exception as e:
        st.caption(f"(íƒ€ì„ë¼ì¸ ìš”ì•½ ë Œë” ì˜¤ë¥˜: {e})")

    # ë¸”ë¡ í‘œ
    st.subheader("ë¸”ë¡ ìš”ì•½")
    cols_show = ["block_id","start","end","count","top_level","top_keywords","rep_title"]
    for c in cols_show:
        if c not in df_blk.columns:
            df_blk[c] = "" if c != "count" else 0
    st.dataframe(df_blk[cols_show], use_container_width=True, height=300)

    # íŠ¹ì • ë¸”ë¡ ì„ íƒ â†’ ë©¤ë²„ ë³´ê¸°
    st.subheader("ë¸”ë¡ ìƒì„¸")
    pick = st.selectbox("Block ì„ íƒ", options=df_blk["block_id"].tolist())
    sub = df_mem[df_mem["block_id"]==pick].copy()
    if not sub.empty:
        sub["dt"] = pd.to_datetime(sub["dt"], errors="coerce")
        sub = sub.sort_values("dt")
    st.dataframe(sub[["dt","alert_level","score","title","id"]] if not sub.empty else sub,
                 use_container_width=True, height=320)

    # ë‹¤ìš´ë¡œë“œ
    st.download_button(
        "eventblocks.jsonl ë‹¤ìš´ë¡œë“œ",
        data="\n".join(json.dumps(r, ensure_ascii=False) for r in df_blk.to_dict(orient="records")),
        file_name="eventblocks.jsonl",
        mime="application/json"
    )
    st.caption(f"source: {blk_p} Â· updated: {time.ctime(_mt(blk_p))}")
